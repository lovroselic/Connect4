{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d268fc-a3ce-4a0a-8ce9-14e42afe27b9",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a policy-gradient reinforcement learning algorithm that trains an *actor* (policy) and a *critic* (value function) together. It improves the policy using batches of collected trajectories while keeping updates ‚Äúproximal‚Äù (not too large), which makes training more stable and sample-efficient than vanilla policy gradients.\n",
    "\n",
    "Key ideas:\n",
    "- **Actor‚ÄìCritic**:  \n",
    "  - Actor: outputs a probability distribution over actions.  \n",
    "  - Critic: estimates the value function \\(V(s)\\) to reduce variance of gradient estimates.\n",
    "- **Clipped objective**\n",
    "\n",
    "  PPO uses a clipped surrogate objective to keep the new policy close to the old one:\n",
    "\n",
    "  `L_clip(Œ∏) = E_t[ min( r_t(Œ∏) * A_t, clip(r_t(Œ∏), 1 - Œµ, 1 + Œµ) * A_t ) ]`\n",
    "\n",
    "  where\n",
    "\n",
    "  `r_t(Œ∏) = œÄ_Œ∏(a_t | s_t) / œÄ_{Œ∏_old}(a_t | s_t)`\n",
    "\n",
    "  and `A_t` is the advantage estimate.\n",
    "\n",
    "- **On-policy with mini-batches**:  \n",
    "  Uses trajectories from the current policy, but performs multiple epochs of SGD on the same batch (with clipping to stay stable).\n",
    "- **Generalized Advantage Estimation (GAE)**:  \n",
    "  Often combined with PPO to compute smoother, lower-variance advantages from value predictions.\n",
    "\n",
    "In practice, PPO is popular because it is relatively easy to implement, works well with neural networks, and tends to be robust across a variety of environments and hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0a7db-43cf-4108-b8cc-cd5e1fd6b097",
   "metadata": {},
   "source": [
    "Version log:\n",
    "* 0.9 restart\n",
    "    * using DQN lessons learned as template\n",
    "* 0.10 fixed agent, oppo labels\n",
    "* 0.11 horizontal, color swaps\n",
    "* 0.12 learning not improved - dev paused or stopped\n",
    "* 0.20 restarted with corrected  LA and ENV\n",
    "* 0.30 restart redesign;\n",
    "* 0.31 changed ENV weights\n",
    "* 0.32 zero-sum approach\n",
    "* 0.33 more center forced env\n",
    "* 0.34 mentoring; warm start, offline RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5faf5-32c9-478b-9aab-073caa647b7a",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "[https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/](https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "<br>\n",
    "[https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68](https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68)\n",
    "<br>\n",
    "[https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html](https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bbbee-d469-4e6f-ade0-a73cf3a43f81",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8de2b-83d7-487f-94ca-ad1d71edf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import warnings\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16f9f0-2d38-44b4-948a-23517b4d7c1b",
   "metadata": {},
   "source": [
    "### Fixed Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ec004-01f5-4ee2-8f72-38f034b2db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c6675-8a18-42ee-b47e-aab3d9a9933a",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405a67f-62bc-4f8e-9048-eb8fece20d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4.connect4_env import Connect4Env\n",
    "from C4.fast_connect4_lookahead import Connect4Lookahead\n",
    "from PPO.ppo_training_phases_config import TRAINING_PHASES\n",
    "from PPO.actor_critic import ActorCritic\n",
    "from PPO.ppo_buffer import PPOBuffer, PPOHyperParams\n",
    "from PPO.ppo_update import ppo_update, PPOUpdateCfg\n",
    "from C4.connect4_board_display import Connect4_BoardDisplayer\n",
    "from C4.plot_phase_summary import plot_phase_summary\n",
    "from PPO.ppo_utilities import *\n",
    "from PPO.ppo_live_plot import plot_live_training_ppo\n",
    "from PPO.checkpoint import save_checkpoint, load_checkpoint\n",
    "from DQN.dqn_utilities import *\n",
    "from C4.connect4_board_display import display_final_boards_PPO\n",
    "from PPO.ppo_opponent_sampler import OpponentSampler\n",
    "from PPO.ppo_agent_eval import *\n",
    "from PPO.ppo_loop_helpers import *\n",
    "from C4.eval_oppo_dict import EVALUATION_OPPONENTS\n",
    "from PPO.ppo_hall_of_fame import PPOHallOfFame\n",
    "from PPO.LookaheadMentor import LookaheadMentor\n",
    "from PPO.ppo_bc_bootcamp_rl import BCBootcampRLConfig, run_bc_bootcamp_rl, plot_bc_bootcamp_rl\n",
    "from PPO.ppo_update import PPOUpdateCfg\n",
    "\n",
    "Lookahead = Connect4Lookahead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6033b-40d7-4206-8e80-f904a4428592",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513498ca-c36c-4197-b09e-2144271df931",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interval = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b2ebe-5a1c-4b33-8791-b1e87b7b7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR =\"Logs/PPO/\"\n",
    "MODEL_DIR =\"Models/PPO/\"\n",
    "PLOTS = \"Plots/PPO/\"\n",
    "TEMP_DIR =\"Models/PPO/TEMP/\"           \n",
    "reset_dir(TEMP_DIR)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc917d-6791-4316-b887-80f39b4099ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard defaults\n",
    "STEPS_PER_UPDATE    = 8192           # agent decisions per PPO update\n",
    "LEARNING_RATE       = 3e-4\n",
    "TEMPERATURE         = 1.0            # 0.0 = greedy sampling\n",
    "ATTR_LOSS_TO_LAST   = True           # attribute opponent's terminal win to our last move\n",
    "CLEAR_BUFFER_ON_PHASE_CHANGE = True  # avoid leaking rollouts across phases\n",
    "BENCHMARK_EVERY     = 25\n",
    "H2H_EVERY           = BENCHMARK_EVERY \n",
    "AUG_MULT            = 4\n",
    "REWARD_SCALE        = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d311b3-98ad-4af0-9972-0bbed630e6e8",
   "metadata": {},
   "source": [
    "# Training phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496731be-eb9c-4fe4-b07d-3e545e4d0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASES = PhaseTracker(TRAINING_PHASES)\n",
    "L, lastPhase=display_phases_table(TRAINING_PHASES)\n",
    "print(L, lastPhase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc4d62-5511-4ecb-815f-0bd7fab47b51",
   "metadata": {},
   "source": [
    "# Training session name - settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35877da1-7b13-41f7-a98b-f438eb0ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = True\n",
    "SEED_EPOCHS = 12 #2-3\n",
    "if not SEED: SEED_EPOCHS = 0\n",
    "number_episodes = L # all\n",
    "begin_start_time = time.time()\n",
    "model_name = \"DIST_X\"\n",
    "run_name =  \"Mentor5\"[:10]\n",
    "time_str = time.strftime('%Y-%m-%d %H-%M-%S', time.localtime(begin_start_time))\n",
    "tag = f\"{run_name}-{lastPhase}-{model_name}-S-{SEED}-E-{SEED_EPOCHS}\"\n",
    "TRAINING_SESSION = f\"PPQ-{number_episodes}-{tag} - at-{time_str}\"\n",
    "print(\"Start training session\", TRAINING_SESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66dade5-87d4-48de-9a7c-96e90de03b1a",
   "metadata": {},
   "source": [
    "## Hall of Fame / policy ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd0ef1-9a96-45b4-b7b8-34fed246c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOF_SPECS = {\n",
    "    \"RND_22b\": \"RND_22b PPO model.pt\",\n",
    "    \"RND_27a\": \"RND_27a PPO model.pt\",\n",
    "    \"RND_27b\": \"RND_27b PPO model.pt\",\n",
    "\n",
    "    \"DIST_X\": \"DIST_X PPO model.pt\",\n",
    "}\n",
    "\n",
    "hof = PPOHallOfFame(device=DEVICE)\n",
    "\n",
    "HOF_METASCORES = {\n",
    "    \"RND_22b\": 0.500,\n",
    "    \"RND_27a\": 0.500,\n",
    "    \"RND_27b\": 0.500,\n",
    "    \"DIST_X\": 0.500,\n",
    "}\n",
    "\n",
    "\n",
    "for name, path in HOF_SPECS.items():\n",
    "    hof.add_member(\n",
    "        name=name,\n",
    "        ckpt_path=path,\n",
    "        metascore=HOF_METASCORES.get(name),\n",
    "    )\n",
    "\n",
    "print(\"HOF members:\", hof.list_members())\n",
    "POP_ENSEMBLE_NAMES = list(HOF_SPECS.keys())  # use all for now\n",
    "\n",
    "pop_ensemble_policy = hof.build_ensemble(POP_ENSEMBLE_NAMES, use_metascore_weights=True)\n",
    "\n",
    "for p in pop_ensemble_policy.parameters():\n",
    "    p.requires_grad = False\n",
    "pop_ensemble_policy.eval()\n",
    "print(f\"using {len(POP_ENSEMBLE_NAMES)} checkpoints in ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d2a26-8e77-41d0-a7cf-76d01e566f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for P in hof.members.keys():\n",
    "    counts = exploitation_histogram(hof.members[P].policy, n_states=256, rng_seed=666)\n",
    "    eh = plot_exploitation_histogram(counts,  P)\n",
    "    plt.show(eh)\n",
    "    eba = plot_empty_board_action_distribution(hof.members[P].policy, tag=P)\n",
    "    plt.show(eba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846bc07-d22a-4e47-9a08-c5f5a58c3103",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32394ce-a7df-46a0-9fa9-6e49892200c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, resolved_path, meta = load_policy_simple(model_name, device=DEVICE, default_suffix=\" PPO model.pt\")\n",
    "print(f\"‚úÖ Loaded: {resolved_path}\")\n",
    "if meta:\n",
    "    epi = meta.get(\"episode\", None)\n",
    "    print(f\"Meta ‚Äî episode:{epi}  ts:{meta.get('timestamp','')}\")\n",
    "policy.eval(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243173f-7ad2-49bd-98ba-6f5647d9280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eh_pre = plot_exploitation_histogram(exploitation_histogram(policy, n_states=256, rng_seed=666),  f\"Starting from {model_name}\")\n",
    "plt.show(eh_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4dcb6b-7de7-4561-b92f-a7d334a04f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "eba_pre = plot_empty_board_action_distribution(policy, f\"Starting from {model_name}\")\n",
    "plt.show(eba_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43200cac-339d-478d-9a75-2e7176467e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== H2H inline metric config ====\n",
    "H2H_ENABLED      = True\n",
    "H2H_GAMES        = 200           # e.g., 150‚Äì250 ~ quick\n",
    "H2H_SEED         = SEED\n",
    "\n",
    "H2H_BASELINE_PATH = f\"{TEMP_DIR}H2H_baseline.pt\"\n",
    "H2H_KEEP_TEMPS    = 100          # keep last N temp ckpts\n",
    "h2h_history       = {\"episode\": [], \"score\": [], \"lo\": [], \"hi\": [], \"n\": []}\n",
    "ensemble_h2h_history       = {\"episode\": [], \"score\": [], \"lo\": [], \"hi\": [], \"n\": []}\n",
    "_h2h_temp_paths   = []\n",
    "hth_best_score    = -1.0\n",
    "\n",
    "# Save the baseline (B) once ‚Äî the model as loaded *before* new training\n",
    "save_checkpoint(\n",
    "    policy=policy, optim=None,\n",
    "    episode=0, \n",
    "    cfg=None, hparams=None,\n",
    "    model_path=H2H_BASELINE_PATH,\n",
    "    default_model_path=None\n",
    ")\n",
    "print(f\"‚úÖ H2H baseline saved to: {H2H_BASELINE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbd58d-8866-46c0-a111-3fc10f79053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(policy, input_size=(1, 2, 6, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f10894-1883-438b-b86f-befd71985f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Env / Policy / Optimizer / Buffer ---\n",
    "env     = Connect4Env()\n",
    "optim   = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "hparams = PPOHyperParams(gamma=0.99, gae_lambda=0.95, normalize_adv=True)\n",
    "buffer  = PPOBuffer(capacity=STEPS_PER_UPDATE * AUG_MULT, action_dim=7, hparams=hparams)\n",
    "\n",
    "cfg     = PPOUpdateCfg(epochs=4, batch_size=512, clip_range=0.20, vf_clip_range=0.20, ent_coef=0.01, \n",
    "                vf_coef=0.5, max_grad_norm=0.5, target_kl=0.03, distill_coef=0.0 )\n",
    "\n",
    "opening_kpis = init_opening_kpis(cols=7)          \n",
    "openings = OpeningTracker(cols=7, log_every=plot_interval, ma_window=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df06d5-a8ed-40e5-9077-d597a311f915",
   "metadata": {},
   "source": [
    "## Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d5ede-8a63-48a6-9511-451727ff41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED:\n",
    "    DATA_FILES = [\n",
    "        \"DATA/C4.L1L2L3_play_data.xlsx\" ,\n",
    "        \"DATA/C4.L3L2L1_play_data.xlsx\" ,\n",
    "        \"DATA/noisy_C4.L1L2L3_play_data.xlsx\",\n",
    "        \"DATA/C4.L5L1_play_data.xlsx\",\n",
    "        \"DATA/noisy_C4.L1L5_play_data.xlsx\",\n",
    "        \"DATA/noisy_C4.L3L5_play_data.xlsx\",\n",
    "        \"DATA/C4.L5L3_play_data.xlsx\",\n",
    "        \"DATA/C4.L5L7_play_data.xlsx\",\n",
    "    ]\n",
    "    \n",
    "    frames = [pd.read_excel(f) for f in DATA_FILES]\n",
    "    DATA = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    BOARD_COLS = [f\"{r}-{c}\" for r in range(6) for c in range(7)]\n",
    "    DATA[BOARD_COLS] = DATA[BOARD_COLS].astype(\"int8\")\n",
    "    DATA[\"reward\"] = pd.to_numeric(DATA[\"reward\"], errors=\"coerce\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(DATA_FILES)} files, total rows: {len(DATA):,}\")\n",
    "    display(DATA[[\"reward\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0a3dd-4d1f-411f-9b25-3b7bad22617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED:\n",
    "    r = DATA[\"reward\"].astype(float).dropna()\n",
    "    fig, ax = plt.subplots(figsize=(8,4.5))\n",
    "    bins = np.arange(r.min()-0.5, r.max()+1.5, 1) if np.allclose(r, r.round()) else \"auto\"\n",
    "    n, bins, _ = ax.hist(r, bins=bins, edgecolor=\"black\")\n",
    "    ax.axvline(r.mean(),   ls=\"--\", lw=2, label=f\"mean = {r.mean():.2f}\")\n",
    "    ax.axvline(r.median(), ls=\":\",  lw=2, label=f\"median = {r.median():.2f}\")\n",
    "    ax.set(title=\"Reward Distribution\", xlabel=\"reward\", ylabel=\"count\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5c97f-4cdb-43b8-822b-8296c0648303",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED:\n",
    "    begin_seed_time = time.time()\n",
    "    \n",
    "    ppo_cfg = PPOUpdateCfg(\n",
    "        epochs=4,\n",
    "        batch_size=256,\n",
    "        clip_range=0.2,\n",
    "        vf_clip_range=0.2,\n",
    "        ent_coef=0.005,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        target_kl=0.02,\n",
    "    )\n",
    "\n",
    "    bc_cfg = BCBootcampRLConfig(\n",
    "        passes=SEED_EPOCHS,  # e.g. 1..3\n",
    "        batch_size_ppo=256,\n",
    "        steps_per_update=2048,\n",
    "        reward_scale=REWARD_SCALE,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        augment_hflip=False,\n",
    "        augment_colorswap=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    metrics = run_bc_bootcamp_rl(\n",
    "        policy=policy,\n",
    "        optimizer=optim,\n",
    "        data_frames=[DATA],   # or [DATA_CLEAN, DATA_NOISY]\n",
    "        ppo_cfg=ppo_cfg,\n",
    "        bc_cfg=bc_cfg,\n",
    "    )\n",
    "    \n",
    "    end_seed_time = time.time()\n",
    "    total_seeding = (end_seed_time - begin_seed_time)\n",
    "    policy.eval()\n",
    "    print(f\"Seeding completed in {total_seeding:.1f} seconds\")\n",
    "    print(\"‚úÖ BC warm-start complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7e42e-bb01-48bb-ab87-1e793f310535",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED: \n",
    "    fig = plot_bc_bootcamp_rl(metrics)\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727bee4-1fd5-4d86-a0a6-605634a97e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = exploitation_histogram(policy, n_states=256, rng_seed=0)\n",
    "ehbc = plot_exploitation_histogram(counts, title=f\"PPO greedy exploit histogram (post-BC) {model_name}\")\n",
    "plt.show(ehbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a297e6-7c80-42b8-89dd-ee08d5c1f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eba_bc = plot_empty_board_action_distribution(policy, f\"After seeding BC for {SEED_EPOCHS} epochs {model_name}\")\n",
    "plt.show(eba_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97fbec-6734-48d8-ae97-63cea1940d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEED: torch.save(policy.state_dict(), f\"{model_name}_PPO_BC-CENTER.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e72bf9-bc15-41da-8402-f41ce929a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logging / histories ---\n",
    "win_history = []  \n",
    "\n",
    "ppo_metrics_history = {\n",
    "    \"episodes\": [],\n",
    "    \"loss_pi\": [],\n",
    "    \"loss_v\": [],\n",
    "    \"entropy\": [],\n",
    "    \"approx_kl\": [],\n",
    "    \"clip_frac\": [],\n",
    "    \"explained_variance\": [],\n",
    "}\n",
    "\n",
    "update_idx = 0\n",
    "steps_collected = 0\n",
    "reward_history = []              # per-episode total reward\n",
    "benchmark_history  = None\n",
    "wins = losses = draws = 0\n",
    "win_count  = 0\n",
    "loss_count = 0\n",
    "draw_count = 0\n",
    "win_history= []\n",
    "\n",
    "# --- opponent overlay / logging helpers ---\n",
    "opponent_timeline: list[str] = []   # one short key per episode -> drives the overlay\n",
    "sampled_opponent_key: str | None = None\n",
    "phase_mix: dict[str, float] = {\"R\": 1.0}  # default shown in logs (no effect yet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3be032-d64f-47a1-bc28-075063fba9d8",
   "metadata": {},
   "source": [
    "### Live plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5d6b1-d103-4797-a110-19adeec7f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_handle = display(None, display_id=True)   \n",
    "fig = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3b9cf-a5b2-44de-be50-ba5f7c87a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "eh_handle = display(None, display_id=True)   \n",
    "EH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19680bf-bd76-45b5-b5ef-485261e76264",
   "metadata": {},
   "outputs": [],
   "source": [
    "eba_handle = display(None, display_id=True)   \n",
    "EBA = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d822e-b348-46a2-8684-a965417d423f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a66d3-26bb-4fce-ae31-df97bb885a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# PPO TRAINING LOOP \n",
    "# ============================\n",
    "\n",
    "# temp state\n",
    "steps_collected = 0\n",
    "update_idx = 0\n",
    "current_phase_name = None\n",
    "last_agent_reward_entries_ref = [None]  # 1-slot box mutated in helpers\n",
    "current_mentor = None\n",
    "\n",
    "# phase-aware temperature (mutated in-place)\n",
    "temperature_ref = {\"T\": 1.0}\n",
    "recompute_fn = make_recompute_lp_val(policy, DEVICE, temperature_ref)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=number_episodes, desc=\"Episodes\", leave=True, dynamic_ncols=True) as epbar:\n",
    "    opponent_sampler = None\n",
    "\n",
    "    for episode in range(1, number_episodes + 1):\n",
    "        # --- curriculum phase ---\n",
    "        phase_info, changed = PHASES.start_episode(episode)\n",
    "        current_phase_name = phase_info.name\n",
    "\n",
    "        if changed:\n",
    "            # ---- apply phase params ----\n",
    "            p = params_for_phase(current_phase_name, cfg)\n",
    "\n",
    "            if p[\"mentor_prob\"] > 0.0 and p[\"mentor_coef\"] > 0.0:\n",
    "                current_mentor = LookaheadMentor(depth=p[\"mentor_prob\"], device=DEVICE)\n",
    "            else:\n",
    "                current_mentor = None\n",
    "\n",
    "            # optimizer LR\n",
    "            for g in optim.param_groups:\n",
    "                g[\"lr\"] = p[\"lr\"]\n",
    "\n",
    "            # PPO update cfg\n",
    "            cfg.clip_range    = p[\"clip\"]\n",
    "            cfg.ent_coef      = p[\"entropy\"]\n",
    "            cfg.epochs        = p[\"epochs\"]\n",
    "            cfg.target_kl     = p[\"target_kl\"]\n",
    "            cfg.vf_coef       = p[\"vf_coef\"]\n",
    "            cfg.vf_clip_range = p[\"vf_clip\"]\n",
    "            cfg.batch_size    = p[\"batch_size\"]\n",
    "            cfg.max_grad_norm = p[\"max_grad_norm\"]\n",
    "            temperature_ref[\"T\"] = p[\"temperature\"]\n",
    "            cfg.mentor_depth = p[\"mentor_depth\"]\n",
    "            cfg.mentor_prob  = p[\"mentor_prob\"]\n",
    "            cfg.mentor_coef  = p[\"mentor_coef\"]\n",
    "\n",
    "\n",
    "            # --- heuristic knobs (phase dependent) ---\n",
    "            policy.set_phase_heuristics(\n",
    "                center_start=p[\"center_start\"],\n",
    "                guard_prob=p[\"guard_prob\"],\n",
    "                win_now_prob=p[\"win_now_prob\"],\n",
    "                guard_ply_min=p[\"guard_ply_min\"],\n",
    "                guard_ply_max=p[\"guard_ply_max\"],\n",
    "            )\n",
    "\n",
    "            # rollout size change\n",
    "            new_spu = p.get(\"steps_per_update\", STEPS_PER_UPDATE)\n",
    "            if new_spu != STEPS_PER_UPDATE:\n",
    "                STEPS_PER_UPDATE = new_spu\n",
    "                buffer = PPOBuffer(\n",
    "                    capacity=STEPS_PER_UPDATE * AUG_MULT + 512,\n",
    "                    action_dim=7,\n",
    "                    hparams=hparams,\n",
    "                    reward_scale=REWARD_SCALE,\n",
    "                )\n",
    "                steps_collected = 0\n",
    "                last_agent_reward_entries_ref[0] = None\n",
    "\n",
    "            if CLEAR_BUFFER_ON_PHASE_CHANGE:\n",
    "                buffer.clear()\n",
    "                steps_collected = 0\n",
    "                last_agent_reward_entries_ref[0] = None\n",
    "\n",
    "            # opponent sampler for this phase\n",
    "            phase_mix_cfg = TRAINING_PHASES[current_phase_name].get(\"opponent_mix\", None)\n",
    "            phase_mix = {str(k): float(v) for k, v in (phase_mix_cfg or {\"R\": 1.0}).items()}\n",
    "            opponent_sampler = OpponentSampler(phase_mix, seed=episode)\n",
    "\n",
    "            # ---- phase checkpoint (once per phase) ----\n",
    "            phase_ckpt = f\"{MODEL_DIR}{TRAINING_SESSION}_phase_{current_phase_name}_ep{episode}.pt\"\n",
    "            save_checkpoint(\n",
    "                policy=policy, optim=optim,\n",
    "                episode=episode,\n",
    "                cfg=cfg, hparams=hparams,\n",
    "                model_path=phase_ckpt,\n",
    "                default_model_path=None\n",
    "            )\n",
    "\n",
    "        # choose opponent for THIS episode\n",
    "        sampled_key = opponent_sampler.sample_key()\n",
    "        lookahead_mode = OpponentSampler.key_to_mode(sampled_key)\n",
    "        opponent_timeline.append(sampled_key)\n",
    "\n",
    "        # POP opponent uses hall-of-fame ensemble instead of student\n",
    "        is_pop = (lookahead_mode == \"POP\")\n",
    "\n",
    "        if is_pop:\n",
    "            opp_policy = pop_ensemble_policy    # frozen ensemble\n",
    "            opp_mode   = \"self\"                 # reuse self-play branch in helpers\n",
    "        else:\n",
    "            opp_policy = policy                 # current student\n",
    "            opp_mode   = lookahead_mode\n",
    "\n",
    "        # ------------- episode init -------------\n",
    "        env.reset()\n",
    "        ep_return = 0.0                    # sum of shaped rewards (debug only)\n",
    "        total_reward = 0.0                 # terminal mapped reward (WIN/DRAW/LOSS)\n",
    "        final_result = None                # 1 win, -1 loss, 0.5 draw\n",
    "        last_agent_reward_entries_ref[0] = None\n",
    "        ply_idx = 0\n",
    "        policy.center_forced_used = False\n",
    "\n",
    "        # alternating start: maybe opponent opens (keep parity with DQN: use episode-1)\n",
    "        done, ply_idx, penalty = ppo_maybe_opponent_opening(\n",
    "            env=env,\n",
    "            policy=opp_policy,             # <--- ensemble if POP\n",
    "            episode=episode - 1,\n",
    "            lookahead_mode=opp_mode,       # \"self\" if POP, else original mode\n",
    "            temperature=temperature_ref[\"T\"],\n",
    "            buffer=buffer,\n",
    "            last_agent_reward_entries_ref=last_agent_reward_entries_ref,\n",
    "            loss_penalty=Connect4Env.LOSS_PENALTY,\n",
    "            opening_kpis=opening_kpis,\n",
    "            openings=openings,\n",
    "            attr_loss_to_last=ATTR_LOSS_TO_LAST,\n",
    "        )\n",
    "        ep_return += penalty\n",
    "\n",
    "        # ================= EPISODE LOOP =================\n",
    "        while not env.done:\n",
    "            # --- Agent move (+1) ---\n",
    "            done, r_agent, ply_idx = ppo_agent_step(\n",
    "                env=env,\n",
    "                policy=policy,\n",
    "                buffer=buffer,\n",
    "                temperature=temperature_ref[\"T\"],\n",
    "                recompute_fn=recompute_fn,\n",
    "                last_agent_reward_entries_ref=last_agent_reward_entries_ref,\n",
    "                ply_idx=ply_idx,\n",
    "                opening_kpis=opening_kpis,\n",
    "                openings=openings,\n",
    "            )\n",
    "            ep_return += r_agent\n",
    "            steps_collected += 1\n",
    "\n",
    "            if done:\n",
    "                _, final_result, total_reward = ppo_finalize_if_done(env)\n",
    "                break\n",
    "            \n",
    "            # --- Opponent (-1) responds ---\n",
    "            done, ply_idx, penalty = ppo_opponent_step(\n",
    "                env=env,\n",
    "                policy=opp_policy,\n",
    "                lookahead_mode=lookahead_mode,\n",
    "                temperature=temperature_ref[\"T\"],\n",
    "                buffer=buffer,\n",
    "                last_agent_reward_entries_ref=last_agent_reward_entries_ref,\n",
    "                loss_penalty=Connect4Env.LOSS_PENALTY,\n",
    "                ply_idx=ply_idx,\n",
    "                opening_kpis=opening_kpis,\n",
    "                openings=openings,\n",
    "                attr_loss_to_last=ATTR_LOSS_TO_LAST,\n",
    "            )\n",
    "\n",
    "            ep_return += penalty # penalty is negative\n",
    "\n",
    "            if done:\n",
    "                _, final_result, total_reward = ppo_finalize_if_done(env)\n",
    "                break\n",
    "\n",
    "            # ===== PPO UPDATE GATE (after agent decisions) =====\n",
    "            if steps_collected >= STEPS_PER_UPDATE:\n",
    "                # bootstrap value if non-terminal and agent to move\n",
    "                last_done = not (env.current_player == +1 and not env.done)\n",
    "                last_value = 0.0\n",
    "                if not last_done:\n",
    "                    with torch.no_grad():\n",
    "                        enc_boot = encode_two_channel_agent_centric(env.board, +1)\n",
    "                        inp = torch.from_numpy(enc_boot).unsqueeze(0).to(DEVICE).float()\n",
    "                        _, v_boot = policy.forward(inp)\n",
    "                        last_value = float(v_boot.squeeze().item())\n",
    "\n",
    "                buffer.compute_gae(last_value=last_value, last_done=last_done)\n",
    "                metrics = ppo_update(policy, optim, buffer, cfg, pop_ensemble_policy, current_mentor)\n",
    "\n",
    "                # ---- record metrics per update ----\n",
    "                ppo_metrics_history[\"episodes\"].append(episode)\n",
    "                ppo_metrics_history[\"loss_pi\"].append(float(metrics.get(\"loss_pi\", 0.0)))\n",
    "                ppo_metrics_history[\"loss_v\"].append(float(metrics.get(\"loss_v\", 0.0)))\n",
    "                ppo_metrics_history[\"entropy\"].append(float(metrics.get(\"entropy\", 0.0)))\n",
    "                ppo_metrics_history[\"approx_kl\"].append(float(metrics.get(\"approx_kl\", 0.0)))\n",
    "                ppo_metrics_history[\"clip_frac\"].append(float(metrics.get(\"clip_frac\", 0.0)))\n",
    "                ppo_metrics_history[\"explained_variance\"].append(float(metrics.get(\"explained_variance\", 0.0)))\n",
    "\n",
    "                buffer.clear()\n",
    "                steps_collected = 0\n",
    "                last_agent_reward_entries_ref[0] = None\n",
    "                update_idx += 1\n",
    "\n",
    "        # ============ EPISODE END ============\n",
    "\n",
    "        # Win/loss/draw counters\n",
    "        wins, losses, draws = track_result(final_result, win_history)\n",
    "        win_count += wins\n",
    "        loss_count += losses\n",
    "        draw_count += draws\n",
    "\n",
    "        reward_history.append(ep_return)\n",
    "        openings.maybe_log(episode + 1)\n",
    "        epbar.update(1)\n",
    "\n",
    "        # --- periodic head-to-head vs baseline (inline) ---\n",
    "        if H2H_ENABLED and ((episode % H2H_EVERY) == 0 or episode == 1):\n",
    "            tmp_path = f\"{TEMP_DIR}{TRAINING_SESSION}_H2H_tmp_ep{episode}.pt\"\n",
    "            save_checkpoint(policy=policy, optim=optim, episode=episode, cfg=cfg, hparams=hparams, model_path=tmp_path, default_model_path=None)\n",
    "            _h2h_temp_paths.append(tmp_path)\n",
    "\n",
    "            # trim old temps\n",
    "            while len(_h2h_temp_paths) > H2H_KEEP_TEMPS:\n",
    "                old = _h2h_temp_paths.pop(0)\n",
    "                try: os.remove(old)\n",
    "                except Exception: pass\n",
    "\n",
    "            # A (current) vs B (baseline)\n",
    "            res = head_to_head(\n",
    "                ckptA=tmp_path,\n",
    "                ckptB=H2H_BASELINE_PATH,\n",
    "                n_games=H2H_GAMES,\n",
    "                device=DEVICE,\n",
    "                opening_noise_k=0,\n",
    "                seed=H2H_SEED,\n",
    "                progress=False,\n",
    "            )\n",
    "\n",
    "            score = float(res[\"A_score_rate\"])\n",
    "            if episode > 50 and score >= hth_best_score:\n",
    "                hth_best_score = score\n",
    "                best_path = f\"{TEMP_DIR}H2H_best_score({score:.3f})_ep{episode}.pt\"\n",
    "                save_checkpoint(\n",
    "                    policy=policy, optim=optim,\n",
    "                    episode=episode,\n",
    "                    cfg=cfg, hparams=hparams,\n",
    "                    model_path=best_path,\n",
    "                    default_model_path=None,\n",
    "                )\n",
    "\n",
    "            h2h_append(h2h_history, episode, res)\n",
    "\n",
    "            #H2H vs ensemble\n",
    "            res_pop = head_to_head_models(\n",
    "                policy,\n",
    "                pop_ensemble_policy,\n",
    "                n_games=H2H_GAMES,\n",
    "                A_label=\"Student\",\n",
    "                B_label=\"POP_ensemble\",\n",
    "                opening_noise_k=0,\n",
    "                seed=H2H_SEED,\n",
    "                progress=False,\n",
    "            )\n",
    "            ensemble_score = float(res_pop[\"A_score_rate\"])\n",
    "            h2h_append(ensemble_h2h_history, episode, res_pop)\n",
    "\n",
    "        # --- inline benchmark\n",
    "        if (episode % BENCHMARK_EVERY) == 0 or episode == 1:\n",
    "            benchmark_history = update_benchmark_winrates(\n",
    "                agent=policy,\n",
    "                env=env,\n",
    "                device=DEVICE,\n",
    "                Lookahead=Connect4Lookahead,\n",
    "                episode=episode,\n",
    "                history=benchmark_history,\n",
    "                save=f\"{LOG_DIR}{TRAINING_SESSION}-benchmark.xlsx\",\n",
    "                score = score,\n",
    "                ensemble_score = ensemble_score\n",
    "            )\n",
    "\n",
    "        # --- live plot ---\n",
    "        if (episode % plot_interval) == 0:\n",
    "            fig = plot_live_training_ppo(\n",
    "                episode=episode,\n",
    "                reward_history=reward_history,\n",
    "                win_history=win_history,\n",
    "                phase_name=current_phase_name,\n",
    "                win_count=win_count,\n",
    "                loss_count=loss_count,\n",
    "                draw_count=draw_count,\n",
    "                metrics_history=ppo_metrics_history,\n",
    "                benchmark_history=benchmark_history,\n",
    "                title=TRAINING_SESSION,\n",
    "                phases=TRAINING_PHASES,\n",
    "                save=False,\n",
    "                save_path=PLOTS,\n",
    "                opponent_timeline=opponent_timeline,\n",
    "                overlay_last=100,\n",
    "                h2h_history=h2h_history,\n",
    "                ensemble_h2h_history = ensemble_h2h_history,\n",
    "                openings=openings, \n",
    "                policy=policy,\n",
    "            )\n",
    "\n",
    "            if fig is not None:\n",
    "                plots_handle.update(fig)   \n",
    "                plt.close(fig)\n",
    "\n",
    "            # histograms\n",
    "            cnt = exploitation_histogram(policy, n_states=256, rng_seed=666)\n",
    "            EH = plot_exploitation_histogram(cnt, f\"Training from {model_name}\")\n",
    "            if EH is not None:\n",
    "                eh_handle.update(EH)   \n",
    "                plt.close(EH)\n",
    "\n",
    "            EBA = plot_empty_board_action_distribution(policy, f\"Training from {model_name}\")\n",
    "            if EBA is not None:\n",
    "                eba_handle.update(EBA)   \n",
    "                plt.close(EBA)\n",
    "\n",
    "print(f\"Training finished in {(time.time() - start_time)/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e3ecf-2dda-43be-a01e-b4a532dd6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_opening_kpis(opening_kpis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01644ee-f1c5-43c4-9d84-4a883d473a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 300\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "\n",
    "final_reward_fig, final_reward_ax = plt.subplots(figsize=(10, 5))\n",
    "final_reward_ax.plot(smoothed, label=f\"Smoothed Reward (window={window})\", color='blue')\n",
    "\n",
    "# --- Add phase transitions ---\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    ep = meta[\"length\"]\n",
    "    if ep is not None and ep <= len(reward_history):\n",
    "        final_reward_ax.axvline(x=ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_reward_ax.text(ep + 5, max(smoothed) * 0.95, name, rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "final_reward_ax.set_title(\"Smoothed Reward Over Episodes\")\n",
    "final_reward_ax.set_xlabel(\"Episode\")\n",
    "final_reward_ax.set_ylabel(\"Smoothed Reward\")\n",
    "final_reward_ax.legend()\n",
    "final_reward_ax.grid(True)\n",
    "final_reward_fig.tight_layout()\n",
    "\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff0af8-5865-4ff7-8039-3b6c437c6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    " fig = plot_live_training_ppo(\n",
    "                episode=episode,\n",
    "                reward_history=reward_history,\n",
    "                win_history=win_history,\n",
    "                phase_name=current_phase_name,\n",
    "                win_count=win_count,\n",
    "                loss_count=loss_count,\n",
    "                draw_count=draw_count,\n",
    "                metrics_history=ppo_metrics_history,\n",
    "                benchmark_history=benchmark_history,\n",
    "                title=TRAINING_SESSION,\n",
    "                phases=TRAINING_PHASES,\n",
    "                save=True,\n",
    "                save_path=PLOTS,\n",
    "                opponent_timeline=opponent_timeline,\n",
    "                overlay_last=100,\n",
    "                h2h_history=h2h_history,\n",
    "                ensemble_h2h_history = ensemble_h2h_history,\n",
    "                openings=openings, \n",
    "                policy=policy,\n",
    "            )\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055063e0-9f09-4fe5-af86-33da05a13551",
   "metadata": {},
   "source": [
    "### Post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b0647-814f-4f7e-ac14-d994551bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = exploitation_histogram(policy, n_states=256, rng_seed=0)\n",
    "posteh = plot_exploitation_histogram(counts, title=f\"PPO greedy exploit histogram (post training) - from:{model_name}\")\n",
    "plt.show(posteh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97bb12-2d16-4675-be26-09c655354b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "eba = plot_empty_board_action_distribution(policy,f\"Empty board action, post training from {model_name}\")\n",
    "plt.show(eba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5e697-5315-45ed-9af2-8595d438cab3",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b1a48-39dc-426c-a021-c792eca76ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 ppo_model_{timestamp} episodes-{number_episodes}.pt\"\n",
    "default_model_path = \"Connect4 PPQ model.pt\"   \n",
    "\n",
    "model_path, default_model_path = save_checkpoint(\n",
    "    policy=policy,\n",
    "    optim=optim,\n",
    "    episode=episode,\n",
    "    cfg=cfg,\n",
    "    hparams=hparams,\n",
    "    model_path=model_path,\n",
    "    default_model_path=default_model_path,\n",
    ")\n",
    "print(f\"Model saved to {model_path}\\nAlso wrote: {default_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1adc007-acdc-401e-86f5-6734e1263488",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64643d22-82e8-43e6-ab41-162c8c473d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCEL_OUT = \"EVAL_PPO_results.xlsx\"\n",
    "t0 = time.time()\n",
    "evaluation_results = evaluate_actor_critic_model(policy, EVALUATION_OPPONENTS, seed=SEED)\n",
    "elapsed_h = (time.time() - t0) / 3600.0\n",
    "row = results_to_row(run_tag=TRAINING_SESSION, results=evaluation_results, elapsed_h=elapsed_h, episodes=meta.get(\"episode\"))\n",
    "display(row)\n",
    "append_eval_row_to_excel(row, EXCEL_OUT)\n",
    "print(\"Saved to:\", EXCEL_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ffdee-8354-400c-87b0-e8c87a25fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text summary ---\n",
    "print(\"üìä Evaluation Summary:\")\n",
    "for label, s in evaluation_results.items():\n",
    "    print(f\"{label:>11}: {s['wins']}W / {s['losses']}L / {s['draws']}D ‚Üí \"\n",
    "          f\"Win {s['win_rate']*100:.1f}% | Loss {s['loss_rate']*100:.1f}% | Draw {s['draw_rate']*100:.1f}%\")\n",
    "\n",
    "# --- Plot ---\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates  = [evaluation_results[k]['win_rate'] * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %')\n",
    "plt.bar(x + bar_width, loss_rates, width=bar_width, label='Loss %')\n",
    "plt.bar(x + 2*bar_width, draw_rates, width=bar_width, label='Draw %')\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('PPO Agent Performance vs Various Opponents')\n",
    "plt.xticks(x + bar_width, labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e1a1b-87c3-47f7-8308-93de464a5740",
   "metadata": {},
   "source": [
    "### Boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88194fac-2816-4c84-add8-0da75d3ce646",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_final_boards_PPO(policy, [\"Random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592ecca-a2ee-4aa7-82da-4758f723ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_final_boards_PPO(policy, [\"Lookahead-1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b745fe-307f-4bec-a334-6868583ec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_final_boards_PPO(policy, [\"Lookahead-2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a6017-0f44-4d1c-8cbe-db5ee5509538",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_final_boards_PPO(policy, [\"Lookahead-3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523ccf9-5780-4bb0-8d83-8d9947a9dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_final_boards_PPO(policy, [\"Lookahead-4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680460bb-a773-496b-8404-c2b2cb57f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_final_boards_PPO(policy, [\"Lookahead-5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40fe8e-3c07-41d3-84b1-da60664d1ced",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e609264-b56c-4d75-b4b3-f6b9e8f526ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = (total_end_time - begin_start_time) / 3600\n",
    "print(f\"Evaluation completed in {total_elapsed:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd715e9-7eb1-4868-80f8-07374616b796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
