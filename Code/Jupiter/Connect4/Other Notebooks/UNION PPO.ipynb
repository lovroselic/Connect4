{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6129695f",
   "metadata": {},
   "source": [
    "# UNION (single-channel) â€” policy distillation to CNet192\n",
    "\n",
    "This notebook distills an **ensemble of CNet192 teachers**  into a **single CNet192** student;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e244ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from C4.connect4_env import Connect4Env\n",
    "from C4.CNet192 import CNet192, load_cnet192, save_cnet192\n",
    "from PPO.ppo_hall_of_fame import PPOHallOfFame, _state_to_1ch_tensor, _argmax_legal_center_tiebreak\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "N_ACTIONS = 7\n",
    "\n",
    "# --- global seeds ---\n",
    "SEED = 666\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d004b5ce-e7fb-452e-ba94-07171b15e8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 7500\n",
    "MAX_SAMPLES = 300_000\n",
    "BATCH_SIZE = 512 \n",
    "LR = 3e-4 \n",
    "WD = 1e-5\n",
    "NUM_EPOCHS = 20\n",
    "GRAD_CLIP = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ced837-1d8d-4e6e-93a6-22e639ea779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOF_METASCORES = {\n",
    "    \"PPO_Models/MIX_12a.pt\": 0.586667,\n",
    "    #\"PPO_Models/MIX_12b.pt\": 0.507500,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8541c44d-4c3b-4e06-a80b-1f29443bb4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teachers: ['MIX_12a']\n",
      "Weights: {'MIX_12a': np.float32(1.0)}\n",
      "Loaded teacher MIX_12a from PPO_Models/MIX_12a.pt\n"
     ]
    }
   ],
   "source": [
    "# Derive teacher names/weights from HOF_METASCORES and load policies\n",
    "\n",
    "TEACHER_CKPT_PATHS = list(HOF_METASCORES.keys())\n",
    "TEACHER_NAMES = [Path(p).stem for p in TEACHER_CKPT_PATHS]\n",
    "\n",
    "# Convenience maps: name -> path, name -> metascore\n",
    "HOF_PATHS_BY_NAME  = {Path(p).stem: p for p in HOF_METASCORES.keys()}\n",
    "HOF_SCORES_BY_NAME = {Path(p).stem: score for p, score in HOF_METASCORES.items()}\n",
    "\n",
    "# Normalized weights in TEACHER_NAMES order\n",
    "TEACHER_WEIGHTS = np.array(\n",
    "    [HOF_SCORES_BY_NAME[name] for name in TEACHER_NAMES],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "TEACHER_WEIGHTS /= TEACHER_WEIGHTS.sum()\n",
    "\n",
    "print(\"Teachers:\", TEACHER_NAMES)\n",
    "print(\"Weights:\", dict(zip(TEACHER_NAMES, TEACHER_WEIGHTS)))\n",
    "\n",
    "# Instantiate Hall of Fame and register members\n",
    "hof = PPOHallOfFame(device=DEVICE)\n",
    "\n",
    "for name in TEACHER_NAMES:\n",
    "    ckpt_path = HOF_PATHS_BY_NAME[name]\n",
    "    if not Path(ckpt_path).exists():\n",
    "        raise FileNotFoundError(f\"HOF checkpoint not found: {ckpt_path}\")\n",
    "    hof.add_member(\n",
    "        name=name,\n",
    "        ckpt_path=str(ckpt_path),\n",
    "        metascore=HOF_SCORES_BY_NAME[name],\n",
    "    )\n",
    "\n",
    "# Materialize wrapped, frozen policies\n",
    "TEACHERS: Dict[str, nn.Module] = {}\n",
    "for name in TEACHER_NAMES:\n",
    "    pol = hof.get_policy(name)\n",
    "    pol.eval()\n",
    "    TEACHERS[name] = pol\n",
    "    print(f\"Loaded teacher {name} from {hof.get_member(name).ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47156cda-2f78-410a-bce4-b7f10add06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation dataset + logits-mixture helpers\n",
    "\n",
    "NEG_INF = -1e9  # masking value for illegal actions\n",
    "\n",
    "\n",
    "class DistillDataset(Dataset):\n",
    "    \"\"\"\n",
    "    states: (N,1,6,7) float32\n",
    "    targets: (N,7) float32  (teacher mixture probs)\n",
    "    legal_mask: (N,7) bool  (True for legal moves)\n",
    "    \"\"\"\n",
    "    def __init__(self, states: np.ndarray, targets: np.ndarray, legal_mask: np.ndarray):\n",
    "        assert states.ndim == 4 and states.shape[1:] == (1, 6, 7), f\"states must be (N,1,6,7), got {states.shape}\"\n",
    "        assert targets.ndim == 2 and targets.shape[1] == 7, f\"targets must be (N,7), got {targets.shape}\"\n",
    "        assert legal_mask.ndim == 2 and legal_mask.shape[1] == 7, f\"legal_mask must be (N,7), got {legal_mask.shape}\"\n",
    "\n",
    "        self.states = torch.from_numpy(states.astype(np.float32, copy=False))\n",
    "        self.targets = torch.from_numpy(targets.astype(np.float32, copy=False))\n",
    "        self.legal_mask = torch.from_numpy(legal_mask.astype(np.bool_, copy=False))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.states.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.states[i], self.targets[i], self.legal_mask[i]\n",
    "\n",
    "\n",
    "def _compute_legal_mask_from_state_1ch(states_1ch: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    states_1ch: (N,1,6,7) mover POV scalar board (board * player)\n",
    "    Legal iff top cell in that column is zero => states[:,0,0,c] == 0\n",
    "    \"\"\"\n",
    "    top_row = states_1ch[:, 0, 0, :]  # (N,7)\n",
    "    return (top_row == 0.0)\n",
    "\n",
    "def _softmax_masked(logits: np.ndarray, legal_mask: np.ndarray) -> np.ndarray:\n",
    "    z = logits.astype(np.float32, copy=True)\n",
    "    z[~legal_mask] = NEG_INF\n",
    "    m = float(np.max(z[legal_mask])) if legal_mask.any() else 0.0\n",
    "    e = np.zeros_like(z, dtype=np.float32)\n",
    "    e[legal_mask] = np.exp(z[legal_mask] - m)\n",
    "    s = float(e.sum())\n",
    "    if s <= 0.0:\n",
    "        # fallback uniform over legal\n",
    "        p = np.zeros(7, dtype=np.float32)\n",
    "        if legal_mask.any():\n",
    "            p[legal_mask] = 1.0 / float(legal_mask.sum())\n",
    "        return p\n",
    "    return (e / s).astype(np.float32)\n",
    "\n",
    "def ensemble_mixture_probs(state_1ch: np.ndarray, legal_actions: List[int]) -> np.ndarray:\n",
    "    legal_mask = np.zeros(7, dtype=np.bool_)\n",
    "    legal_mask[np.asarray(legal_actions, dtype=np.int64)] = True\n",
    "\n",
    "    # mixture over teachers: sum_i w_i * softmax_i(masked)\n",
    "    mix = np.zeros(7, dtype=np.float32)\n",
    "    for name, w in zip(TEACHER_NAMES, TEACHER_WEIGHTS):\n",
    "        teacher = TEACHERS[name]\n",
    "        logits = _teacher_logits_np(teacher, state_1ch)  # (7,)\n",
    "        p = _softmax_masked(logits, legal_mask)          # (7,)\n",
    "        mix += float(w) * p\n",
    "\n",
    "    # renormalize (just in case)\n",
    "    mix[~legal_mask] = 0.0\n",
    "    s = float(mix.sum())\n",
    "    if s <= 0.0:\n",
    "        if legal_mask.any():\n",
    "            mix[legal_mask] = 1.0 / float(legal_mask.sum())\n",
    "            return mix\n",
    "        return np.zeros(7, dtype=np.float32)\n",
    "    return (mix / s).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce324fe-6b3a-4762-8dae-0f546ef9b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def _teacher_logits_np(teacher: nn.Module, state_1ch: np.ndarray) -> np.ndarray:\n",
    "    # state_1ch: (1,6,7) float32\n",
    "    x = torch.from_numpy(state_1ch[None]).to(DEVICE)  # (1,1,6,7)\n",
    "    out = teacher(x)\n",
    "    logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "    return logits[0].detach().float().cpu().numpy()   # (7,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13bd4b09-4ba8-4231-8d0f-703d4bd04581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distill_data(\n",
    "    n_episodes: int = 200,\n",
    "    max_moves: int = 42,\n",
    "    max_samples: int = 80_000,\n",
    "    seed: int = 666,\n",
    ") -> DistillDataset:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    states: List[np.ndarray] = []\n",
    "    targets: List[np.ndarray] = []\n",
    "    legal_masks: List[np.ndarray] = []\n",
    "\n",
    "    total_samples = 0\n",
    "\n",
    "    pbar = tqdm(range(n_episodes), desc=\"Generating distill rollouts\", leave=True)\n",
    "    for _ in pbar:\n",
    "        env = Connect4Env()\n",
    "        env.reset()\n",
    "        done = False\n",
    "        moves = 0\n",
    "\n",
    "        while (not done) and moves < max_moves and total_samples < max_samples:\n",
    "            legal = env.available_actions()\n",
    "            if not legal:\n",
    "                break\n",
    "\n",
    "            # env already returns mover-centric POV (1,6,7)\n",
    "            s = env.get_state(perspective=env.current_player).astype(np.float32, copy=False)\n",
    "\n",
    "            lm = np.zeros(7, dtype=np.bool_)\n",
    "            lm[np.asarray(legal, dtype=np.int64)] = True\n",
    "\n",
    "            target_probs = ensemble_mixture_probs(state_1ch=s, legal_actions=legal)\n",
    "\n",
    "            states.append(s)                  # (1,6,7)\n",
    "            targets.append(target_probs)      # (7,)\n",
    "            legal_masks.append(lm)            # (7,)\n",
    "            total_samples += 1\n",
    "\n",
    "            # drive env with teacher-mixture argmax (legal-safe)\n",
    "            a = int(target_probs.argmax())\n",
    "            if a not in legal:\n",
    "                a = int(rng.choice(legal))\n",
    "            _, _, done = env.step(a)\n",
    "            moves += 1\n",
    "\n",
    "        pbar.set_postfix(samples=total_samples)\n",
    "\n",
    "        if total_samples >= max_samples:\n",
    "            break\n",
    "\n",
    "    states_arr = np.stack(states, axis=0)        # (N,1,6,7)\n",
    "    targets_arr = np.stack(targets, axis=0)      # (N,7)\n",
    "    legal_arr = np.stack(legal_masks, axis=0)    # (N,7)\n",
    "\n",
    "    print(f\"Final distill dataset: states {states_arr.shape}, targets {targets_arr.shape}, legal {legal_arr.shape}\")\n",
    "    return DistillDataset(states_arr, targets_arr, legal_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af06af08-a451-4e23-878e-33965b890e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbe2deeca0e44b8808297406235c93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating distill rollouts:   0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final distill dataset: states (127500, 1, 6, 7), targets (127500, 7), legal (127500, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kick off dataset generation \n",
    "distill_dataset = generate_distill_data(\n",
    "    n_episodes=EPISODES,\n",
    "    max_moves=42,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "len(distill_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6384d13-5398-435a-b428-9689218428e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Run dataset generation cell first (states_arr/targets_arr).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ---- assume you already have these from dataset generation:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# states_arr: (N,1,6,7)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# targets_arr: (N,7)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates_arr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets_arr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun dataset generation cell first (states_arr/targets_arr).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m legal_mask_arr \u001b[38;5;241m=\u001b[39m _compute_legal_mask_from_state_1ch(states_arr)\n\u001b[0;32m      8\u001b[0m distill_dataset \u001b[38;5;241m=\u001b[39m DistillDataset(states_arr, targets_arr, legal_mask_arr)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Run dataset generation cell first (states_arr/targets_arr)."
     ]
    }
   ],
   "source": [
    "# ---- assume you already have these from dataset generation:\n",
    "# states_arr: (N,1,6,7)\n",
    "# targets_arr: (N,7)\n",
    "assert \"states_arr\" in globals() and \"targets_arr\" in globals(), \"Run dataset generation cell first (states_arr/targets_arr).\"\n",
    "\n",
    "legal_mask_arr = _compute_legal_mask_from_state_1ch(states_arr)\n",
    "\n",
    "distill_dataset = DistillDataset(states_arr, targets_arr, legal_mask_arr)\n",
    "\n",
    "# ---- split + loaders\n",
    "VAL_FRAC = 0.10\n",
    "N = len(distill_dataset)\n",
    "n_val = int(round(N * VAL_FRAC))\n",
    "n_train = N - n_val\n",
    "\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "train_ds, val_ds = random_split(distill_dataset, [n_train, n_val], generator=g)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE.type == \"cuda\"),\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE.type == \"cuda\"),\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"Loaders ready: train={len(train_ds)}  val={len(val_ds)}  batch={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b1b32-804d-4dcb-8b7c-447d4a1b537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train: bool = True, desc: str | None = None) -> float:\n",
    "    student.train(train)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_items = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc or (\"Train\" if train else \"Val\"), leave=False)\n",
    "\n",
    "    for states, targets, legal_mask in pbar:\n",
    "        states = states.to(DEVICE, non_blocking=True)            # (B,1,6,7)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)          # (B,7)\n",
    "        legal_mask = legal_mask.to(DEVICE, non_blocking=True)    # (B,7) bool\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        out = student(states)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out  # (B,7)\n",
    "\n",
    "        # CRITICAL: mask illegal actions before log_softmax\n",
    "        logits = logits.masked_fill(~legal_mask, -1e9)\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = criterion(log_probs, targets)  # KL(student || teacher-mixture)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "        bs = int(states.size(0))\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_items += bs\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{(total_loss / max(1,total_items)):.4f}\")\n",
    "\n",
    "    return total_loss / max(1, total_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b10f80-b85a-4d0f-b274-fb99cbbba21d",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e11a6d-7a65-4aa3-8055-3f6cf5de447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train: bool = True) -> float:\n",
    "    student.train(train)\n",
    "    total_loss = 0.0\n",
    "    total_items = 0\n",
    "\n",
    "    it = tqdm(loader, desc=(\"Train\" if train else \"Val\"), leave=False)\n",
    "    for states, targets, legal_mask in it:\n",
    "        states = states.to(DEVICE, non_blocking=True)            # (B,1,6,7)\n",
    "        targets = targets.to(DEVICE, non_blocking=True)          # (B,7)\n",
    "        legal_mask = legal_mask.to(DEVICE, non_blocking=True)    # (B,7) bool\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        out = student(states)\n",
    "        logits = out[0] if isinstance(out, (tuple, list)) else out  # (B,7)\n",
    "\n",
    "        # CRITICAL: prevent illegal-action leakage\n",
    "        logits = logits.masked_fill(~legal_mask, -1e9)\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = criterion(log_probs, targets)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(student.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "        bs = int(states.size(0))\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        total_items += bs\n",
    "        it.set_postfix(loss=f\"{(total_loss/max(1,total_items)):.4f}\")\n",
    "\n",
    "    return total_loss / max(1, total_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34152178-a7ef-4ffe-bae2-d312c5da205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- main training loop ----------\n",
    "\n",
    "meta_base = {\n",
    "    \"source\": \"UNION_PPO_distill\",\n",
    "    \"teachers\": list(HOF_METASCORES.keys()),\n",
    "    \"metascores\": HOF_METASCORES,\n",
    "    \"seed\": SEED,\n",
    "    \"num_samples\": len(distill_dataset),\n",
    "}\n",
    "\n",
    "best_val = math.inf\n",
    "best_path = Path(\"CNet192_UNION_PPO_best.pt\")\n",
    "\n",
    "# loss histories for plotting later\n",
    "train_loss_history = []\n",
    "val_loss_history   = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = run_epoch(train_loader, train=True,  desc=f\"Train {epoch}/{NUM_EPOCHS}\")\n",
    "    val_loss   = run_epoch(val_loader,   train=False, desc=f\"Val   {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f\"[Epoch {epoch:03d}]  train_loss={train_loss:.6f}   val_loss={val_loss:.6f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        meta = dict(meta_base)\n",
    "        meta[\"tag\"] = \"best_val\"\n",
    "        meta[\"epoch\"] = epoch\n",
    "        meta[\"best_val_loss\"] = float(best_val)\n",
    "        # NOTE: save_cnet192 expects path as first positional arg\n",
    "        save_cnet192(str(best_path), student, meta)\n",
    "        print(f\"  -> New best model saved to: {best_path}\")\n",
    "\n",
    "# also save final model (last epoch)\n",
    "final_path = Path(\"CNet192_UNION_PPO_last.pt\")\n",
    "final_meta = dict(meta_base)\n",
    "final_meta[\"tag\"] = \"final\"\n",
    "final_meta[\"epoch\"] = NUM_EPOCHS\n",
    "final_meta[\"best_val_loss\"] = float(best_val)\n",
    "save_cnet192(str(final_path), student, final_meta)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(\" Best model:\", best_path)\n",
    "print(\" Final model:\", final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6299c1a-e944-403d-bda7-f423d690e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(train_loss_history) + 1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(epochs, train_loss_history, marker=\"o\", label=\"Train KL\")\n",
    "plt.plot(epochs, val_loss_history, marker=\"o\", label=\"Val KL\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"KL(student || teacher)\")\n",
    "plt.title(\"CNet192 UNION distillation losses\")\n",
    "plt.grid(True, alpha=0.35)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
