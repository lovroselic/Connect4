{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1db620-d548-4056-b511-e9ac63c7dc6c",
   "metadata": {},
   "source": [
    "# Connect4 DQN model\n",
    "By LaughingSkull \n",
    "as new RL agent for my game COnnect-4: https://www.laughingskull.org/Games/Connect4/Connect4.php\n",
    "\n",
    "v 0.8.0 - start with new convolutional part<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56d151-4c55-4871-b5c6-5a91c75cab0a",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0589098-7517-4db7-bfb2-04a24f2d8e77",
   "metadata": {},
   "source": [
    "Training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb8661-d4cf-4cce-a79f-9100c87e6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_start_time = time.time()\n",
    "TRAINING_SESSION = \"RANDOM\"\n",
    "LOG_DIR =\"Logs/\"\n",
    "MODEL_DIR =\"Models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5dd4e-2529-4d7c-9e38-7bef1135eff8",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768e305-7069-4d06-9530-a2d5617560eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bde0a9-9c4e-4fd2-a7f2-237245aa3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4Env:\n",
    "    ROWS = 6\n",
    "    COLS = 7\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.ROWS, self.COLS), dtype=int)\n",
    "        self.current_player = 1\n",
    "        self.done = False\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board.copy()\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [c for c in range(self.COLS) if self.board[0][c] == 0]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done or self.board[0][action] != 0:\n",
    "            return self.get_state(), -10, True  # Illegal move penalty\n",
    "\n",
    "        # Drop piece in the selected column\n",
    "        for row in reversed(range(self.ROWS)):\n",
    "            if self.board[row][action] == 0:\n",
    "                self.board[row][action] = self.current_player\n",
    "                break\n",
    "\n",
    "        reward = 0\n",
    "        self.done, winner = self.check_game_over()\n",
    "        if self.done:\n",
    "            if winner == self.current_player:\n",
    "                reward = 1  # Win\n",
    "            elif winner == 0:\n",
    "                reward = 0.5  # Draw\n",
    "            else:\n",
    "                reward = -1  # Loss\n",
    "\n",
    "        self.current_player *= -1  # Switch turns\n",
    "        return self.get_state(), reward, self.done\n",
    "\n",
    "    def check_game_over(self):\n",
    "        # Horizontal\n",
    "        for r in range(self.ROWS):\n",
    "            for c in range(self.COLS - 3):\n",
    "                line = self.board[r, c:c+4]\n",
    "                if abs(np.sum(line)) == 4:\n",
    "                    return True, int(np.sign(np.sum(line)))\n",
    "\n",
    "        # Vertical\n",
    "        for r in range(self.ROWS - 3):\n",
    "            for c in range(self.COLS):\n",
    "                line = self.board[r:r+4, c]\n",
    "                if abs(np.sum(line)) == 4:\n",
    "                    return True, int(np.sign(np.sum(line)))\n",
    "\n",
    "        # Diagonal /\n",
    "        for r in range(3, self.ROWS):\n",
    "            for c in range(self.COLS - 3):\n",
    "                line = [self.board[r-i][c+i] for i in range(4)]\n",
    "                if abs(np.sum(line)) == 4:\n",
    "                    return True, int(np.sign(np.sum(line)))\n",
    "\n",
    "        # Diagonal \\\n",
    "        for r in range(self.ROWS - 3):\n",
    "            for c in range(self.COLS - 3):\n",
    "                line = [self.board[r+i][c+i] for i in range(4)]\n",
    "                if abs(np.sum(line)) == 4:\n",
    "                    return True, int(np.sign(np.sum(line)))\n",
    "\n",
    "        # Draw\n",
    "        if not np.any(self.board == 0):\n",
    "            return True, 0\n",
    "\n",
    "\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375d295-3210-4fa0-ba5c-f3c85a361d86",
   "metadata": {},
   "source": [
    " ## Neural Network for Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8133a5-b93e-42ef-bfb2-af29c7fb5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape=(6, 7), output_size=7):\n",
    "        super(DQN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = output_size\n",
    "\n",
    "        h, w = input_shape\n",
    "        padding = 1  # consistent padding\n",
    "\n",
    "        # Convolutional layers with BatchNorm\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  \n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "        # Estimate output size after conv+pool for FC input\n",
    "        dummy_input = torch.zeros(1, 1, *input_shape)\n",
    "        with torch.no_grad():\n",
    "            conv_out_size = self.conv(dummy_input).shape[1]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Shape: (batch, 1, 6, 7)\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42c8fc-51bd-4cb5-af36-0d21b93a6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = DQN()\n",
    "summary(_model, input_size=(1, 6, 7))  # batch height, width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab9760-5a2d-4c2c-844a-a980cd51c8b8",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d2c40-07e4-4671-8830-f01860736fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def prune(self, fraction=0.8):\n",
    "        if fraction > 1.0: return\n",
    "        prune_size = int(len(self.memory) * fraction)\n",
    "        if prune_size == 0: return\n",
    "        # Keep the most recent (len - prune_size) experiences\n",
    "        self.memory = deque(list(self.memory)[prune_size:], maxlen=self.memory.maxlen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24355bf3-76f9-497c-8690-5b9032c94d96",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759bcd9-8f57-4eb4-ad4b-bf36d29717cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, lr=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.999, device=None):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = DQN().to(self.device)\n",
    "        self.target_model = DQN().to(self.device)\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.memory = ReplayMemory()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        #self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-5) #L2 regularization\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def act(self, state, valid_actions, player, depth, strategy_weights):\n",
    "        choice = random.choices([0, 1, 2, 3, 4, 5], weights=strategy_weights)[0]\n",
    "        board = np.array(state)\n",
    "\n",
    "        # exploration: random or lookahead actions\n",
    "        if random.random() < self.epsilon:\n",
    "            if choice == 0:\n",
    "                return random.choice(valid_actions)\n",
    "            elif choice == 1:\n",
    "                return n_step_lookahead(board, player=player, depth=1) # L-1\n",
    "            elif choice == 2:\n",
    "                return n_step_lookahead(board, player=player, depth=3) # L-3\n",
    "            elif choice == 3:\n",
    "                return n_step_lookahead(board, player=player, depth=5) # L-5\n",
    "            elif choice == 4:\n",
    "                return n_step_lookahead(board, player=player, depth=max(1, depth - 1)) # N-1\n",
    "            elif choice == 5:\n",
    "                return n_step_lookahead(board, player=player, depth=depth) # N\n",
    "                \n",
    "        # exploatation: from replay\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "     \n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor).cpu().numpy()[0]\n",
    "\n",
    "        # Mask invalid actions\n",
    "        masked_q = np.full_like(q_values, -np.inf)\n",
    "        for a in valid_actions:\n",
    "            masked_q[a] = q_values[a]\n",
    "\n",
    "        return int(np.argmax(masked_q))\n",
    "\n",
    "    def remember(self, s, a, r, s2, done):\n",
    "        self.memory.push(s, a, r, s2, done)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, device=self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool, device=self.device)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "\n",
    "        # Double DQN: use model to select next actions, target_model to evaluate\n",
    "        next_actions = self.model(next_states).argmax(dim=1, keepdim=True)\n",
    "        next_q_values = self.target_model(next_states).gather(1, next_actions).squeeze()\n",
    "\n",
    "        # Compute targets\n",
    "        q_targets = q_values.clone()\n",
    "        for i in range(len(batch)):\n",
    "            q_targets[i][actions[i]] = rewards[i] + (0 if dones[i] else self.gamma * next_q_values[i])\n",
    "\n",
    "        loss = self.loss_fn(q_values, q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f902c4-6bc6-4d8f-abd3-53d6179e0c55",
   "metadata": {},
   "source": [
    "## N-step lookahead, ported from JS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5b303-7c27-43e6-970b-760f815f3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_legal_moves(board):\n",
    "    return [c for c in range(7) if board[0][c] == 0]\n",
    "\n",
    "def drop_piece(board, col, player):\n",
    "    new_board = board.copy()\n",
    "    for row in reversed(range(6)):\n",
    "        if new_board[row][col] == 0:\n",
    "            new_board[row][col] = player\n",
    "            break\n",
    "    return new_board\n",
    "\n",
    "def board_to_patterns(board, players):\n",
    "    patterns = []\n",
    "    # Horizontal\n",
    "    for r in range(6):\n",
    "        for c in range(4):\n",
    "            window = board[r, c:c+4]\n",
    "            if any(val in players for val in window):\n",
    "                patterns.append(window)\n",
    "    # Vertical\n",
    "    for r in range(3):\n",
    "        for c in range(7):\n",
    "            window = board[r:r+4, c]\n",
    "            if any(val in players for val in window):\n",
    "                patterns.append(window)\n",
    "    # Diagonal \\\n",
    "    for r in range(3):\n",
    "        for c in range(4):\n",
    "            window = np.array([board[r+i][c+i] for i in range(4)])\n",
    "            if any(val in players for val in window):\n",
    "                patterns.append(window)\n",
    "    # Diagonal /\n",
    "    for r in range(3):\n",
    "        for c in range(3, 7):\n",
    "            window = np.array([board[r+i][c-i] for i in range(4)])\n",
    "            if any(val in players for val in window):\n",
    "                patterns.append(window)\n",
    "    return patterns\n",
    "\n",
    "def count_windows_in_pattern(patterns, count, player):\n",
    "    return sum(1 for w in patterns if np.count_nonzero(w == player) == count and np.count_nonzero(w == 0) == 4 - count)\n",
    "\n",
    "def get_heuristic(board, player):\n",
    "    opponent = -player\n",
    "    patterns = board_to_patterns(board, [player, opponent])\n",
    "    score = 0\n",
    "    weights = {2: 10, 3: 100, 4: 1000}\n",
    "    for n in [2, 3, 4]:\n",
    "        my_count = count_windows_in_pattern(patterns, n, player)\n",
    "        opp_count = count_windows_in_pattern(patterns, n, opponent)\n",
    "        score += weights[n] * (my_count - 1.5 * opp_count)\n",
    "    return score\n",
    "\n",
    "def is_terminal_node(board):\n",
    "    patterns = board_to_patterns(board, [1, -1])\n",
    "    return count_windows_in_pattern(patterns, 4, 1) > 0 or count_windows_in_pattern(patterns, 4, -1) > 0 or np.all(board != 0)\n",
    "\n",
    "def minimax(board, depth, maximizing, player, alpha, beta):\n",
    "    if depth == 0 or is_terminal_node(board):\n",
    "        return get_heuristic(board, player)\n",
    "\n",
    "    valid_moves = get_legal_moves(board)\n",
    "    if maximizing:\n",
    "        value = -np.inf\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(board, col, player)\n",
    "            value = max(value, minimax(child, depth - 1, False, player, alpha, beta))\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta:\n",
    "                break\n",
    "        return value\n",
    "    else:\n",
    "        value = np.inf\n",
    "        opponent = -player\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(board, col, opponent)\n",
    "            value = min(value, minimax(child, depth - 1, True, player, alpha, beta))\n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return value\n",
    "\n",
    "def n_step_lookahead(board, player, depth=3):\n",
    "    valid_moves = get_legal_moves(board)\n",
    "    scores = {}\n",
    "    for move in valid_moves:\n",
    "        new_board = drop_piece(board, move, player)\n",
    "        score = minimax(new_board, depth - 1, False, player, -np.inf, np.inf)\n",
    "        scores[move] = score\n",
    "\n",
    "    max_score = max(scores.values())\n",
    "    best_moves = [m for m, s in scores.items() if s == max_score]\n",
    "    center = 3\n",
    "    best_moves.sort(key=lambda x: abs(center - x))  # prefer center columns\n",
    "    return best_moves[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d81c26-194c-4b97-8578-4333d99f85bc",
   "metadata": {},
   "source": [
    "## Training loop - DQN against lookahead opponent (Prophet-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121fe38-6bba-4a8e-9123-442e110add5f",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41419f73-e91d-4a64-a71e-721346a4dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead_depth = 5  # prophet = 7\n",
    "\n",
    "# num_episodes = 7000\n",
    "# num_episodes = 300 # - shortened for debug\n",
    "batch_size = 64\n",
    "target_update_interval = 10\n",
    "log_every_x_episode = 200\n",
    "\n",
    "TRAINING_PHASE = {\n",
    "    \"Random\": 250,\n",
    "    \"Mixed\": 500, #250\n",
    "    \"Mixed2\": 1000, #500 \n",
    "    \"VeryShallow\": 1500, #500  L-1\n",
    "    \"SelfPlay_Begin\": 1750, #250\n",
    "    \"Shallow\": 2250, #500  RND(1,2)\n",
    "    \"Variable3\": 2750, #500 RND(1,N)\n",
    "    \"Fixed2\": 3250, #500 L-2\n",
    "    \"SelfPlay_L2\": 3500, #250 \n",
    "    \"Variable23\": 4000, #500 RND(2,N)\n",
    "    \"SelfPlay_L3\": 4250, #250\n",
    "    \"Fixed3\": 4750, #500 l-3\n",
    "    \"Fixed4\": 5250, #500 L-4\n",
    "    \"SelfPlay_L4\": 5500, #250\n",
    "    \"Variable35\": 6000, #500 RND(3,N)\n",
    "    \"Full\": 6500, #500 L-N\n",
    "    \"Final\": None, #final 500 self play\n",
    "}\n",
    "\n",
    "ACT_W = {\n",
    "    # [R, L1, L3, L5, N-1, N]\n",
    "    \"Random\": [0.75, 0.25, 0, 0, 0, 0],\n",
    "    \"Mixed\": [0.5, 0.5, 0, 0, 0, 0],\n",
    "    \"Mixed2\": [0.4, 0.6, 0, 0, 0, 0], \n",
    "    \"VeryShallow\": [0.4, 0.5, 0.1, 0, 0, 0], \n",
    "    \"SelfPlay_Begin\": [0.4, 0.5, 0.1, 0, 0, 0], \n",
    "    \"Shallow\": [0.4, 0.3, 0.3, 0, 0, 0],\n",
    "    \"Variable3\": [0.3, 0.3, 0.4, 0, 0, 0],\n",
    "    \"Fixed2\": [0.3, 0.25, 0.45, 0, 0, 0],\n",
    "    \"SelfPlay_L2\": [0.3, 0.25, 0.45, 0, 0, 0],\n",
    "    \"Variable23\": [0.25, 0.15, 0.5, 0.1, 0, 0],\n",
    "    \"SelfPlay_L3\": [0.25, 0.15, 0.5, 0.1, 0, 0],\n",
    "    \"Fixed3\": [0.2, 0.1, 0.5, 0.2, 0, 0],\n",
    "    \"Fixed4\": [0.2, 0.1, 0.4, 0.3, 0, 0],\n",
    "    \"SelfPlay_L4\": [0.2, 0.1, 0.4, 0.3, 0, 0],\n",
    "    \"Variable35\": [0.2, 0.1, 0.2, 0.5, 0, 0],\n",
    "    \"Full\": [0.2, 0.1, 0.2, 0.5, 0, 0], # amend for prophet\n",
    "    \"Final\": [0.2, 0.1, 0.2, 0.5, 0, 0], # amend for prophet\n",
    "}\n",
    "\n",
    "EPSILON_PHASE_RESET = {\n",
    "    \"Random\": 1.0,\n",
    "    \"Mixed\": 0.9,\n",
    "    \"Mixed2\": 0.5,\n",
    "    \"VeryShallow\": 0.5,\n",
    "    \"SelfPlay_Begin\": 0.8,\n",
    "    \"Shallow\": 0.85,\n",
    "    \"Variable3\": 0.8,\n",
    "    \"Fixed2\": 0.5,\n",
    "    \"SelfPlay_L2\": 0.75,\n",
    "    \"Variable23\": 0.8,\n",
    "    \"SelfPlay_L3\": 0.7,\n",
    "    \"Fixed3\": 0.6,\n",
    "    \"Fixed4\": 0.8,\n",
    "    \"SelfPlay_L4\": 0.8,\n",
    "    \"Variable35\": 0.8,\n",
    "    \"Full\": 0.75,\n",
    "    \"Final\": 0.7,\n",
    "}\n",
    "\n",
    "PHASE_MEMORY_PRUNE = {\n",
    "    \"Random\": 0.2,\n",
    "    \"Mixed\": 0.5,\n",
    "    \"Mixed2\": 0.2,\n",
    "    \"VeryShallow\": 0.5,\n",
    "    \"SelfPlay_Begin\": 0.2,\n",
    "    \"Shallow\": 0.1,\n",
    "    \"Variable3\": 0.1,\n",
    "    \"Fixed2\": 0.1,\n",
    "    \"SelfPlay_L2\": 0.1,\n",
    "    \"Variable23\": 0.1,\n",
    "    \"SelfPlay_L3\": 0.1,\n",
    "    \"Fixed3\": 0.1,\n",
    "    \"Fixed4\": 0.1,\n",
    "    \"SelfPlay_L4\": 0.05,\n",
    "    \"Variable35\": 0.1,\n",
    "    \"Full\": 0.0,\n",
    "    \"Final\": 0.0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a266e5-de42-4d8f-9e56-1b56b93baad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opponent_action(env, episode, next_state, player, depth):\n",
    "    valid_actions = env.available_actions()\n",
    "    \n",
    "    if episode < TRAINING_PHASE[\"Random\"]:\n",
    "        opp_action = random.choice(env.available_actions())\n",
    "        \n",
    "    elif episode < TRAINING_PHASE[\"Mixed\"]:\n",
    "        if random.randint(0,1) == 0:\n",
    "            opp_action = random.choice(env.available_actions())\n",
    "        else:\n",
    "            opp_action = n_step_lookahead(next_state, player=-1, depth=1)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"Mixed2\"]:\n",
    "        if random.randint(0,2) == 0:\n",
    "            opp_action = random.choice(env.available_actions())\n",
    "        else:\n",
    "            opp_action = n_step_lookahead(next_state, player=-1, depth=1)\n",
    "        \n",
    "    elif episode < TRAINING_PHASE[\"VeryShallow\"]:\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=1)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"SelfPlay_Begin\"]:\n",
    "        opp_action = agent.act(next_state, valid_actions, player=-1, depth=depth)\n",
    "        \n",
    "    elif episode < TRAINING_PHASE[\"Shallow\"]:\n",
    "        shallow_depth = random.randint(1, 2)\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=shallow_depth)\n",
    "    \n",
    "    elif episode < TRAINING_PHASE[\"Variable3\"]:\n",
    "        variable_depth = random.randint(1, 3)\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=variable_depth)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"Fixed2\"]:\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=2)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"SelfPlay_L2\"]:\n",
    "        opp_action = agent.act(next_state, valid_actions, player=-1, depth=depth)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"Variable23\"]:\n",
    "        variable_depth = random.randint(2, 3)\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=variable_depth)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"SelfPlay_L3\"]:\n",
    "        opp_action = agent.act(next_state, valid_actions, player=-1, depth=depth)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"Fixed3\"]:\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=3)\n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"Fixed4\"]:\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=4) \n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"SelfPlay_L4\"]:\n",
    "        opp_action = agent.act(next_state, valid_actions, player=-1, depth=depth)\n",
    "        \n",
    "\n",
    "    elif episode < TRAINING_PHASE[\"Variable35\"]:\n",
    "        variable_depth = random.randint(3, 5)\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=variable_depth)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    elif episode < TRAINING_PHASE[\"Full\"]:\n",
    "        #Full N -step lookahead\n",
    "        opp_action = n_step_lookahead(next_state, player=-1, depth=depth)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # Final self play\n",
    "        opp_action = agent.act(next_state, valid_actions, player=-1, depth=depth)\n",
    "       \n",
    "\n",
    "    return opp_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa44b92-350d-4733-b718-5f2bdec9f0c9",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2447563-415a-437c-aa19-9dc9b9ad4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase(episode):\n",
    "    for phase_name, end_episode in TRAINING_PHASE.items():\n",
    "        if end_episode is None or episode < end_episode:\n",
    "            return phase_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334f9db-3bdb-432c-96c4-b1db012333eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = {}  # store stats\n",
    "\n",
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "env = Connect4Env()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "agent = DQNAgent(device=device)\n",
    "\n",
    "reward_history = []\n",
    "win_history = []\n",
    "epsilon_history = []\n",
    "win_count = loss_count = draw_count = 0\n",
    "phase = \"Random\"  \n",
    "strategy_weights = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=num_episodes, desc=\"Training Episodes\") as pbar:\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_actions = env.available_actions()\n",
    "            strategy_weights = ACT_W.get(phase, [1, 1, 1, 1, 1, 1])\n",
    "            action = agent.act(state, valid_actions, player=env.current_player, depth=lookahead_depth, strategy_weights=strategy_weights)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            if not done:\n",
    "                opp_action = get_opponent_action(env, episode, next_state, player=-1, depth=lookahead_depth)\n",
    "                next_state, reward, done = env.step(opp_action)\n",
    "                if done:\n",
    "                    reward = -1\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        reward_history.append(total_reward)\n",
    "        reward = round(reward, 1)\n",
    "\n",
    "        if reward == 1:\n",
    "            win_count += 1\n",
    "            win_history.append(1)\n",
    "        elif reward == -1:\n",
    "            loss_count += 1\n",
    "            win_history.append(0)\n",
    "        elif reward == 0.5:\n",
    "            draw_count += 1\n",
    "            win_history.append(0)\n",
    "\n",
    "        if episode % target_update_interval == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        new_phase = get_phase(episode)\n",
    "\n",
    "        # phase reaction\n",
    "        if new_phase != phase:\n",
    "            agent.epsilon = EPSILON_PHASE_RESET.get(new_phase, agent.epsilon)\n",
    "            if PHASE_MEMORY_PRUNE.get(new_phase, False):\n",
    "                agent.memory.prune(PHASE_MEMORY_PRUNE[new_phase])\n",
    "\n",
    "\n",
    "            phase = new_phase\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(reward_history[-25:])\n",
    "            pbar.set_postfix(avg_reward=f\"{avg_reward:.2f}\",\n",
    "                             epsilon=f\"{agent.epsilon:.3f}\",\n",
    "                             wins=win_count,\n",
    "                             losses=loss_count,\n",
    "                             draws=draw_count,\n",
    "                             phase=phase)\n",
    "\n",
    "            # Live plot\n",
    "            clear_output(wait=True)\n",
    "            fig, ax = plt.subplots(3, 1, figsize=(10, 9), sharex=True)\n",
    "            \n",
    "            # Reward plot\n",
    "            ax[0].plot(reward_history, label='Reward')\n",
    "            if len(reward_history) >= 25:\n",
    "                avg = np.convolve(reward_history, np.ones(25)/25, mode='valid')\n",
    "                ax[0].plot(range(24, len(reward_history)), avg, label='25-ep Moving Avg', linewidth=2)\n",
    "            ax[0].set_ylabel('Total Reward')\n",
    "            ax[0].legend()\n",
    "            ax[0].grid(True)\n",
    "            \n",
    "            # Win rate plot\n",
    "            if len(win_history) >= 25:\n",
    "                win_avg = np.convolve(win_history, np.ones(25)/25, mode='valid')\n",
    "                ax[1].plot(range(24, len(win_history)), win_avg, label='Win Rate (25 ep)', color='green')\n",
    "            ax[1].set_ylabel('Win Rate')\n",
    "            if len(ax[1].lines) > 0:\n",
    "                ax[1].legend()\n",
    "            ax[1].grid(True)\n",
    "            \n",
    "            # Epsilon plot\n",
    "            ax[2].plot(epsilon_history, label='Epsilon', color='orange')\n",
    "            ax[2].set_xlabel('Episode')\n",
    "            ax[2].set_ylabel('Epsilon')\n",
    "            ax[2].legend()\n",
    "            ax[2].grid(True)\n",
    "\n",
    "            fig.suptitle(f\"Episode {episode} — Phase: {phase} | Wins: {win_count}, Losses: {loss_count}, Draws: {draw_count} | ε={agent.epsilon:.3f}\")\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "\n",
    "            # Draw vertical phase transition lines with labels\n",
    "            for name, transition_ep in TRAINING_PHASE.items():\n",
    "                if transition_ep is not None and transition_ep <= episode:\n",
    "                    for axis in ax:\n",
    "                        axis.axvline(transition_ep, color='black', linestyle='dotted', linewidth=1)\n",
    "                        axis.text(transition_ep + 2, axis.get_ylim()[1]*0.95, name, rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "            \n",
    "            display(fig)\n",
    "            plt.close()\n",
    "\n",
    "        # Every 50 episodes, store summary\n",
    "        if episode % log_every_x_episode == 0:\n",
    "            recent_rewards = reward_history[-25:] if len(reward_history) >= 25 else reward_history\n",
    "            recent_win_rate = np.mean(win_history[-25:]) if len(win_history) >= 25 else np.mean(win_history)\n",
    "            summary_stats[episode] = {\n",
    "                \"phase\": phase,\n",
    "                \"strategy_weights\": strategy_weights.copy(),\n",
    "                \"wins\": win_count,\n",
    "                \"losses\": loss_count,\n",
    "                \"draws\": draw_count,\n",
    "                \"avg_reward_25\": round(np.mean(recent_rewards), 2),\n",
    "                \"win_rate_25\": round(recent_win_rate, 2)\n",
    "            }\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes ({elapsed / num_episodes:.2f} s/episode)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216ccf7-ac10-48ba-8cf3-c01b1586e6e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nSummary stats (every {log_every_x_episode} episodes):\")\n",
    "pprint.pprint(summary_stats)\n",
    "pd.DataFrame.from_dict(summary_stats, orient='index').to_csv(f\"{LOG_DIR}{TRAINING_SESSION}training_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb65df-c8fe-428f-8ea5-d1749e669815",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Progress\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ebe25-6a98-4fa3-83ed-706bfa90e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f636ff5-a35d-4aa3-bbfd-389616cdc3b9",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bdd1a-62a5-4fc6-b191-d8927b1da9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 dqn_model_{timestamp} episodes-{num_episodes} lookahead-{lookahead_depth}.pt\"\n",
    "default_model_path = \"Connect4 DQN model.pt\"\n",
    "\n",
    "torch.save(agent.model.state_dict(), model_path)\n",
    "torch.save(agent.model.state_dict(), default_model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c93c4b-80f3-4e23-9e2d-1f01f9112d8d",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87360d37-f8d9-48e2-935a-660863a3da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(device=device)  # Fresh agent instance\n",
    "state_dict = torch.load(default_model_path, map_location=device, weights_only=True)\n",
    "agent.model.load_state_dict(state_dict)\n",
    "agent.update_target_model()\n",
    "agent.epsilon = 0.0  # Fully greedy — no exploration\n",
    "print(\"✅ Trained model loaded and ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31153e6e-79a6-4106-af92-b6cac6b6f834",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb37c78-44b6-47d8-8fc3-75b12780f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION CONFIGURATION ===\n",
    "evaluation_opponents = {\n",
    "    \"Random\": 50,\n",
    "    \"Lookahead-1\": 50,\n",
    "    \"Lookahead-3\": 25,\n",
    "    \"Lookahead-5\": 10,\n",
    "    \"Lookahead-7\": 5\n",
    "}\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "evaluation_results = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for label, num_games in evaluation_opponents.items():\n",
    "    wins = losses = draws = 0\n",
    "    depth = int(label.split(\"-\")[1]) if label.startswith(\"Lookahead\") else None\n",
    "\n",
    "    with tqdm(total=num_games, desc=f\"Opponent: {label}\") as pbar:\n",
    "        for _ in range(num_games):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            agent_first = random.choice([True, False])\n",
    "\n",
    "            while not done:\n",
    "                if (env.current_player == 1 and agent_first) or (env.current_player == -1 and not agent_first):\n",
    "                    valid_actions = env.available_actions()\n",
    "                    action = agent.act(state, valid_actions, player=env.current_player, depth=lookahead_depth)\n",
    "                else:\n",
    "                    if label == \"Random\":\n",
    "                        action = random.choice(env.available_actions())\n",
    "                    else:\n",
    "                        board = np.array(state)\n",
    "                        action = n_step_lookahead(board, env.current_player, depth=depth)\n",
    "            \n",
    "                state, reward, done = env.step(action)\n",
    "\n",
    "\n",
    "            # Determine winner\n",
    "            winner = -env.current_player\n",
    "            if winner == 1:\n",
    "                if agent_first:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "            elif winner == -1:\n",
    "                if not agent_first:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    evaluation_results[label] = {\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"draws\": draws,\n",
    "        \"win_rate\": round(wins / num_games, 2),\n",
    "        \"loss_rate\": round(losses / num_games, 2),\n",
    "        \"draw_rate\": round(draws / num_games, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\Evaluation completed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "# === Print Summary ===\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "for label, stats in evaluation_results.items():\n",
    "    print(f\"{label}: {stats['wins']}W / {stats['losses']}L / {stats['draws']}D → \"\n",
    "          f\"Win Rate: {stats['win_rate']*100:.1f}%, Loss: {stats['loss_rate']*100:.1f}%, Draws: {stats['draw_rate']*100:.1f}%\")\n",
    "\n",
    "# === Bar Plot Summary ===\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates = [evaluation_results[k]['win_rate'] * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %', color='green')\n",
    "plt.bar([i + bar_width for i in x], loss_rates, width=bar_width, label='Loss %', color='red')\n",
    "plt.bar([i + 2 * bar_width for i in x], draw_rates, width=bar_width, label='Draw %', color='gray')\n",
    "\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('DQN Agent Performance vs Various Opponents')\n",
    "plt.xticks([i + bar_width for i in x], labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert evaluation results to DataFrame\n",
    "df_eval = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "df_eval.index.name = \"Opponent\"\n",
    "\n",
    "# Save to CSV\n",
    "df_eval.to_csv(f\"{LOG_DIR}{TRAINING_SESSION}-evaluation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b01784-2970-4c87-a91b-ca40a635ab0d",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf946-a47a-427e-8aa9-ccc9c844a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = total_end_time - begin_start_time\n",
    "print(f\"\\Evaluation completed in {total_elapsed/60/60:.1f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dqn-gpu]",
   "language": "python",
   "name": "conda-env-dqn-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
