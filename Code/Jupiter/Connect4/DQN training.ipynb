{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1db620-d548-4056-b511-e9ac63c7dc6c",
   "metadata": {},
   "source": [
    "# Connect4 DQN model\n",
    "By LaughingSkull \n",
    "as new RL agent for my game COnnect-4: https://www.laughingskull.org/Games/Connect4/Connect4.php\n",
    "\n",
    "\n",
    "## DQN (Deep Q-Network)\n",
    "\n",
    "* A value-based reinforcement learning method.\n",
    "* Uses a neural network to approximate the Q-function: estimates the expected future rewards for taking actions in given states.\n",
    "* Learns via Q-learning: updates the Q-values using the Bellman equation.\n",
    "* Typically uses techniques like experience replay and target networks to stabilize training.\n",
    "\n",
    "### version log    \n",
    "\n",
    "* 0.8.0 - start with new convolutional part\n",
    "    * keeping L2 regularization\n",
    "* 0.9.0 - extension to shallow training\n",
    "* 0.10.0 - extension to L3 training\n",
    "    * NO PRUNE:\n",
    "        * worse \n",
    "    * SHARPer PRUNE\n",
    "    * extend phases\n",
    "        * MixedR12: no improvement; &cross;\n",
    "        * Shallow\n",
    "        * Fixed2\n",
    "        * Variable23\n",
    "        * Variable3\n",
    "* 0.11.0 - recover fixed 2\n",
    "    * corrected reward leaking bug\n",
    "    * corrected draw bug\n",
    "* 0.12.0 refactoring and restarting\n",
    "* 0.13 restarting again\n",
    "    * strategy weights djustment\n",
    "* 0.14 new restart from scratch\n",
    "    * intermediate phases\n",
    "* 0.15 shaped rewards\n",
    "* 0.16 testing target_update_interval\n",
    "* 0.17 changing CNN\n",
    "* 0.18 8-part weights approach, rebalancing and restarting (again)\n",
    "    * frozen model for self play\n",
    "    * adding mirrored rewards\n",
    "    * adding player POV then getting rid of it\n",
    "        * splitting agent  and oppo players channel\n",
    "* 0.19 restart with tweaking weights (again)\n",
    "    * new ENV step weights\n",
    "    * added forking and fork blocking\n",
    "    * included forks in lookahead\n",
    "* 0.20 adding PER - prioritized experience replay\n",
    "    * retuning phases and weights (yet again)\n",
    "    * adding immediate win penalty in env\n",
    "* 0.21 3-step returns\n",
    "* 0.22 tactical guard\n",
    "    * random 10%\n",
    "    * changed evaluation\n",
    "    * restart\n",
    "    * rewards less defensive as before, but more as attack\n",
    "    * corrected pov in n-step\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56d151-4c55-4871-b5c6-5a91c75cab0a",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "[https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/](https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "<br>\n",
    "[https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68](https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68)\n",
    "<br>\n",
    "[https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html](https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html)\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5dd4e-2529-4d7c-9e38-7bef1135eff8",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768e305-7069-4d06-9530-a2d5617560eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced052f-8ccc-4e0f-a8cd-dcf6c27a7446",
   "metadata": {},
   "source": [
    "### Fixed Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36059b-73c8-4356-ba7a-037473dca494",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653a09a-6263-4cdf-ad4d-ca960a72645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR =\"Logs/DQN/\"\n",
    "MODEL_DIR =\"Models/DQN/\"\n",
    "PLOTS = \"Plots/DQN/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f9655-bbba-4950-9260-548b64261ac9",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b037ed-2332-4851-ae94-945d511deeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4.connect4_env import Connect4Env\n",
    "from C4.connect4_lookahead import Connect4Lookahead\n",
    "from DQN.training_phases_config import TRAINING_PHASES\n",
    "from DQN.training_phases_config import set_training_phases_length\n",
    "from DQN.opponent_action import get_opponent_action\n",
    "from DQN.DQN_replay_memory_per import PrioritizedReplayMemory\n",
    "from DQN.dqn_model import DQN\n",
    "from DQN.dqn_agent import DQNAgent\n",
    "from DQN.dqn_utilities import *\n",
    "from C4.connect4_board_display import Connect4_BoardDisplayer\n",
    "from C4.plot_phase_summary import plot_phase_summary\n",
    "from DQN.nstep_buffer import NStepBuffer\n",
    "from DQN.eval_utilities import build_input, evaluate_with_leaks\n",
    "from DQN.TD_error import *\n",
    "\n",
    "Lookahead = Connect4Lookahead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024c11d-10d5-4f35-bb88-5b822e3551b0",
   "metadata": {},
   "source": [
    "# Training phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597e001-442c-49ce-934d-7577b778e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_training_phases_length(TRAINING_PHASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0589098-7517-4db7-bfb2-04a24f2d8e77",
   "metadata": {},
   "source": [
    "# Training session name and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb8661-d4cf-4cce-a79f-9100c87e6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead_depth = 7  # prophet = 7\n",
    "start_episode = 0\n",
    "\n",
    "num_episodes = 3600\n",
    "\n",
    "batch_size = 128\n",
    "# target_update_interval = 25\n",
    "target_update_interval = 200\n",
    "plot_interval = 10\n",
    "log_every_x_episode = 100\n",
    "tag = \"SelfPlay_Mixed_R11  V3\"\n",
    "\n",
    "TRAINING_SESSION = f\"{tag}-{num_episodes}-TU-{target_update_interval}-BS-{batch_size}\"\n",
    "begin_start_time = time.time()\n",
    "\n",
    "print(\"Started training session\", TRAINING_SESSION)\n",
    "\n",
    "#DEV\n",
    "PLOT_TD = True\n",
    "num_episodes -= 1 # debug\n",
    "num_episodes = max(num_episodes, 101) #debug  safeguard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75673c-d12f-4749-947c-bbf08cf5a2cb",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b1923-c3e5-49b5-9d35-d31d37263051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save training configuration to Excel ---\n",
    "from C4.training_config_logger import export_training_config\n",
    "\n",
    "paths = export_training_config(\n",
    "    training_phases=TRAINING_PHASES,\n",
    "    lookahead_depth=lookahead_depth,\n",
    "    num_episodes=num_episodes,\n",
    "    batch_size=batch_size,\n",
    "    target_update_interval=target_update_interval,\n",
    "    log_dir=LOG_DIR,\n",
    "    session_name=TRAINING_SESSION,\n",
    "    write_excel=True,    # set False if you only want CSV/JSON\n",
    "    write_json=False,     # handy for exact reproduction later\n",
    ")\n",
    "\n",
    "print(\"config written:\", paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59762c43-70fe-4a7b-9e8e-d5da65d7e628",
   "metadata": {},
   "source": [
    "### Model overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42c8fc-51bd-4cb5-af36-0d21b93a6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = DQN()\n",
    "summary(_model, input_size=(1, 2, 6, 7))  # batch=1, channels=2, height=6, width=7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d81c26-194c-4b97-8578-4333d99f85bc",
   "metadata": {},
   "source": [
    "## Training loop - DQN against lookahead opponent (Prophet-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa44b92-350d-4733-b718-5f2bdec9f0c9",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334f9db-3bdb-432c-96c4-b1db012333eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training loop with 3-step returns into PER ===\n",
    "\n",
    "summary_stats = {}  \n",
    "env = Connect4Env()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "agent = DQNAgent(device=device)\n",
    "\n",
    "N_STEP = 3\n",
    "nstep_buf = NStepBuffer(n=N_STEP, gamma=agent.gamma, memory=agent.memory)\n",
    "\n",
    "reward_history = []\n",
    "win_history = []\n",
    "epsilon_history = []\n",
    "epsilon_min_history = []\n",
    "memory_prune_history = []\n",
    "memory_prune_low_history = []\n",
    "\n",
    "win_count = loss_count = draw_count = 0\n",
    "phase = None\n",
    "frozen_opp = None\n",
    "strategy_weights = []\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=num_episodes, desc=\"Training Episodes\") as pbar:\n",
    "    for episode in range(start_episode + 1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        nstep_buf.reset()  # important: clear rolling window per episode\n",
    "\n",
    "        total_reward = 0      # terminal reward only\n",
    "        done = False\n",
    "        final_result = None   # 1 win, -1 loss, 0.5 draw\n",
    "\n",
    "        # --- handle phase & hyperparams ---\n",
    "        new_phase, strategy_weights, epsilon, memory_prune_recent, memory_prune_low, epsilon_min = get_phase(episode)\n",
    "        phase, frozen_opp = handle_phase_change(\n",
    "            agent, new_phase, phase, epsilon,\n",
    "            memory_prune_recent, memory_prune_low, epsilon_min, frozen_opp\n",
    "        )\n",
    "\n",
    "        # --------- main training loop ---------\n",
    "        # Optional random opponent start (player -1)\n",
    "        #if random.random() < 0.5 and not done:\n",
    "        if episode % 2 == 1:\n",
    "            s0 = state\n",
    "            opp_action = get_opponent_action(env, agent, episode, s0, player=-1,\n",
    "                                             depth=lookahead_depth, frozen_opp=frozen_opp, phase=phase)\n",
    "            s1, r_opp, done = env.step(opp_action)\n",
    "            # append the opponent step\n",
    "            nstep_buf.append(s0, opp_action, r_opp, s1, done, player=-1)\n",
    "            state = s1\n",
    "\n",
    "        # Turn-by-turn until terminal\n",
    "        while not done:\n",
    "            # --- Agent (+1) acts ---\n",
    "            valid_actions = env.available_actions()\n",
    "            action = agent.act(state, valid_actions, player=+1, depth=lookahead_depth, strategy_weights=strategy_weights)\n",
    "            next_state, r_agent, done = env.step(action)\n",
    "\n",
    "            # append agent step\n",
    "            nstep_buf.append(state, action, r_agent, next_state, done, player=+1)\n",
    "\n",
    "            if done:\n",
    "                # terminal on agent move\n",
    "                final_result = evaluate_final_result(env, agent_player=+1)\n",
    "                total_reward = map_final_result_to_reward(final_result)\n",
    "                state = next_state\n",
    "                break\n",
    "\n",
    "            # --- Opponent (−1) responds ---\n",
    "            opp_action = get_opponent_action(env, agent, episode, next_state, player=-1,\n",
    "                                             depth=lookahead_depth, frozen_opp=frozen_opp, phase=phase)\n",
    "            next_state2, r_opp, done = env.step(opp_action)\n",
    "\n",
    "            # append opponent step\n",
    "            nstep_buf.append(next_state, opp_action, r_opp, next_state2, done, player=-1)\n",
    "\n",
    "            if done:\n",
    "                final_result = evaluate_final_result(env, agent_player=+1)\n",
    "                total_reward = map_final_result_to_reward(final_result)\n",
    "                state = next_state2\n",
    "                break\n",
    "\n",
    "            # continue\n",
    "            state = next_state2\n",
    "\n",
    "        # Episode ended — flush remaining short tails so they also yield n-step targets\n",
    "        nstep_buf.flush()\n",
    "\n",
    "        # One (or a few) optimization steps\n",
    "        WARMUP = max(batch_size * 5, 2000)\n",
    "        UPDATES_PER_EP = 4\n",
    "        if len(agent.memory) >= WARMUP:\n",
    "            for _ in range(UPDATES_PER_EP):\n",
    "                agent.replay(batch_size, mix_1step=0.6)  # try 0.5–0.7\n",
    "        elif len(agent.memory) >= WARMUP // 2:\n",
    "            agent.replay(max(batch_size // 2, 32), mix_1step=0.8)\n",
    "\n",
    "\n",
    "        # --------- bookkeeping ---------\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        epsilon_min_history.append(agent.epsilon_min)\n",
    "        reward_history.append(total_reward)\n",
    "        memory_prune_history.append(memory_prune_recent)\n",
    "        memory_prune_low_history.append(memory_prune_low)\n",
    "\n",
    "        # Epsilon decay (per episode)\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "            agent.epsilon = max(agent.epsilon, agent.epsilon_min)\n",
    "\n",
    "        # Win/loss/draw counters\n",
    "        wins, losses, draws = track_result(final_result, win_history)\n",
    "        win_count += wins\n",
    "        loss_count += losses\n",
    "        draw_count += draws\n",
    "\n",
    "        # Target network sync\n",
    "        if episode % target_update_interval == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        # Live plot\n",
    "        if episode % plot_interval == 0:\n",
    "            avg_reward = np.mean(reward_history[-25:]) if len(reward_history) >= 1 else 0.0\n",
    "            pbar.set_postfix(avg_reward=f\"{avg_reward:.2f}\", epsilon=f\"{agent.epsilon:.3f}\",\n",
    "                             wins=win_count, losses=loss_count, draws=draw_count, phase=phase)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            # optional board show:\n",
    "            # Connect4_BoardDisplayer.display_board(state)\n",
    "            plot_live_training(\n",
    "                episode, reward_history, win_history, epsilon_history,\n",
    "                phase, win_count, loss_count, draw_count, TRAINING_SESSION,\n",
    "                memory_prune_history, epsilon_min_history, memory_prune_low_history\n",
    "            )\n",
    "\n",
    "            if PLOT_TD:\n",
    "                plot_td_error_hist(agent)\n",
    "                plot_td_running(agent, window=1000)\n",
    "                plot_is_weights(agent)\n",
    "                #check_per_priority_correlation(agent, sample=1024)\n",
    "                #show_sample_mix(agent)\n",
    "\n",
    "        # Periodic logging\n",
    "        if episode % log_every_x_episode == 0:\n",
    "            log_summary_stats(\n",
    "                episode=episode, reward_history=reward_history, win_history=win_history, phase=phase,\n",
    "                strategy_weights=strategy_weights, agent=agent, win_count=win_count, loss_count=loss_count,\n",
    "                draw_count=draw_count, summary_stats_dict=summary_stats\n",
    "            )\n",
    "            \n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes ({elapsed / num_episodes:.2f} s/episode)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd75ba-a7ac-4694-9550-a280aeeb672d",
   "metadata": {},
   "source": [
    "## Final training plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e429a-62e8-4d22-b439-2b7ecb829bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_live_training(episode, reward_history, win_history, epsilon_history, \n",
    "                               phase, win_count, loss_count, draw_count, TRAINING_SESSION, memory_prune_history,\n",
    "                              epsilon_min_history, memory_prune_low_history,\n",
    "                   save = True, path= PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc5d38-d108-40a3-804a-2c679f54bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save final Win Rate plot ---\n",
    "save_final_winrate_plot(win_history=win_history, training_phases=TRAINING_PHASES, save_path=PLOTS, session_name=TRAINING_SESSION)\n",
    "print(f\"Win rate plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_winrate.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216ccf7-ac10-48ba-8cf3-c01b1586e6e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nSummary stats (every {log_every_x_episode} episodes):\")\n",
    "pd.DataFrame.from_dict(summary_stats, orient='index').to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-training_summary.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2badbf-dabe-4008-8c45-391a67652a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_phase_summary(summary_stats, TRAINING_PHASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ebe25-6a98-4fa3-83ed-706bfa90e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26e8ff-1e6d-4f9c-ba89-048f152a10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dda0c-1079-4204-9881-757af54a747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1000\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232098e-bae9-425b-b74b-4b1e5ce71a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i - window):i + 1]) for i in range(len(reward_history))]\n",
    "\n",
    "final_reward_fig, final_reward_ax = plt.subplots(figsize=(10, 5))\n",
    "final_reward_ax.plot(smoothed, label=f\"Smoothed Reward (window={window})\", color='blue')\n",
    "\n",
    "# --- Add phase transitions ---\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    ep = meta[\"length\"]\n",
    "    if ep is not None and ep <= len(reward_history):\n",
    "        final_reward_ax.axvline(x=ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_reward_ax.text(ep + 5, max(smoothed) * 0.95, name,\n",
    "                             rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "final_reward_ax.set_title(\"Smoothed Reward Over Episodes\")\n",
    "final_reward_ax.set_xlabel(\"Episode\")\n",
    "final_reward_ax.set_ylabel(\"Smoothed Reward\")\n",
    "final_reward_ax.legend()\n",
    "final_reward_ax.grid(True)\n",
    "final_reward_fig.tight_layout()\n",
    "\n",
    "# --- Show plot ---\n",
    "plt.show()\n",
    "\n",
    "# --- Save to file ---\n",
    "final_reward_fig.savefig(f\"{PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n",
    "plt.close(final_reward_fig)\n",
    "print(f\"Smoothed reward plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f636ff5-a35d-4aa3-bbfd-389616cdc3b9",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bdd1a-62a5-4fc6-b191-d8927b1da9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 dqn_model_{timestamp} episodes-{num_episodes} lookahead-{lookahead_depth}.pt\"\n",
    "default_model_path = \"Connect4 DQN model.pt\"\n",
    "\n",
    "torch.save(agent.model.state_dict(), model_path)\n",
    "torch.save(agent.model.state_dict(), default_model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c93c4b-80f3-4e23-9e2d-1f01f9112d8d",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87360d37-f8d9-48e2-935a-660863a3da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(device=device)  # Fresh agent instance\n",
    "state_dict = torch.load(default_model_path, map_location=device, weights_only=True)\n",
    "agent.model.load_state_dict(state_dict)\n",
    "agent.update_target_model()\n",
    "agent.epsilon = 0.0  # Fully greedy — no exploration\n",
    "print(\"✅ Trained model loaded and ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31153e6e-79a6-4106-af92-b6cac6b6f834",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb37c78-44b6-47d8-8fc3-75b12780f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pure-model evaluation ( ===\n",
    "# Opponents and games\n",
    "evaluation_opponents = {\n",
    "    \"Random\": 203,\n",
    "    \"Lookahead-1\": 101,\n",
    "    \"Lookahead-2\": 53,\n",
    "    \"Lookahead-3\": 11,\n",
    "}\n",
    "evaluation_results = {}\n",
    "\n",
    "# Make model deterministic & greedy\n",
    "agent_model_mode  = agent.model.training\n",
    "agent_target_mode = agent.target_model.training\n",
    "_eps_backup, _epsmin_backup = agent.epsilon, agent.epsilon_min\n",
    "\n",
    "agent.model.eval()\n",
    "agent.target_model.eval()\n",
    "agent.epsilon = 0.0\n",
    "agent.epsilon_min = 0.0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for label, num_games in evaluation_opponents.items():\n",
    "    wins = losses = draws = 0\n",
    "    depth = int(label.split(\"-\")[1]) if label.startswith(\"Lookahead\") else None\n",
    "\n",
    "    with tqdm(total=num_games, desc=f\"Opponent: {label}\") as pbar:\n",
    "        for game_index in range(num_games):\n",
    "            state = env.reset()\n",
    "            # Alternate starters; agent is +1 in this env\n",
    "            env.current_player = 1 if (game_index % 2 == 0) else -1\n",
    "            #env.current_player = 1 \n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                valid_actions = env.available_actions()\n",
    "\n",
    "                if env.current_player == 1:\n",
    "                    # --- Pure model forward, mask illegal, argmax ---\n",
    "                    with torch.no_grad():\n",
    "                        x = build_input(agent, state, device)                 # (1, C, 6, 7)\n",
    "                        q = agent.model(x).squeeze(0).detach().cpu().numpy()  # (7,)\n",
    "                    mask = np.full_like(q, -1e9, dtype=np.float32)\n",
    "                    for a in valid_actions:\n",
    "                        mask[a] = 0.0\n",
    "                    action = int(np.argmax(q + mask))\n",
    "                    if action not in valid_actions:\n",
    "                        action = random.choice(valid_actions)\n",
    "                else:\n",
    "                    # Opponent move\n",
    "                    if label == \"Random\":\n",
    "                        action = random.choice(valid_actions)\n",
    "                    else:\n",
    "                        action = Lookahead.n_step_lookahead(np.array(state), -1, depth=depth)\n",
    "                        if action not in valid_actions:\n",
    "                            action = random.choice(valid_actions)\n",
    "\n",
    "                state, _, done = env.step(action)\n",
    "\n",
    "            # Tally from agent’s perspective (+1 is agent)\n",
    "            if env.winner == 1:\n",
    "                wins += 1\n",
    "            elif env.winner == -1:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    evaluation_results[label] = {\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"draws\": draws,\n",
    "        \"win_rate\": round(wins / num_games, 3),\n",
    "        \"loss_rate\": round(losses / num_games, 3),\n",
    "        \"draw_rate\": round(draws / num_games, 3),\n",
    "    }\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Evaluation completed in {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016fd424-c11f-48d9-b7a7-68f8285e556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Print Summary ===\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "for label, stats in evaluation_results.items():\n",
    "    print(f\"{label}: {stats['wins']}W / {stats['losses']}L / {stats['draws']}D → \"\n",
    "          f\"Win: {stats['win_rate']*100:.1f}%, Loss: {stats['loss_rate']*100:.1f}%, Draw: {stats['draw_rate']*100:.1f}%\")\n",
    "\n",
    "# === Bar Plot Summary ===\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates  = [evaluation_results[k]['win_rate']  * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %')\n",
    "plt.bar([i + bar_width for i in x], loss_rates, width=bar_width, label='Loss %')\n",
    "plt.bar([i + 2 * bar_width for i in x], draw_rates, width=bar_width, label='Draw %')\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('DQN Agent Performance vs Various Opponents')\n",
    "plt.xticks([i + bar_width for i in x], labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = f\"{PLOTS}DQN-{TRAINING_SESSION}-evaluation_plot.png\"\n",
    "plt.savefig(plot_path, dpi=150)\n",
    "print(f\"📊 Plot saved to {plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "df_eval = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "df_eval.index.name = \"Opponent\"\n",
    "# Use Excel if available; otherwise fall back to CSV\n",
    "try:\n",
    "    df_eval.to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-evaluation_results.xlsx\", index=True)\n",
    "except Exception as e:\n",
    "    print(\"Excel export failed, saving CSV instead:\", e)\n",
    "    df_eval.to_csv(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-evaluation_results.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1665bfe-a7b4-4cfa-a12f-df7d215f11ab",
   "metadata": {},
   "source": [
    "## Debug evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722a7e7-2c53-4ace-95f5-714ece83daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs random (sanity)\n",
    "stats_rand = evaluate_with_leaks(agent, env, episodes=100, opponent=\"random\", depth=0)\n",
    "print(stats_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd982a-602a-4021-bfe7-62e6ed6160f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs L1\n",
    "stats_l1 = evaluate_with_leaks(agent, env, episodes=100, opponent=\"lookahead\", depth=1)\n",
    "print(stats_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b01784-2970-4c87-a91b-ca40a635ab0d",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf946-a47a-427e-8aa9-ccc9c844a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = (total_end_time - begin_start_time) / 3600\n",
    "print(f\"Evaluation completed in {total_elapsed:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e8d05-4a25-47b9-afb9-aea190a1f90e",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601c437-841c-459f-93d4-029cdfa9dc43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING_SESSION\n",
    "\n",
    "training_log_file = \"DQN training_sessions.xlsx\"\n",
    "log_row = {\"TRAINING_SESSION\": TRAINING_SESSION, \"TIME [h]\": total_elapsed, \"EPISODES\": num_episodes}\n",
    "\n",
    "for label, stats in evaluation_results.items():\n",
    "    log_row[label] = stats[\"win_rate\"]\n",
    "\n",
    "# === Load or Create Excel File ===\n",
    "if os.path.exists(training_log_file):\n",
    "    df_log = pd.read_excel(training_log_file)\n",
    "else:\n",
    "    df_log = pd.DataFrame()\n",
    "\n",
    "# === Append and Save ===\n",
    "df_log = pd.concat([df_log, pd.DataFrame([log_row])], ignore_index=True)\n",
    "df_log.to_excel(training_log_file, index=False)\n",
    "\n",
    "print(f\"\\n📁 Training session logged to: {training_log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
