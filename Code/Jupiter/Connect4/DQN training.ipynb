{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1db620-d548-4056-b511-e9ac63c7dc6c",
   "metadata": {},
   "source": [
    "# Connect4 DQN model\n",
    "By LaughingSkull \n",
    "as new RL agent for my game COnnect-4: https://www.laughingskull.org/Games/Connect4/Connect4.php\n",
    "\n",
    "\n",
    "## DQN (Deep Q-Network)\n",
    "\n",
    "* A value-based reinforcement learning method.\n",
    "* Uses a neural network to approximate the Q-function: estimates the expected future rewards for taking actions in given states.\n",
    "* Learns via Q-learning: updates the Q-values using the Bellman equation.\n",
    "* Typically uses techniques like experience replay and target networks to stabilize training.\n",
    "\n",
    "### version log    \n",
    "\n",
    "* 0.8.0 - start with new convolutional part<br>\n",
    "    * keeping L2 regularization\n",
    "* 0.9.0 - extension to shallow training\n",
    "* 0.10.0 - extension to L3 training\n",
    "    * NO PRUNE:\n",
    "        * worse \n",
    "    * SHARPer PRUNE\n",
    "    * extend phases\n",
    "        * MixedR12: no improvement; &cross;\n",
    "        * Shallow\n",
    "        * Fixed2\n",
    "        * Variable23\n",
    "        * Variable3\n",
    "* 0.11.0 - recover fixed 2\n",
    "    * corrected reward leaking bug\n",
    "    * corrected draw bug\n",
    "* 0.12.0 refactoring and restarting\n",
    "* 0.13 restarting again\n",
    "    * strategy weights djustment\n",
    "* 0.14 new restart from scratch\n",
    "    * intermediate phases\n",
    "* 0.15 shaped rewards\n",
    "* 0.16 testing target_update_interval\n",
    "* 0.17 changing CNN\n",
    "* 0.18 8-part weights approach, rebalancing and restarting (again)\n",
    "    * frozen model for self play\n",
    "    * adding mirrored rewards\n",
    "    * adding player POV then getting rid of it\n",
    "        * splitting agent  and oppo players channel\n",
    "* 0.19 restart with tweaking weights (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56d151-4c55-4871-b5c6-5a91c75cab0a",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "<br>\n",
    "[https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68](https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68)\n",
    "<br>\n",
    "[https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html](https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html)\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5dd4e-2529-4d7c-9e38-7bef1135eff8",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768e305-7069-4d06-9530-a2d5617560eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced052f-8ccc-4e0f-a8cd-dcf6c27a7446",
   "metadata": {},
   "source": [
    "### Fixed Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36059b-73c8-4356-ba7a-037473dca494",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f9655-bbba-4950-9260-548b64261ac9",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b037ed-2332-4851-ae94-945d511deeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4.connect4_env import Connect4Env\n",
    "from C4.connect4_lookahead import Connect4Lookahead\n",
    "from DQN.training_phases_config import TRAINING_PHASES\n",
    "from DQN.training_phases_config import set_training_phases_length\n",
    "from DQN.opponent_action import get_opponent_action\n",
    "from DQN.DQN_replay_memory import ReplayMemory\n",
    "from DQN.dqn_model import DQN\n",
    "from DQN.dqn_agent import DQNAgent\n",
    "from DQN.dqn_utilities import *\n",
    "from C4.connect4_board_display import Connect4_BoardDisplayer\n",
    "from C4.plot_phase_summary import plot_phase_summary\n",
    "\n",
    "Lookahead = Connect4Lookahead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024c11d-10d5-4f35-bb88-5b822e3551b0",
   "metadata": {},
   "source": [
    "#Training phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597e001-442c-49ce-934d-7577b778e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_training_phases_length(TRAINING_PHASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0589098-7517-4db7-bfb2-04a24f2d8e77",
   "metadata": {},
   "source": [
    "## Training session name and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb8661-d4cf-4cce-a79f-9100c87e6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead_depth = 7  # prophet = 7\n",
    "\n",
    "start_episode = 0\n",
    "num_episodes = 700\n",
    "num_episodes -= 1 # debug\n",
    "\n",
    "batch_size = 128\n",
    "target_update_interval = 25\n",
    "plot_interval = 10\n",
    "log_every_x_episode = 100\n",
    "tag = \"Random\"\n",
    "\n",
    "TRAINING_SESSION = f\"{tag}-{num_episodes}-TU-{target_update_interval}-BS-{batch_size}\"\n",
    "begin_start_time = time.time()\n",
    "LOG_DIR =\"Logs/DQN/\"\n",
    "MODEL_DIR =\"Models/DQN/\"\n",
    "PLOTS = \"Plots/DQN/\"\n",
    "PRUNE = True\n",
    "print(\"Started training session\", TRAINING_SESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59762c43-70fe-4a7b-9e8e-d5da65d7e628",
   "metadata": {},
   "source": [
    "### Model overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42c8fc-51bd-4cb5-af36-0d21b93a6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = DQN()\n",
    "summary(_model, input_size=(1, 2, 6, 7))  # batch=1, channels=2, height=6, width=7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d81c26-194c-4b97-8578-4333d99f85bc",
   "metadata": {},
   "source": [
    "## Training loop - DQN against lookahead opponent (Prophet-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75673c-d12f-4749-947c-bbf08cf5a2cb",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b1923-c3e5-49b5-9d35-d31d37263051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save training configuration to Excel ---\n",
    "from C4.training_config_logger import export_training_config\n",
    "\n",
    "paths = export_training_config(\n",
    "    training_phases=TRAINING_PHASES,\n",
    "    lookahead_depth=lookahead_depth,\n",
    "    num_episodes=num_episodes,\n",
    "    batch_size=batch_size,\n",
    "    target_update_interval=target_update_interval,\n",
    "    log_dir=LOG_DIR,\n",
    "    session_name=TRAINING_SESSION,\n",
    "    write_excel=True,    # set False if you only want CSV/JSON\n",
    "    write_json=False,     # handy for exact reproduction later\n",
    ")\n",
    "\n",
    "print(\"config written:\", paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa44b92-350d-4733-b718-5f2bdec9f0c9",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334f9db-3bdb-432c-96c4-b1db012333eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = {}  \n",
    "env = Connect4Env()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "agent = DQNAgent(device=device)\n",
    "\n",
    "reward_history = []\n",
    "win_history = []\n",
    "epsilon_history = []\n",
    "epsilon_min_history = []\n",
    "memory_prune_history = []\n",
    "win_count = loss_count = draw_count = 0\n",
    "phase = None\n",
    "frozen_opp = None\n",
    "strategy_weights = []\n",
    "memory_prune = 0  # default until first get_phase() call\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=num_episodes, desc=\"Training Episodes\") as pbar:\n",
    "    for episode in range(start_episode + 1, num_episodes + 1):\n",
    "        state = env.reset()        \n",
    "        total_reward = 0           # will hold ONLY the terminal reward\n",
    "        done = False\n",
    "        final_result = None        # 1 = win, -1 = loss, 0.5 = draw\n",
    "\n",
    "        new_phase, strategy_weights, epsilon, memory_prune, epsilon_min = get_phase(episode)    \n",
    "        phase, frozen_opp = handle_phase_change(agent, new_phase, phase, epsilon, memory_prune, epsilon_min)\n",
    "\n",
    "\n",
    "        # --------- main training loop --------- --------- --------- ---------\n",
    "        # Random start (opponent moves first, player -1)\n",
    "        if random.random() < 0.5:\n",
    "            opp_action = get_opponent_action(env, agent, episode, state, player=-1, depth=lookahead_depth, frozen_opp=frozen_opp, phase=phase)\n",
    "            state, r_opp, done = env.step(opp_action)\n",
    "\n",
    "        # --- Main episode loop --- (agent moves first -> player 1)\n",
    "        while not done:\n",
    "            valid_actions = env.available_actions()\n",
    "            action = agent.act(state, valid_actions, player=1, depth=lookahead_depth, strategy_weights=strategy_weights)\n",
    "            next_state, r_agent, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                # Terminal on the agent's move\n",
    "                final_result = evaluate_final_result(env, agent_player=1)\n",
    "                terminal_reward = map_final_result_to_reward(final_result)  \n",
    "                total_reward = terminal_reward        \n",
    "                agent.remember(state, 1, action, r_agent, next_state, -1, True)\n",
    "\n",
    "            else:\n",
    "                # Opponent responds (player -1)\n",
    "                opp_action = get_opponent_action(env, agent, episode, next_state, player=-1, depth=lookahead_depth, frozen_opp=frozen_opp, phase=phase)\n",
    "                next_state2, r_opp, done = env.step(opp_action)\n",
    "\n",
    "                if done:\n",
    "                    # Opponent ended the game\n",
    "                    final_result = evaluate_final_result(env, agent_player=1)\n",
    "                    terminal_reward = map_final_result_to_reward(final_result) \n",
    "                    total_reward = terminal_reward \n",
    "                    agent.remember(state, 1, action, total_reward, next_state2, 1, True)\n",
    "                    next_state = next_state2  # maintain state for any displays/logs\n",
    "\n",
    "                else:\n",
    "                    # Non-terminal full turn: turn reward\n",
    "                    agent.remember(state, 11, action, r_agent, next_state2, 1, False)\n",
    "                    next_state = next_state2\n",
    "\n",
    "            agent.replay(batch_size)\n",
    "            state = next_state\n",
    "\n",
    "        # --------- --------- --------- --------- --------- --------- ---------\n",
    "\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        epsilon_min_history.append(agent.epsilon_min)\n",
    "        reward_history.append(total_reward)  # strictly  terminal reward\n",
    "        memory_prune_history.append(memory_prune)\n",
    "\n",
    "        # Decay epsilon once per episode\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "            agent.epsilon = max(agent.epsilon, agent.epsilon_min)\n",
    "\n",
    "\n",
    "        wins, losses, draws = track_result(final_result, win_history)\n",
    "        win_count += wins\n",
    "        loss_count += losses\n",
    "        draw_count += draws\n",
    "\n",
    "        if episode % target_update_interval == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        if episode % plot_interval == 0:\n",
    "            avg_reward = np.mean(reward_history[-25:])\n",
    "            pbar.set_postfix(avg_reward=f\"{avg_reward:.2f}\", epsilon=f\"{agent.epsilon:.3f}\",\n",
    "                             wins=win_count, losses=loss_count, draws=draw_count, phase=phase)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if done: Connect4_BoardDisplayer.display_board(next_state)\n",
    "            plot_live_training(episode, reward_history, win_history, epsilon_history, \n",
    "                               phase, win_count, loss_count, draw_count, TRAINING_SESSION, memory_prune_history,\n",
    "                              epsilon_min_history)\n",
    "\n",
    "        if episode % log_every_x_episode == 0:\n",
    "            log_summary_stats(episode=episode, reward_history=reward_history, win_history=win_history, phase=phase,\n",
    "                              strategy_weights=strategy_weights, agent=agent, win_count=win_count, loss_count=loss_count,\n",
    "                              draw_count=draw_count, summary_stats_dict=summary_stats)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes ({elapsed / num_episodes:.2f} s/episode)\")\n",
    "\n",
    "# --- Save final Win Rate plot ---\n",
    "save_final_winrate_plot(win_history=win_history, training_phases=TRAINING_PHASES, save_path=PLOTS, session_name=TRAINING_SESSION)\n",
    "print(f\"Win rate plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_winrate.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216ccf7-ac10-48ba-8cf3-c01b1586e6e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nSummary stats (every {log_every_x_episode} episodes):\")\n",
    "#pprint.pprint(summary_stats)\n",
    "pd.DataFrame.from_dict(summary_stats, orient='index').to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-training_summary.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2badbf-dabe-4008-8c45-391a67652a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_phase_summary(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb65df-c8fe-428f-8ea5-d1749e669815",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Progress\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ebe25-6a98-4fa3-83ed-706bfa90e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26e8ff-1e6d-4f9c-ba89-048f152a10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dda0c-1079-4204-9881-757af54a747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1000\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232098e-bae9-425b-b74b-4b1e5ce71a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i - window):i + 1]) for i in range(len(reward_history))]\n",
    "\n",
    "final_reward_fig, final_reward_ax = plt.subplots(figsize=(10, 5))\n",
    "final_reward_ax.plot(smoothed, label=f\"Smoothed Reward (window={window})\", color='blue')\n",
    "\n",
    "# --- Add phase transitions ---\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    ep = meta[\"length\"]\n",
    "    if ep is not None and ep <= len(reward_history):\n",
    "        final_reward_ax.axvline(x=ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_reward_ax.text(ep + 5, max(smoothed) * 0.95, name,\n",
    "                             rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "final_reward_ax.set_title(\"Smoothed Reward Over Episodes\")\n",
    "final_reward_ax.set_xlabel(\"Episode\")\n",
    "final_reward_ax.set_ylabel(\"Smoothed Reward\")\n",
    "final_reward_ax.legend()\n",
    "final_reward_ax.grid(True)\n",
    "final_reward_fig.tight_layout()\n",
    "\n",
    "# --- Show plot ---\n",
    "plt.show()\n",
    "\n",
    "# --- Save to file ---\n",
    "final_reward_fig.savefig(f\"{PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n",
    "plt.close(final_reward_fig)\n",
    "print(f\"Smoothed reward plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f636ff5-a35d-4aa3-bbfd-389616cdc3b9",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bdd1a-62a5-4fc6-b191-d8927b1da9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 dqn_model_{timestamp} episodes-{num_episodes} lookahead-{lookahead_depth}.pt\"\n",
    "default_model_path = \"Connect4 DQN model.pt\"\n",
    "\n",
    "torch.save(agent.model.state_dict(), model_path)\n",
    "torch.save(agent.model.state_dict(), default_model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c93c4b-80f3-4e23-9e2d-1f01f9112d8d",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87360d37-f8d9-48e2-935a-660863a3da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(device=device)  # Fresh agent instance\n",
    "state_dict = torch.load(default_model_path, map_location=device, weights_only=True)\n",
    "agent.model.load_state_dict(state_dict)\n",
    "agent.update_target_model()\n",
    "agent.epsilon = 0.0  # Fully greedy â€” no exploration\n",
    "print(\"âœ… Trained model loaded and ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31153e6e-79a6-4106-af92-b6cac6b6f834",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb37c78-44b6-47d8-8fc3-75b12780f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION CONFIGURATION ===\n",
    "evaluation_opponents = {\n",
    "    \"Random\": 200,\n",
    "    \"Lookahead-1\": 100,\n",
    "    \"Lookahead-2\": 100,\n",
    "    \"Lookahead-3\": 25,\n",
    "    # \"Lookahead-5\": 10,\n",
    "    # \"Lookahead-7\": 5,\n",
    "}\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "evaluation_results = {}\n",
    "\n",
    "# --- Force deterministic, greedy evaluation ---\n",
    "agent_model_mode = agent.model.training\n",
    "agent_target_mode = agent.target_model.training\n",
    "agent.model.eval()\n",
    "agent.target_model.eval()\n",
    "_eps_backup = agent.epsilon\n",
    "_epsmin_backup = agent.epsilon_min\n",
    "agent.epsilon = 0.0\n",
    "agent.epsilon_min = 0.0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for label, num_games in evaluation_opponents.items():\n",
    "    wins = losses = draws = 0\n",
    "    depth = int(label.split(\"-\")[1]) if label.startswith(\"Lookahead\") else None\n",
    "\n",
    "    with tqdm(total=num_games, desc=f\"Opponent: {label}\") as pbar:\n",
    "        for _ in range(num_games):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            agent_first = random.choice([True, False])  # randomize who starts\n",
    "\n",
    "            while not done:\n",
    "                # Agent's turn?\n",
    "                is_agent_turn = ((env.current_player == 1 and agent_first) or\n",
    "                                 (env.current_player == -1 and not agent_first))\n",
    "\n",
    "                if is_agent_turn:\n",
    "                    valid_actions = env.available_actions()\n",
    "                    action = agent.act(state, valid_actions, player=env.current_player,\n",
    "                                       depth=lookahead_depth, strategy_weights=None)\n",
    "                else:\n",
    "                    valid_actions = env.available_actions()\n",
    "                    if label == \"Random\":\n",
    "                        action = random.choice(valid_actions)\n",
    "                    else:\n",
    "                        # Lookahead opponent\n",
    "                        board = np.array(state)\n",
    "                        action = Lookahead.n_step_lookahead(board, env.current_player, depth=depth)\n",
    "                        # Safety: if lookahead returns a filled column (shouldn't), fall back to random legal\n",
    "                        if action not in valid_actions:\n",
    "                            action = random.choice(valid_actions)\n",
    "\n",
    "                state, _, done = env.step(action)\n",
    "\n",
    "            # --- Use env.winner (1, -1, or 0 for draw) ---\n",
    "            if env.winner == 1:\n",
    "                # Player +1 won\n",
    "                wins += 1 if agent_first else 0\n",
    "                losses += 0 if agent_first else 1\n",
    "            elif env.winner == -1:\n",
    "                # Player -1 won\n",
    "                wins += 0 if agent_first else 1\n",
    "                losses += 1 if agent_first else 0\n",
    "            elif env.winner == 0:\n",
    "                draws += 1\n",
    "            else:\n",
    "                # Shouldn't happen; treat as draw to avoid skewing\n",
    "                draws += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    evaluation_results[label] = {\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"draws\": draws,\n",
    "        \"win_rate\": round(wins / num_games, 3),\n",
    "        \"loss_rate\": round(losses / num_games, 3),\n",
    "        \"draw_rate\": round(draws / num_games, 3),\n",
    "    }\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "# --- Restore agent mode/state ---\n",
    "agent.epsilon = _eps_backup\n",
    "agent.epsilon_min = _epsmin_backup\n",
    "agent.model.train(agent_model_mode)\n",
    "agent.target_model.train(agent_target_mode)\n",
    "\n",
    "print(f\"Evaluation completed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "# === Print Summary ===\n",
    "print(\"\\nðŸ“Š Evaluation Summary:\")\n",
    "for label, stats in evaluation_results.items():\n",
    "    print(f\"{label}: {stats['wins']}W / {stats['losses']}L / {stats['draws']}D â†’ \"\n",
    "          f\"Win: {stats['win_rate']*100:.1f}%, Loss: {stats['loss_rate']*100:.1f}%, Draw: {stats['draw_rate']*100:.1f}%\")\n",
    "\n",
    "# === Bar Plot Summary ===\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates  = [evaluation_results[k]['win_rate']  * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %')\n",
    "plt.bar([i + bar_width for i in x], loss_rates, width=bar_width, label='Loss %')\n",
    "plt.bar([i + 2 * bar_width for i in x], draw_rates, width=bar_width, label='Draw %')\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('DQN Agent Performance vs Various Opponents')\n",
    "plt.xticks([i + bar_width for i in x], labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "df_eval = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "df_eval.index.name = \"Opponent\"\n",
    "# Use Excel if available; otherwise fall back to CSV\n",
    "try:\n",
    "    df_eval.to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-evaluation_results.xlsx\", index=True)\n",
    "except Exception as e:\n",
    "    print(\"Excel export failed, saving CSV instead:\", e)\n",
    "    df_eval.to_csv(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-evaluation_results.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b01784-2970-4c87-a91b-ca40a635ab0d",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf946-a47a-427e-8aa9-ccc9c844a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = (total_end_time - begin_start_time) / 3600\n",
    "print(f\"Evaluation completed in {total_elapsed:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e8d05-4a25-47b9-afb9-aea190a1f90e",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601c437-841c-459f-93d4-029cdfa9dc43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING_SESSION\n",
    "\n",
    "training_log_file = \"DQN training_sessions.xlsx\"\n",
    "log_row = {\"TRAINING_SESSION\": TRAINING_SESSION, \"TIME [h]\": total_elapsed, \"EPISODES\": num_episodes}\n",
    "\n",
    "for label, stats in evaluation_results.items():\n",
    "    log_row[label] = stats[\"win_rate\"]\n",
    "\n",
    "# === Load or Create Excel File ===\n",
    "if os.path.exists(training_log_file):\n",
    "    df_log = pd.read_excel(training_log_file)\n",
    "else:\n",
    "    df_log = pd.DataFrame()\n",
    "\n",
    "# === Append and Save ===\n",
    "df_log = pd.concat([df_log, pd.DataFrame([log_row])], ignore_index=True)\n",
    "df_log.to_excel(training_log_file, index=False)\n",
    "\n",
    "print(f\"\\nðŸ“ Training session logged to: {training_log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011fa1c-5e49-4e42-a44a-f07585f6ccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
