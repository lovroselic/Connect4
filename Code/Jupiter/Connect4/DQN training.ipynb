{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1db620-d548-4056-b511-e9ac63c7dc6c",
   "metadata": {},
   "source": [
    "# Connect4 DQN model\n",
    "By LaughingSkull \n",
    "as new RL agent for my game COnnect-4: https://www.laughingskull.org/Games/Connect4/Connect4.php\n",
    "\n",
    "\n",
    "## DQN (Deep Q-Network)\n",
    "\n",
    "* A value-based reinforcement learning method.\n",
    "* Uses a neural network to approximate the Q-function: estimates the expected future rewards for taking actions in given states.\n",
    "* Learns via Q-learning: updates the Q-values using the Bellman equation.\n",
    "* Typically uses techniques like experience replay and target networks to stabilize training.\n",
    "\n",
    "### version log    \n",
    "\n",
    "* 0.8.0 - start with new convolutional part<br>\n",
    "    * keeping L2 regularization\n",
    "* 0.9.0 - extension to shallow training\n",
    "* 0.10.0 - extension to L3 training\n",
    "    * NO PRUNE:\n",
    "        * worse \n",
    "    * SHARPer PRUNE\n",
    "    * extend phases\n",
    "        * MixedR12: no improvement; &cross;\n",
    "        * Shallow\n",
    "        * Fixed2\n",
    "        * Variable23\n",
    "        * Variable3\n",
    "* 0.11.0 - recover fixed 2\n",
    "    * corrected reward leaking bug\n",
    "    * corrected draw bug\n",
    "* 0.12.0 refactoring and restarting\n",
    "* 0.13 restarting again\n",
    "    * strategy weights djustment\n",
    "* 0.14 new restart from scratch\n",
    "    * intermediate phases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56d151-4c55-4871-b5c6-5a91c75cab0a",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "<br>\n",
    "[https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68](https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68)\n",
    "<br>\n",
    "[https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html](https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html)\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5dd4e-2529-4d7c-9e38-7bef1135eff8",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b768e305-7069-4d06-9530-a2d5617560eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced052f-8ccc-4e0f-a8cd-dcf6c27a7446",
   "metadata": {},
   "source": [
    "### Fixed Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36059b-73c8-4356-ba7a-037473dca494",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f9655-bbba-4950-9260-548b64261ac9",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b037ed-2332-4851-ae94-945d511deeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4.connect4_env import Connect4Env\n",
    "from C4.connect4_lookahead import Connect4Lookahead\n",
    "from DQN.training_phases_config import TRAINING_PHASES\n",
    "from DQN.opponent_action import get_opponent_action\n",
    "from DQN.DQN_replay_memory import ReplayMemory\n",
    "from DQN.dqn_model import DQN\n",
    "from DQN.dqn_agent import DQNAgent\n",
    "from DQN.get_phase import get_phase\n",
    "from C4.connect4_board_display import Connect4_BoardDisplayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0589098-7517-4db7-bfb2-04a24f2d8e77",
   "metadata": {},
   "source": [
    "## Training session name and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb8661-d4cf-4cce-a79f-9100c87e6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookahead_depth = 7  # prophet = 7\n",
    "\n",
    "num_episodes = 5000\n",
    "\n",
    "batch_size = 64\n",
    "target_update_interval = 10\n",
    "log_every_x_episode = 100\n",
    "tag = \"Mixed_R1122 0.1, 0.4, 0.5, 0, 0, 0 E04b MP04b\"\n",
    "\n",
    "TRAINING_SESSION = f\"{tag}-{num_episodes}\"\n",
    "begin_start_time = time.time()\n",
    "LOG_DIR =\"Logs/DQN/\"\n",
    "MODEL_DIR =\"Models/DQN/\"\n",
    "PLOTS = \"Plots/DQN/\"\n",
    "PRUNE = True\n",
    "print(\"Started training session\", TRAINING_SESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59762c43-70fe-4a7b-9e8e-d5da65d7e628",
   "metadata": {},
   "source": [
    "### Model overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42c8fc-51bd-4cb5-af36-0d21b93a6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = DQN()\n",
    "summary(_model, input_size=(1, 6, 7))  # batch height, width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d81c26-194c-4b97-8578-4333d99f85bc",
   "metadata": {},
   "source": [
    "## Training loop - DQN against lookahead opponent (Prophet-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75673c-d12f-4749-947c-bbf08cf5a2cb",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b1923-c3e5-49b5-9d35-d31d37263051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save training configuration to Excel ---\n",
    "\n",
    "config_log = {\n",
    "    \"TRAINING_PHASES\": TRAINING_PHASES,  # Updated structure\n",
    "    \"lookahead_depth\": lookahead_depth,\n",
    "    \"num_episodes\": num_episodes,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"target_update_interval\": target_update_interval\n",
    "}\n",
    "\n",
    "# Flatten nested structure for Excel output\n",
    "flat_config = []\n",
    "for section, values in config_log.items():\n",
    "    if isinstance(values, dict):\n",
    "        for phase_name, params in values.items():\n",
    "            if isinstance(params, dict):\n",
    "                for param_key, param_value in params.items():\n",
    "                    flat_config.append({\n",
    "                        \"Section\": section,\n",
    "                        \"Phase\": phase_name,\n",
    "                        \"Key\": param_key,\n",
    "                        \"Value\": str(param_value)\n",
    "                    })\n",
    "            else:\n",
    "                flat_config.append({\n",
    "                    \"Section\": section,\n",
    "                    \"Phase\": phase_name,\n",
    "                    \"Key\": \"\",\n",
    "                    \"Value\": str(params)\n",
    "                })\n",
    "    else:\n",
    "        flat_config.append({\n",
    "            \"Section\": section,\n",
    "            \"Phase\": \"\",\n",
    "            \"Key\": \"\",\n",
    "            \"Value\": str(values)\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "config_df = pd.DataFrame(flat_config)\n",
    "\n",
    "# Save to Excel\n",
    "excel_path = f\"{LOG_DIR}DQN-{TRAINING_SESSION}_training_config.xlsx\"\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:\n",
    "    config_df.to_excel(writer, sheet_name='config', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa44b92-350d-4733-b718-5f2bdec9f0c9",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334f9db-3bdb-432c-96c4-b1db012333eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = {}  # store stats\n",
    "env = Connect4Env()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)\n",
    "agent = DQNAgent(device=device)\n",
    "\n",
    "reward_history = []\n",
    "win_history = []\n",
    "epsilon_history = []\n",
    "win_count = loss_count = draw_count = 0\n",
    "phase = \"Random\"\n",
    "strategy_weights = []\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=num_episodes, desc=\"Training Episodes\") as pbar:\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        final_result = None  # 1 = win, -1 = loss, 0.5 = draw\n",
    "\n",
    "        while not done:\n",
    "            valid_actions = env.available_actions()\n",
    "            new_phase, strategy_weights, epsilon, memory_prune = get_phase(episode)\n",
    "\n",
    "            # react to phase change\n",
    "            if new_phase != phase:\n",
    "                agent.epsilon = epsilon\n",
    "                if PRUNE and memory_prune:\n",
    "                    agent.memory.prune(memory_prune)\n",
    "                phase = new_phase\n",
    "\n",
    "            player_before_move = env.current_player  # <- track agent identity\n",
    "            action = agent.act(state, valid_actions, player=env.current_player, depth=lookahead_depth, strategy_weights=strategy_weights)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                if env.winner == player_before_move:\n",
    "                    final_result = 1  # agent won\n",
    "                elif env.winner == -player_before_move:\n",
    "                    final_result = -1  # opponent won\n",
    "                elif env.winner == 0:\n",
    "                    final_result = 0.5  # draw\n",
    "                else:\n",
    "                    final_result = None\n",
    "\n",
    "                if final_result == 1:\n",
    "                    agent.remember(state, action, reward, next_state, done)\n",
    "            else:\n",
    "                opp_action = get_opponent_action(env, agent, episode, next_state, player=-1, depth=lookahead_depth)\n",
    "                next_state, opp_reward, done = env.step(opp_action)\n",
    "                if done:\n",
    "                    if env.winner == player_before_move:\n",
    "                        final_result = 1\n",
    "                    elif env.winner == -player_before_move:\n",
    "                        final_result = -1\n",
    "                    elif env.winner == 0:\n",
    "                        final_result = 0.5\n",
    "                    else:\n",
    "                        final_result = None\n",
    "                else:\n",
    "                    agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            agent.replay(batch_size)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        if final_result == 1:\n",
    "            win_count += 1\n",
    "            win_history.append(1)\n",
    "        elif final_result == -1:\n",
    "            loss_count += 1\n",
    "            win_history.append(0)\n",
    "        elif final_result == 0.5:\n",
    "            draw_count += 1\n",
    "            win_history.append(0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid final_result â€” env.winner was not set correctly.\") # should not happen anymore\n",
    "\n",
    "        if episode % target_update_interval == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(reward_history[-25:])\n",
    "            pbar.set_postfix(avg_reward=f\"{avg_reward:.2f}\",\n",
    "                             epsilon=f\"{agent.epsilon:.3f}\",\n",
    "                             wins=win_count,\n",
    "                             losses=loss_count,\n",
    "                             draws=draw_count,\n",
    "                             phase=phase)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if done: Connect4_BoardDisplayer.display_board(next_state)\n",
    "\n",
    "            # live plots   \n",
    "            fig, ax = plt.subplots(3, 1, figsize=(10, 9), sharex=True)\n",
    "\n",
    "            ax[0].plot(reward_history, label='Reward')\n",
    "            if len(reward_history) >= 25:\n",
    "                avg = np.convolve(reward_history, np.ones(25) / 25, mode='valid')\n",
    "                ax[0].plot(range(24, len(reward_history)), avg, label='25-ep Moving Avg', linewidth=2)\n",
    "            ax[0].set_ylabel('Total Reward')\n",
    "            ax[0].legend()\n",
    "            ax[0].grid(True)\n",
    "\n",
    "            if len(win_history) >= 25:\n",
    "                win_avg = np.convolve(win_history, np.ones(25) / 25, mode='valid')\n",
    "                ax[1].plot(range(24, len(win_history)), win_avg, label='Win Rate (25 ep)', color='green')\n",
    "            ax[1].set_ylabel('Win Rate')\n",
    "            if len(ax[1].lines) > 0:\n",
    "                ax[1].legend()\n",
    "            ax[1].grid(True)\n",
    "\n",
    "            ax[2].plot(epsilon_history, label='Epsilon', color='orange')\n",
    "            ax[2].set_xlabel('Episode')\n",
    "            ax[2].set_ylabel('Epsilon')\n",
    "            ax[2].legend()\n",
    "            ax[2].grid(True)\n",
    "\n",
    "            fig.suptitle(f\"Episode {episode} â€” Phase: {phase} | Wins: {win_count}, Losses: {loss_count}, Draws: {draw_count} | Îµ={agent.epsilon:.3f}\")\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "            for name, meta in TRAINING_PHASES.items():\n",
    "                transition_ep = meta[\"length\"]\n",
    "                if transition_ep is not None and transition_ep <= episode:\n",
    "                    for axis in ax:\n",
    "                        axis.axvline(transition_ep, color='black', linestyle='dotted', linewidth=1)\n",
    "                        axis.text(transition_ep + 2, axis.get_ylim()[1] * 0.95, name,\n",
    "                                  rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "            display(fig)\n",
    "            plt.close()\n",
    "\n",
    "        if episode % log_every_x_episode == 0:\n",
    "            recent_rewards = reward_history[-25:] if len(reward_history) >= 25 else reward_history\n",
    "            recent_win_rate = np.mean(win_history[-25:]) if len(win_history) >= 25 else np.mean(win_history)\n",
    "            summary_stats[episode] = {\n",
    "                \"phase\": phase,\n",
    "                \"strategy_weights\": strategy_weights.copy(),\n",
    "                \"epsilon\": agent.epsilon,\n",
    "                \"wins\": win_count,\n",
    "                \"losses\": loss_count,\n",
    "                \"draws\": draw_count,\n",
    "                \"avg_reward_25\": round(np.mean(recent_rewards), 2),\n",
    "                \"win_rate_25\": round(recent_win_rate, 2)\n",
    "            }\n",
    "            \n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes ({elapsed / num_episodes:.2f} s/episode)\")\n",
    "\n",
    "# --- Save final Win Rate plot ---\n",
    "final_win_fig, final_win_ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "if len(win_history) >= 25:\n",
    "    win_avg = np.convolve(win_history, np.ones(25)/25, mode='valid')\n",
    "    final_win_ax.plot(range(24, len(win_history)), win_avg, label='Win Rate (25 ep)', color='green')\n",
    "\n",
    "final_win_ax.set_title(\"Final Win Rate Over Training\")\n",
    "final_win_ax.set_xlabel(\"Episode\")\n",
    "final_win_ax.set_ylabel(\"Win Rate\")\n",
    "final_win_ax.grid(True)\n",
    "final_win_ax.legend()\n",
    "\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    transition_ep = meta[\"length\"]\n",
    "    if transition_ep is not None and transition_ep <= len(win_history):\n",
    "        final_win_ax.axvline(transition_ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_win_ax.text(\n",
    "            transition_ep + 2,\n",
    "            final_win_ax.get_ylim()[1] * 0.95,\n",
    "            name,\n",
    "            rotation=90,\n",
    "            va='top',\n",
    "            ha='left',\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "\n",
    "final_win_fig.savefig(f\"{PLOTS}DQN-{TRAINING_SESSION}_final_winrate.png\")\n",
    "plt.close(final_win_fig)\n",
    "print(f\"Win rate plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_winrate.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216ccf7-ac10-48ba-8cf3-c01b1586e6e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nSummary stats (every {log_every_x_episode} episodes):\")\n",
    "#pprint.pprint(summary_stats)\n",
    "pd.DataFrame.from_dict(summary_stats, orient='index').to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-training_summary.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb65df-c8fe-428f-8ea5-d1749e669815",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Progress\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ebe25-6a98-4fa3-83ed-706bfa90e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232098e-bae9-425b-b74b-4b1e5ce71a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i - window):i + 1]) for i in range(len(reward_history))]\n",
    "\n",
    "final_reward_fig, final_reward_ax = plt.subplots(figsize=(10, 5))\n",
    "final_reward_ax.plot(smoothed, label=f\"Smoothed Reward (window={window})\", color='blue')\n",
    "\n",
    "# --- Add phase transitions ---\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    ep = meta[\"length\"]\n",
    "    if ep is not None and ep <= len(reward_history):\n",
    "        final_reward_ax.axvline(x=ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_reward_ax.text(ep + 5, max(smoothed) * 0.95, name,\n",
    "                             rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "final_reward_ax.set_title(\"Smoothed Reward Over Episodes\")\n",
    "final_reward_ax.set_xlabel(\"Episode\")\n",
    "final_reward_ax.set_ylabel(\"Smoothed Reward\")\n",
    "final_reward_ax.legend()\n",
    "final_reward_ax.grid(True)\n",
    "final_reward_fig.tight_layout()\n",
    "\n",
    "# --- Show plot ---\n",
    "plt.show()\n",
    "\n",
    "# --- Save to file ---\n",
    "final_reward_fig.savefig(f\"{PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n",
    "plt.close(final_reward_fig)\n",
    "print(f\"Smoothed reward plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f636ff5-a35d-4aa3-bbfd-389616cdc3b9",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bdd1a-62a5-4fc6-b191-d8927b1da9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 dqn_model_{timestamp} episodes-{num_episodes} lookahead-{lookahead_depth}.pt\"\n",
    "default_model_path = \"Connect4 DQN model.pt\"\n",
    "\n",
    "torch.save(agent.model.state_dict(), model_path)\n",
    "torch.save(agent.model.state_dict(), default_model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c93c4b-80f3-4e23-9e2d-1f01f9112d8d",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87360d37-f8d9-48e2-935a-660863a3da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(device=device)  # Fresh agent instance\n",
    "state_dict = torch.load(default_model_path, map_location=device, weights_only=True)\n",
    "agent.model.load_state_dict(state_dict)\n",
    "agent.update_target_model()\n",
    "agent.epsilon = 0.0  # Fully greedy â€” no exploration\n",
    "print(\"âœ… Trained model loaded and ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31153e6e-79a6-4106-af92-b6cac6b6f834",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb37c78-44b6-47d8-8fc3-75b12780f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION CONFIGURATION ===\n",
    "evaluation_opponents = {\n",
    "    \"Random\": 100,\n",
    "    \"Lookahead-1\": 100,\n",
    "    \"Lookahead-2\": 100,\n",
    "    \"Lookahead-3\": 25,\n",
    "    # \"Lookahead-5\": 10, # too soon to test\n",
    "    # \"Lookahead-7\": 5  # too soon to test\n",
    "}\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "evaluation_results = {}\n",
    "Lookahead = Connect4Lookahead()\n",
    "start_time = time.time()\n",
    "\n",
    "for label, num_games in evaluation_opponents.items():\n",
    "    wins = losses = draws = 0\n",
    "    depth = int(label.split(\"-\")[1]) if label.startswith(\"Lookahead\") else None\n",
    "\n",
    "    with tqdm(total=num_games, desc=f\"Opponent: {label}\") as pbar:\n",
    "        for _ in range(num_games):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            agent_first = random.choice([True, False])\n",
    "\n",
    "            while not done:\n",
    "                if (env.current_player == 1 and agent_first) or (env.current_player == -1 and not agent_first):\n",
    "                    valid_actions = env.available_actions()\n",
    "                    action = agent.act(state, valid_actions, player=env.current_player, depth=lookahead_depth, strategy_weights=None)\n",
    "                else:\n",
    "                    if label == \"Random\":\n",
    "                        action = random.choice(env.available_actions())\n",
    "                    else:\n",
    "                        board = np.array(state)\n",
    "                        action = Lookahead.n_step_lookahead(board, env.current_player, depth=depth)\n",
    "            \n",
    "                state, reward, done = env.step(action)\n",
    "\n",
    "\n",
    "            # Determine winner\n",
    "            winner = -env.current_player\n",
    "            if winner == 1:\n",
    "                if agent_first:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "            elif winner == -1:\n",
    "                if not agent_first:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    evaluation_results[label] = {\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"draws\": draws,\n",
    "        \"win_rate\": round(wins / num_games, 2),\n",
    "        \"loss_rate\": round(losses / num_games, 2),\n",
    "        \"draw_rate\": round(draws / num_games, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Evaluation completed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "# === Print Summary ===\n",
    "print(\"\\nðŸ“Š Evaluation Summary:\")\n",
    "for label, stats in evaluation_results.items():\n",
    "    print(f\"{label}: {stats['wins']}W / {stats['losses']}L / {stats['draws']}D â†’ \"\n",
    "          f\"Win Rate: {stats['win_rate']*100:.1f}%, Loss: {stats['loss_rate']*100:.1f}%, Draws: {stats['draw_rate']*100:.1f}%\")\n",
    "\n",
    "# === Bar Plot Summary ===\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates = [evaluation_results[k]['win_rate'] * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %', color='green')\n",
    "plt.bar([i + bar_width for i in x], loss_rates, width=bar_width, label='Loss %', color='red')\n",
    "plt.bar([i + 2 * bar_width for i in x], draw_rates, width=bar_width, label='Draw %', color='gray')\n",
    "\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('DQN Agent Performance vs Various Opponents')\n",
    "plt.xticks([i + bar_width for i in x], labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert evaluation results to DataFrame\n",
    "df_eval = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "df_eval.index.name = \"Opponent\"\n",
    "\n",
    "# Save to CSV\n",
    "df_eval.to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-evaluation_results.xlsx\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b01784-2970-4c87-a91b-ca40a635ab0d",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf946-a47a-427e-8aa9-ccc9c844a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = (total_end_time - begin_start_time) / 3600\n",
    "print(f\"Evaluation completed in {total_elapsed:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e8d05-4a25-47b9-afb9-aea190a1f90e",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601c437-841c-459f-93d4-029cdfa9dc43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING_SESSION\n",
    "\n",
    "training_log_file = \"DQN training_sessions.xlsx\"\n",
    "log_row = {\"TRAINING_SESSION\": TRAINING_SESSION, \"TIME [h]\": total_elapsed, \"EPISODES\": num_episodes}\n",
    "\n",
    "for label, stats in evaluation_results.items():\n",
    "    log_row[label] = stats[\"win_rate\"]\n",
    "\n",
    "# === Load or Create Excel File ===\n",
    "if os.path.exists(training_log_file):\n",
    "    df_log = pd.read_excel(training_log_file)\n",
    "else:\n",
    "    df_log = pd.DataFrame()\n",
    "\n",
    "# === Append and Save ===\n",
    "df_log = pd.concat([df_log, pd.DataFrame([log_row])], ignore_index=True)\n",
    "df_log.to_excel(training_log_file, index=False)\n",
    "\n",
    "print(f\"\\nðŸ“ Training session logged to: {training_log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
