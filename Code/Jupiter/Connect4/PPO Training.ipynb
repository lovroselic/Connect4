{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78dda5bb-40ee-4ae4-807d-6c3bb56ce335",
   "metadata": {},
   "source": [
    "# Connect4 PPQ model\n",
    "\n",
    "By LaughingSkull as new RL agent for my game COnnect-4: https://www.laughingskull.org/Games/Connect4/Connect4.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c66fe-748a-4000-8c42-8d2684db45d1",
   "metadata": {},
   "source": [
    "PPO (Proximal Policy Optimization)\n",
    "\n",
    "* A policy-based reinforcement learning method.\n",
    "\n",
    "* Uses a neural network to directly learn the policy: outputs a probability distribution over actions for given states.\n",
    "\n",
    "* Optimizes with clipped policy gradients to prevent overly large, destabilizing updates.\n",
    "\n",
    "* Balances exploration and exploitation via stochastic action sampling.\n",
    "\n",
    "* Typically more stable and sample-efficient than vanilla policy gradient methods, while simpler to implement than TRPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0a7db-43cf-4108-b8cc-cd5e1fd6b097",
   "metadata": {},
   "source": [
    "Version log:\n",
    "* 0.9 restart\n",
    "    * using DQN lessons learned as template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5faf5-32c9-478b-9aab-073caa647b7a",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "[https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/](https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "<br>\n",
    "[https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68](https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68)\n",
    "<br>\n",
    "[https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html](https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bbbee-d469-4e6f-ade0-a73cf3a43f81",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8de2b-83d7-487f-94ca-ad1d71edf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import warnings\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16f9f0-2d38-44b4-948a-23517b4d7c1b",
   "metadata": {},
   "source": [
    "### Fixed Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ec004-01f5-4ee2-8f72-38f034b2db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6033b-40d7-4206-8e80-f904a4428592",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513498ca-c36c-4197-b09e-2144271df931",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_every_x_episode = 100\n",
    "plot_interval = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b2ebe-5a1c-4b33-8791-b1e87b7b7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR =\"Logs/PPQ/\"\n",
    "MODEL_DIR =\"Models/PPQ/\"\n",
    "PLOTS = \"Plots/PPQ/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c6675-8a18-42ee-b47e-aab3d9a9933a",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405a67f-62bc-4f8e-9048-eb8fece20d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4.connect4_env import Connect4Env\n",
    "from C4.connect4_lookahead import Connect4Lookahead\n",
    "from PPO.ppo_training_phases_config import TRAINING_PHASES\n",
    "from PPO.actor_critic import ActorCritic\n",
    "from PPO.ppo_buffer import PPOBuffer, PPOHyperParams\n",
    "from PPO.ppo_update import ppo_update, PPOUpdateCfg\n",
    "from C4.connect4_board_display import Connect4_BoardDisplayer\n",
    "from C4.plot_phase_summary import plot_phase_summary\n",
    "from PPO.ppo_utilities import select_opponent_action, is_draw, PhaseTracker, display_phases_table, params_for_phase   \n",
    "from PPO.ppo_live_plot import plot_live_training_ppo\n",
    "from PPO.ppo_summary import (\n",
    "    init_summary_stats, log_summary_stats_ppo,\n",
    "    save_summary_stats_excel, sanitize_for_plot, summary_stats_df\n",
    ")\n",
    "from PPO.checkpoint import save_checkpoint, load_checkpoint\n",
    "\n",
    "Lookahead = Connect4Lookahead()\n",
    "summary_stats = init_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d311b3-98ad-4af0-9972-0bbed630e6e8",
   "metadata": {},
   "source": [
    "# Training phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496731be-eb9c-4fe4-b07d-3e545e4d0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASES = PhaseTracker(TRAINING_PHASES)\n",
    "display_phases_table(TRAINING_PHASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc4d62-5511-4ecb-815f-0bd7fab47b51",
   "metadata": {},
   "source": [
    "# Training session name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35877da1-7b13-41f7-a98b-f438eb0ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_episodes = 1500 \n",
    "lookahead_depth = 7  # prophet = 7\n",
    "tag = \"SelfPlay sanity\"\n",
    "TRAINING_SESSION = f\"PPQ-{number_episodes}-{tag}\"\n",
    "begin_start_time = time.time()\n",
    "print(\"Start training session\", TRAINING_SESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846bc07-d22a-4e47-9a08-c5f5a58c3103",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbd58d-8866-46c0-a111-3fc10f79053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = ActorCritic(action_dim=7).to(DEVICE)\n",
    "summary(ppo_model, input_size=(1, 1, 6, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d822e-b348-46a2-8684-a965417d423f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e72bf9-bc15-41da-8402-f41ce929a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logging / histories ---\n",
    "win_history = []  \n",
    "\n",
    "ppo_metrics_history = {\n",
    "    \"episodes\": [],\n",
    "    \"loss_pi\": [],\n",
    "    \"loss_v\": [],\n",
    "    \"entropy\": [],\n",
    "    \"approx_kl\": [],\n",
    "    \"clip_frac\": [],\n",
    "    \"explained_variance\": [],\n",
    "}\n",
    "\n",
    "update_idx = 0\n",
    "steps_collected = 0\n",
    "reward_history = []              # per-episode total reward\n",
    "win_count = loss_count = draw_count = 0\n",
    "wins_hist, losses_hist, draws_hist = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a66d3-26bb-4fce-ae31-df97bb885a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PPO TRAINING LOOP (episode-driven, with live plotting) ===\n",
    "\n",
    "# --- Hyperparameters (global defaults; phases may override) ---\n",
    "STEPS_PER_UPDATE    = 2048           # agent decisions per PPO update\n",
    "LEARNING_RATE       = 3e-4\n",
    "TEMPERATURE         = 1.0            # 0.0 = greedy sampling\n",
    "ATTR_LOSS_TO_LAST   = True           # attribute opponent's terminal win to our last move\n",
    "CLEAR_BUFFER_ON_PHASE_CHANGE = True  # ### CHANGED: avoid leaking rollouts across phases\n",
    "\n",
    "# --- Env / Policy / Optimizer / Buffer ---\n",
    "env     = Connect4Env()\n",
    "policy  = ActorCritic(action_dim=7).to(DEVICE)\n",
    "optim   = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "hparams = PPOHyperParams(gamma=0.99, gae_lambda=0.95, normalize_adv=True)\n",
    "buffer  = PPOBuffer(capacity=STEPS_PER_UPDATE * 2, action_dim=7, hparams=hparams)\n",
    "\n",
    "cfg     = PPOUpdateCfg(\n",
    "    epochs=4, batch_size=256,\n",
    "    clip_range=0.20, vf_clip_range=0.20,\n",
    "    ent_coef=0.01, vf_coef=0.5,\n",
    "    max_grad_norm=0.5, target_kl=0.03\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# for final bootstrapping if we stop mid-episode\n",
    "_state_for_bootstrap = None\n",
    "_next_player_for_bootstrap = None\n",
    "_done_for_bootstrap = True  # assume terminal unless proven otherwise\n",
    "current_phase_name = None\n",
    "lookahead_depth = None  # ### CHANGED: keep current lookahead here\n",
    "\n",
    "with tqdm(total=number_episodes, desc=\"Episodes\") as epbar:\n",
    "    for episode in range(1, number_episodes + 1):\n",
    "        phase_info, changed = PHASES.start_episode(episode)\n",
    "        current_phase_name = phase_info.name\n",
    "\n",
    "        if changed:\n",
    "            # apply per-phase opponent & PPO params\n",
    "            lookahead_depth = phase_info.lookahead  # None => Random; int => depth; \"self\" if you support self-play\n",
    "            phase_params = params_for_phase(current_phase_name, cfg)\n",
    "            # Hot-swap optimizer LR\n",
    "            for g in optim.param_groups:\n",
    "                g[\"lr\"] = phase_params[\"lr\"]\n",
    "            # Hot-swap PPO cfg knobs\n",
    "            cfg.clip_range     = phase_params[\"clip\"]\n",
    "            cfg.ent_coef       = phase_params[\"entropy\"]\n",
    "            cfg.epochs         = phase_params[\"epochs\"]\n",
    "            cfg.target_kl      = phase_params[\"target_kl\"]\n",
    "            cfg.vf_coef        = phase_params[\"vf_coef\"]\n",
    "            cfg.vf_clip_range  = phase_params[\"vf_clip\"]\n",
    "            cfg.batch_size     = phase_params[\"batch_size\"]\n",
    "            cfg.max_grad_norm  = phase_params[\"max_grad_norm\"]\n",
    "\n",
    "            if CLEAR_BUFFER_ON_PHASE_CHANGE:\n",
    "                buffer.clear()\n",
    "                steps_collected = 0\n",
    "                last_agent_reward_index = None\n",
    "\n",
    "            print(f\"[Phase → {current_phase_name}] \"\n",
    "                  f\"lookahead={lookahead_depth} | \"\n",
    "                  f\"lr={phase_params['lr']} clip={cfg.clip_range} entropy={cfg.ent_coef} \"\n",
    "                  f\"epochs={cfg.epochs} tKL={cfg.target_kl} bs={cfg.batch_size}\")\n",
    "\n",
    "        # === Episode loop ===\n",
    "        state = env.reset()\n",
    "        ep_return = 0.0\n",
    "        done = False\n",
    "        last_agent_reward_index = None\n",
    "        episode_win_flag = 0  # 1 if agent wins\n",
    "\n",
    "        # choose who starts explicitly: +1 agent, -1 opponent\n",
    "        next_player = random.choice([1, -1])\n",
    "\n",
    "        while not done:\n",
    "            if next_player == 1:\n",
    "                # --- Agent turn ---\n",
    "                legal_actions = env.available_actions()\n",
    "                if not legal_actions:\n",
    "                    draw_count += 1\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "                legal_mask = np.zeros(7, dtype=bool)\n",
    "                legal_mask[legal_actions] = True\n",
    "\n",
    "                action, logprob, value, _ = policy.act(\n",
    "                    state.astype(np.float32), legal_actions, temperature=TEMPERATURE\n",
    "                )\n",
    "                env.current_player = 1\n",
    "                next_state, reward, done = env.step(action)\n",
    "                ep_return += reward\n",
    "\n",
    "                buffer.add(\n",
    "                    state_np=state, action=action, logprob=logprob, value=value,\n",
    "                    reward=reward, done=done, legal_mask_bool=legal_mask\n",
    "                )\n",
    "                last_agent_reward_index = len(buffer.rewards) - 1\n",
    "\n",
    "                steps_collected += 1\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    if env.winner == 0:\n",
    "                        draw_count += 1\n",
    "                    elif env.winner == 1:\n",
    "                        win_count += 1\n",
    "                        episode_win_flag = 1\n",
    "                    else:\n",
    "                        loss_count += 1\n",
    "                    break\n",
    "\n",
    "                # opponent next\n",
    "                next_player = -1\n",
    "\n",
    "            else:\n",
    "                # --- Opponent turn (-1) ---\n",
    "                if lookahead_depth == \"self\":\n",
    "                    # self-play: sample action from current policy for the opponent\n",
    "                    opp_action, _, _, _ = policy.act(\n",
    "                        state.astype(np.float32), env.available_actions(), temperature=TEMPERATURE\n",
    "                    )\n",
    "                else:\n",
    "                    opp_action = select_opponent_action(state, player=-1, depth=lookahead_depth)\n",
    "\n",
    "                if opp_action is None:\n",
    "                    draw_count += 1\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "                env.current_player = -1\n",
    "                state_after_opp, opp_reward, done = env.step(opp_action)\n",
    "\n",
    "                if done:\n",
    "                    if env.winner == 0:\n",
    "                        draw_count += 1\n",
    "                    elif env.winner == -1:\n",
    "                        loss_count += 1\n",
    "                        if ATTR_LOSS_TO_LAST and last_agent_reward_index is not None:\n",
    "                            if 0 <= last_agent_reward_index < len(buffer.rewards):\n",
    "                                buffer.rewards[last_agent_reward_index] += env.LOSS_PENALTY\n",
    "\n",
    "                    state = state_after_opp\n",
    "                    break\n",
    "\n",
    "                state = state_after_opp\n",
    "                next_player = 1\n",
    "\n",
    "            # ---- Trigger PPO update when we've collected enough AGENT decisions ----\n",
    "            if steps_collected >= STEPS_PER_UPDATE:\n",
    "                update_idx += 1\n",
    "                # bootstrap only if next mover would be AGENT\n",
    "                last_done = True\n",
    "                last_value = 0.0\n",
    "                if next_player == 1:\n",
    "                    with torch.no_grad():\n",
    "                        _, v_boot = policy.forward(torch.from_numpy(state.astype(np.float32)))\n",
    "                        last_value = float(v_boot.squeeze().item())\n",
    "                    last_done = False\n",
    "\n",
    "                buffer.compute_gae(last_value=last_value, last_done=last_done)\n",
    "                metrics = ppo_update(policy, optim, buffer, cfg)\n",
    "\n",
    "                # record update metrics at current episode index\n",
    "                ppo_metrics_history[\"episodes\"].append(episode)\n",
    "                for k in [\"loss_pi\",\"loss_v\",\"entropy\",\"approx_kl\",\"clip_frac\",\"explained_variance\"]:\n",
    "                    ppo_metrics_history[k].append(metrics[k])\n",
    "\n",
    "                # reset rollout buffer window\n",
    "                buffer.clear()\n",
    "                steps_collected = 0\n",
    "                last_agent_reward_index = None\n",
    "\n",
    "        # === episode ended ===\n",
    "        reward_history.append(ep_return)\n",
    "        wins_hist.append(win_count); losses_hist.append(loss_count); draws_hist.append(draw_count)\n",
    "        win_history.append(episode_win_flag)   # (use 0.5 instead for draws if you prefer)\n",
    "        epbar.update(1)\n",
    "\n",
    "        # live plot at interval\n",
    "        if episode % plot_interval == 0:\n",
    "            plot_live_training_ppo(\n",
    "                episode=episode,\n",
    "                reward_history=reward_history,\n",
    "                win_history=win_history,\n",
    "                phase_name=current_phase_name,\n",
    "                win_count=win_count, loss_count=loss_count, draw_count=draw_count,\n",
    "                metrics_history=ppo_metrics_history,\n",
    "                title=TRAINING_SESSION,\n",
    "                phases=TRAINING_PHASES,\n",
    "                save=False,\n",
    "                save_path=PLOTS\n",
    "            )\n",
    "\n",
    "        if episode % log_every_x_episode == 0 or episode == 1:\n",
    "            log_summary_stats_ppo(\n",
    "                summary_stats,\n",
    "                episode=episode,\n",
    "                phase_name=current_phase_name,\n",
    "                lookahead_depth=lookahead_depth,\n",
    "                reward_history=reward_history,\n",
    "                win_history=win_history,\n",
    "                win_count=win_count, loss_count=loss_count, draw_count=draw_count,\n",
    "                ppo_metrics_history=ppo_metrics_history,\n",
    "                steps_per_update=STEPS_PER_UPDATE,\n",
    "                lr=optim.param_groups[0][\"lr\"],\n",
    "                # optional for DQN-style columns:\n",
    "                epsilon=None,\n",
    "                strategy_weights=None,\n",
    "            )\n",
    "\n",
    "        # keep state for potential final bootstrap if we stop mid-episode\n",
    "        _state_for_bootstrap = state.copy()\n",
    "        _next_player_for_bootstrap = next_player\n",
    "        _done_for_bootstrap = done\n",
    "\n",
    "# --- Final update on leftover samples (if any) ---\n",
    "if len(buffer) > 0:\n",
    "    last_done = True\n",
    "    last_value = 0.0\n",
    "    if not _done_for_bootstrap and _next_player_for_bootstrap == 1:\n",
    "        with torch.no_grad():\n",
    "            _, v_boot = policy.forward(torch.from_numpy(_state_for_bootstrap.astype(np.float32)))\n",
    "            last_value = float(v_boot.squeeze().item())\n",
    "        last_done = False\n",
    "\n",
    "    buffer.compute_gae(last_value=last_value, last_done=last_done)\n",
    "    metrics = ppo_update(policy, optim, buffer, cfg)\n",
    "\n",
    "    # record final update metrics\n",
    "    ppo_metrics_history[\"episodes\"].append(episode)\n",
    "    for k in [\"loss_pi\",\"loss_v\",\"entropy\",\"approx_kl\",\"clip_frac\",\"explained_variance\"]:\n",
    "        ppo_metrics_history[k].append(metrics[k])\n",
    "\n",
    "# --- Final live plot (saved) ---\n",
    "plot_live_training_ppo(\n",
    "    episode=episode,\n",
    "    reward_history=reward_history,\n",
    "    win_history=win_history,\n",
    "    phase_name=current_phase_name or PHASES.current_name,\n",
    "    win_count=win_count, loss_count=loss_count, draw_count=draw_count,\n",
    "    metrics_history=ppo_metrics_history,\n",
    "    title=TRAINING_SESSION,\n",
    "    phases=TRAINING_PHASES,\n",
    "    save=True,\n",
    "    save_path=PLOTS\n",
    ")\n",
    "\n",
    "print(\"Training finished in {:.1f} min\".format((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da054a-1116-484a-a31c-0a30a8b448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_phase_summary(summary_stats, TRAINING_PHASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fd5d9-dafc-4b91-a6aa-22e11c441361",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "\n",
    "final_reward_fig, final_reward_ax = plt.subplots(figsize=(10, 5))\n",
    "final_reward_ax.plot(smoothed, label=f\"Smoothed Reward (window={window})\", color='blue')\n",
    "\n",
    "# --- Add phase transitions ---\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    ep = meta[\"length\"]\n",
    "    if ep is not None and ep <= len(reward_history):\n",
    "        final_reward_ax.axvline(x=ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_reward_ax.text(ep + 5, max(smoothed) * 0.95, name,\n",
    "                             rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "final_reward_ax.set_title(\"Smoothed Reward Over Episodes\")\n",
    "final_reward_ax.set_xlabel(\"Episode\")\n",
    "final_reward_ax.set_ylabel(\"Smoothed Reward\")\n",
    "final_reward_ax.legend()\n",
    "final_reward_ax.grid(True)\n",
    "final_reward_fig.tight_layout()\n",
    "\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d94ac-c689-4a9e-8f1e-e1d73e19ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame.from_dict(summary_stats, orient=\"index\")\n",
    "df_summary.to_excel(f\"{LOG_DIR}PPO-{TRAINING_SESSION}_summary.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5e697-5315-45ed-9af2-8595d438cab3",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b1a48-39dc-426c-a021-c792eca76ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 ppo_model_{timestamp} episodes-{number_episodes} lookahead-{lookahead_depth}.pt\"\n",
    "default_model_path = \"Connect4 PPQ model.pt\"   \n",
    "\n",
    "model_path, default_model_path = save_checkpoint(\n",
    "    policy=policy,\n",
    "    optim=optim,\n",
    "    episode=episode,\n",
    "    lookahead_depth=lookahead_depth,\n",
    "    cfg=cfg,\n",
    "    hparams=hparams,\n",
    "    model_path=model_path,\n",
    "    default_model_path=default_model_path,\n",
    ")\n",
    "print(f\"Model saved to {model_path}\\nAlso wrote: {default_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70c723-1d5a-4b77-bcb1-6020f55ad23a",
   "metadata": {},
   "source": [
    "## Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cf5b4-2ba5-43a0-9e51-0f84ee92faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"Connect4 PPQ model.pt\"  # EXACT\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    loaded_model, loaded_optim_state, meta = load_checkpoint(load_path, device=DEVICE)\n",
    "\n",
    "loaded_model.eval()\n",
    "print(f\"Model successfully loaded from {load_path}\")\n",
    "if meta:\n",
    "    print(f\"  meta: episode={meta.get('episode')}, lk={meta.get('lookahead_depth')} ts={meta.get('timestamp')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1adc007-acdc-401e-86f5-6734e1263488",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9db9f-7844-43c7-b86b-4885d1f7062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PPO EVALUATION ===\n",
    "#assert 'loaded_model' in globals(), \"loaded_model not found. Load your model first.\"\n",
    "\n",
    "# --- Config ---\n",
    "evaluation_opponents = {\n",
    "    \"Random\": 200,\n",
    "    \"Lookahead-1\": 100,\n",
    "    \"Lookahead-2\": 10,\n",
    "    \"Lookahead-3\": 10,   # raise later if you want\n",
    "    # \"Lookahead-5\": 10,\n",
    "    # \"Lookahead-7\": 5,\n",
    "}\n",
    "\n",
    "TEMPERATURE_EVAL = 0.0     # greedy\n",
    "\n",
    "# Fresh env + lookahead\n",
    "eval_env = Connect4Env()\n",
    "look_eval = Connect4Lookahead()\n",
    "\n",
    "evaluation_results = {}\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for label, num_games in evaluation_opponents.items():\n",
    "        wins = losses = draws = 0\n",
    "        depth = int(label.split(\"-\")[1]) if label.startswith(\"Lookahead\") else None\n",
    "\n",
    "        with tqdm(total=num_games, desc=f\"Opponent: {label}\") as pbar:\n",
    "            for game_index in range(num_games):\n",
    "                state = eval_env.reset()\n",
    "\n",
    "                # --- Alternate who starts; agent is ALWAYS +1 ---\n",
    "                agent_starts = (game_index % 2 == 0)\n",
    "                eval_env.current_player = 1 if agent_starts else -1\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    if eval_env.current_player == 1:\n",
    "                        # Agent move (greedy)\n",
    "                        legal_actions = eval_env.available_actions()\n",
    "                        if not legal_actions:  # extremely rare\n",
    "                            draws += 1\n",
    "                            break\n",
    "                        action, _, _, _ = loaded_model.act(\n",
    "                            state.astype(np.float32),\n",
    "                            legal_actions=legal_actions,\n",
    "                            temperature=TEMPERATURE_EVAL\n",
    "                        )\n",
    "                    else:\n",
    "                        # Opponent move\n",
    "                        legal_actions = eval_env.available_actions()\n",
    "                        if label == \"Random\":\n",
    "                            action = random.choice(legal_actions)\n",
    "                        else:\n",
    "                            a = look_eval.n_step_lookahead(np.array(state), player=-1, depth=depth)\n",
    "                            action = a if a in legal_actions else random.choice(legal_actions)\n",
    "\n",
    "                    # Step env\n",
    "                    state, _, done = eval_env.step(action)\n",
    "\n",
    "                # --- Tally result (agent is +1) ---\n",
    "                if eval_env.winner == 1:\n",
    "                    wins += 1\n",
    "                elif eval_env.winner == -1:\n",
    "                    losses += 1\n",
    "                else:\n",
    "                    draws += 1\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        evaluation_results[label] = {\n",
    "            \"wins\": wins,\n",
    "            \"losses\": losses,\n",
    "            \"draws\": draws,\n",
    "            \"win_rate\": round(wins / num_games, 3),\n",
    "            \"loss_rate\": round(losses / num_games, 3),\n",
    "            \"draw_rate\": round(draws / num_games, 3),\n",
    "        }\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Evaluation completed in {elapsed/60:.1f} minutes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ffdee-8354-400c-87b0-e8c87a25fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text summary ---\n",
    "print(\"📊 Evaluation Summary:\")\n",
    "for label, s in evaluation_results.items():\n",
    "    print(f\"{label:>11}: {s['wins']}W / {s['losses']}L / {s['draws']}D → \"\n",
    "          f\"Win {s['win_rate']*100:.1f}% | Loss {s['loss_rate']*100:.1f}% | Draw {s['draw_rate']*100:.1f}%\")\n",
    "\n",
    "# --- Plot ---\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates  = [evaluation_results[k]['win_rate'] * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %')\n",
    "plt.bar(x + bar_width, loss_rates, width=bar_width, label='Loss %')\n",
    "plt.bar(x + 2*bar_width, draw_rates, width=bar_width, label='Draw %')\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('PPO Agent Performance vs Various Opponents')\n",
    "plt.xticks(x + bar_width, labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Save evaluation table to Excel ---\n",
    "df_eval = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "df_eval.index.name = \"Opponent\"\n",
    "save_path = f\"{LOG_DIR}{TRAINING_SESSION}_PPO_evaluation_results.xlsx\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "df_eval.to_excel(save_path, index=True)\n",
    "print(f\"Evaluation results saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40fe8e-3c07-41d3-84b1-da60664d1ced",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e609264-b56c-4d75-b4b3-f6b9e8f526ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = (total_end_time - begin_start_time) / 3600\n",
    "print(f\"Evaluation completed in {total_elapsed:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5a875-d772-46e2-8e59-400af71c1972",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822f2b3-4c5e-4179-be49-8d58fe7c2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING_SESSION\n",
    "\n",
    "training_log_file = \"PPO Training_Sessions.xlsx\"\n",
    "log_row = {\"TRAINING_SESSION\": TRAINING_SESSION, \"TIME [h]\": total_elapsed, \"EPISODES\": number_episodes}\n",
    "\n",
    "for label, stats in evaluation_results.items():\n",
    "    log_row[label] = stats[\"win_rate\"]\n",
    "\n",
    "# === Load or Create Excel File ===\n",
    "if os.path.exists(training_log_file):\n",
    "    df_log = pd.read_excel(training_log_file)\n",
    "else:\n",
    "    df_log = pd.DataFrame()\n",
    "\n",
    "# === Append and Save ===\n",
    "df_log = pd.concat([df_log, pd.DataFrame([log_row])], ignore_index=True)\n",
    "df_log.to_excel(training_log_file, index=False)\n",
    "\n",
    "print(f\"\\n📁 Training session logged to: {training_log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c0957-66a5-47bc-84d3-74a02a7b19ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
