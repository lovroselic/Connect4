{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b920a5-7a59-4960-af4d-85df558ca3ab",
   "metadata": {},
   "source": [
    "# Connect4 PPQ model\n",
    "By LaughingSkull \n",
    "as new RL agent for my game COnnect-4: https://www.laughingskull.org/Games/Connect4/Connect4.php\n",
    "\n",
    "## PPO (Proximal Policy Optimization)\n",
    "\n",
    "* A policy-based (and often actor-critic) method.\n",
    "* Learns a policy (probability distribution over actions) directly, along with a value function.\n",
    "* Uses clipped objective functions to ensure updates are not too large (for stable learning).\n",
    "* Does on-policy training (uses data from the current policy).\n",
    "\n",
    "### version log   \n",
    "* 0.1.0\n",
    "    * initial, setting up (suffering)\n",
    "* 0.2.0 rstart and refactor\n",
    "    * Fixed1-Reprise - taking setting from DQN\n",
    "* 0.3.0 got rid of DQN approach, pure PPQ with curriculum\n",
    "* 0.4 another restart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617d322-ca54-4f25-85d6-93ac2a55c73f",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "[https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning](https://www.kaggle.com/code/alexisbcook/deep-reinforcement-learning)\n",
    "<br>\n",
    "[https://openai.com/index/openai-baselines-ppo/](https://openai.com/index/openai-baselines-ppo/)\n",
    "<br>\n",
    "[https://github.com/dennybritz/reinforcement-learning](https://github.com/dennybritz/reinforcement-learning)\n",
    "<br>\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "<br>\n",
    "[https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68](https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68)\n",
    "<br>\n",
    "[https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html](https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bbbee-d469-4e6f-ade0-a73cf3a43f81",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8de2b-83d7-487f-94ca-ad1d71edf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import warnings\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16f9f0-2d38-44b4-948a-23517b4d7c1b",
   "metadata": {},
   "source": [
    "### Fixed Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ec004-01f5-4ee2-8f72-38f034b2db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6033b-40d7-4206-8e80-f904a4428592",
   "metadata": {},
   "source": [
    "## Constants, hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513498ca-c36c-4197-b09e-2144271df931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#ENTROPY_COEF = 0.01\n",
    "#ENTROPY_COEF = 0.005\n",
    "ENTROPY_COEF = 0.001\n",
    "#CLIP_RANGE = 0.1\n",
    "CLIP_RANGE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c6675-8a18-42ee-b47e-aab3d9a9933a",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405a67f-62bc-4f8e-9048-eb8fece20d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4.connect4_env import Connect4Env\n",
    "from C4.connect4_lookahead import Connect4Lookahead\n",
    "from PPO.ppo_training_phases_config import TRAINING_PHASES\n",
    "from PPO.actor_critic import ActorCritic\n",
    "from PPO.ppo_memory import PPOMemory\n",
    "from C4.connect4_board_display import Connect4_BoardDisplayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d48db5-5dd9-4542-b2c0-32d85e07737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_ppo(episode):\n",
    "    for name, data in TRAINING_PHASES.items():\n",
    "        if data[\"length\"] is None or episode < data[\"length\"]:\n",
    "            return name, data.get(\"lookahead\"), data.get(\"memory_prune\", 0.0)\n",
    "    return \"Final\", None, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8046a4-c3c2-494a-a34e-c8049566c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lookahead = Connect4Lookahead()\n",
    "\n",
    "def get_opponent_action_ppo(env, agent, state, lookahead_depth):\n",
    "    valid_actions = env.available_actions()\n",
    "\n",
    "    if lookahead_depth is None:\n",
    "        # Random or Self-Play\n",
    "        if agent:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "            return agent.act(state_tensor, valid_actions)[0]\n",
    "        else:\n",
    "            return random.choice(valid_actions)\n",
    "\n",
    "    # Lookahead-based opponent\n",
    "    return Lookahead.n_step_lookahead(state, player=-1, depth=lookahead_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c5fd8-5c72-4639-b80d-5956377653a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase_summary(summary_stats):\n",
    "    # Convert summary to DataFrame\n",
    "    df = pd.DataFrame.from_dict(summary_stats, orient='index')\n",
    "\n",
    "    # Keep last row per phase (for recent stats)\n",
    "    recent_summary = df.groupby(\"phase\").tail(1).set_index(\"phase\")\n",
    "\n",
    "    # Compute total win rate per phase\n",
    "    total_stats = df.groupby(\"phase\")[[\"wins\", \"losses\", \"draws\"]].last()\n",
    "    total_stats[\"total_games\"] = total_stats[[\"wins\", \"losses\", \"draws\"]].sum(axis=1)\n",
    "    total_stats[\"win_rate_total\"] = total_stats[\"wins\"] / total_stats[\"total_games\"]\n",
    "\n",
    "    # Merge with recent summary\n",
    "    merged = recent_summary.join(total_stats[\"win_rate_total\"])\n",
    "\n",
    "    # Sort phases in training order\n",
    "    phase_order = [p for p in TRAINING_PHASES if p in merged.index]\n",
    "    merged = merged.loc[phase_order]\n",
    "\n",
    "    # --- Plot Win Rate: recent vs total ---\n",
    "    x = np.arange(len(merged))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(x - width/2, merged[\"win_rate_25\"], width, label=\"Last 25\")\n",
    "    plt.bar(x + width/2, merged[\"win_rate_total\"], width, label=\"Total\")\n",
    "    plt.xticks(x, merged.index, rotation=30)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Win Rate\")\n",
    "    plt.title(\"Win Rate per Phase (Recent vs Total)\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot Avg Reward (last 25) ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(merged.index, merged[\"avg_reward_25\"], color='steelblue')\n",
    "    plt.ylabel(\"Avg Reward (Last 25)\")\n",
    "    plt.title(\"Average Reward per Phase\")\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc4d62-5511-4ecb-815f-0bd7fab47b51",
   "metadata": {},
   "source": [
    "## Training session name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35877da1-7b13-41f7-a98b-f438eb0ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 500 \n",
    "\n",
    "lookahead_depth = 7  # prophet = 7\n",
    "\n",
    "tag = \"Random\"\n",
    "TRAINING_SESSION = f\"PPQ-{N_EPISODES}-{tag}\"\n",
    "begin_start_time = time.time()\n",
    "LOG_DIR =\"Logs/PPQ/\"\n",
    "MODEL_DIR =\"Models/PPQ/\"\n",
    "PLOTS = \"Plots/PPQ/\"\n",
    "print(\"Start training session\", TRAINING_SESSION)\n",
    "#PRUNE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846bc07-d22a-4e47-9a08-c5f5a58c3103",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbd58d-8866-46c0-a111-3fc10f79053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = ActorCritic()\n",
    "ppo_model.to(DEVICE)\n",
    "summary(ppo_model, input_size=(1, 1, 6, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2077eb-c62e-43b2-ae25-81ca9a9af8df",
   "metadata": {},
   "source": [
    "## Advantage and Return Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcd72f-13df-4f61-801d-f64b16fc060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_and_advantages(rewards, dones, values, gamma=0.99, gae_lambda=0.95):\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    value_next = 0\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        mask = 1.0 - float(dones[step])\n",
    "        delta = rewards[step] + gamma * value_next * mask - values[step]\n",
    "        gae = delta + gamma * gae_lambda * mask * gae\n",
    "        advantages.insert(0, gae)\n",
    "        value_next = values[step]\n",
    "        returns.insert(0, gae + values[step])\n",
    "\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # Normalize advantages for stability\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    return returns, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7cceb0-a9ec-46bf-8520-fc5e23a48ae1",
   "metadata": {},
   "source": [
    "## PPO Loss and Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e18f3-2173-4ea7-b468-8fa18307a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(model, optimizer, states, actions, old_log_probs, returns, advantages,\n",
    "               valid_actions_list=None,\n",
    "               clip_range=CLIP_RANGE, entropy_coef=ENTROPY_COEF, value_coef=0.5):\n",
    "    model.train()\n",
    "\n",
    "    states = torch.stack(states).to(DEVICE)\n",
    "    actions = torch.tensor(actions).to(DEVICE)\n",
    "    old_log_probs = torch.stack(old_log_probs).to(DEVICE)\n",
    "    returns = returns.to(DEVICE)\n",
    "    advantages = advantages.to(DEVICE)\n",
    "\n",
    "    # Forward pass through policy\n",
    "    logits, values = model(states)\n",
    "\n",
    "    if valid_actions_list is not None:\n",
    "        for i, valid in enumerate(valid_actions_list):\n",
    "            mask = torch.full_like(logits[i], -float('inf'))\n",
    "            mask[valid] = 0\n",
    "            logits[i] += mask\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    dist = Categorical(probs)\n",
    "    new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "    # PPO clipped surrogate objective\n",
    "    ratio = (new_log_probs - old_log_probs).exp()\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "    surrogate1 = ratio * advantages\n",
    "    surrogate2 = clipped_ratio * advantages\n",
    "    policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "    # âœ… Value function clipping\n",
    "    values = values.squeeze(-1)\n",
    "    old_values = values.detach()  # no separate storage yet\n",
    "    value_clipped = old_values + (values - old_values).clamp(-clip_range, clip_range)\n",
    "    value_loss_unclipped = F.mse_loss(values, returns, reduction='none')\n",
    "    value_loss_clipped = F.mse_loss(value_clipped, returns, reduction='none')\n",
    "    value_loss = torch.max(value_loss_unclipped, value_loss_clipped).mean()\n",
    "\n",
    "    # Entropy bonus\n",
    "    entropy = dist.entropy().mean()\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss.item(), policy_loss.item(), value_loss.item(), entropy.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d822e-b348-46a2-8684-a965417d423f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe38f4-6a09-4354-8840-ed0f841ae7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_episode(env, model, memory, episode, lookahead_depth):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    player = random.choice([1, -1])\n",
    "    final_result = None\n",
    "    pending_store = None\n",
    "\n",
    "    while not done:\n",
    "        if player == 1:\n",
    "            # Agent's turn\n",
    "            valid_actions = env.available_actions()\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "            action, log_prob, value = model.act(state_tensor, valid_actions)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                if env.winner == 1:\n",
    "                    memory.store(\n",
    "                        state_tensor.squeeze(0).cpu(), action, log_prob, reward, done, value, valid_actions\n",
    "                    )\n",
    "                final_result = 1 if env.winner == 1 else -1 if env.winner == -1 else 0.5\n",
    "                break\n",
    "            else:\n",
    "                pending_store = (\n",
    "                    state_tensor.squeeze(0).cpu(), action, log_prob, reward, done, value, valid_actions\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # Opponent's turn â€” fixed strategy from phase (lookahead or self-play)\n",
    "            action = get_opponent_action_ppo(\n",
    "                env=env,\n",
    "                agent=None if lookahead_depth is not None else model,\n",
    "                state=state,\n",
    "                lookahead_depth=lookahead_depth\n",
    "            )\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                final_result = 1 if env.winner == 1 else -1 if env.winner == -1 else 0.5\n",
    "                break\n",
    "            else:\n",
    "                if pending_store is not None:\n",
    "                    memory.store(*pending_store)\n",
    "                    pending_store = None\n",
    "\n",
    "        state = next_state\n",
    "        player *= -1\n",
    "\n",
    "    return final_result, env.board.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8416fd2-4d8a-4483-9d1f-4f2b6f8f6a1f",
   "metadata": {},
   "source": [
    "### Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ff4db-d294-41fe-b8db-7a8d3bb0c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, model, optimizer, phase_config, episodes=1000, batch_size=BATCH_SIZE):\n",
    "    memory = PPOMemory()\n",
    "    episode_rewards = []\n",
    "    reward_history = []\n",
    "    win_history = []\n",
    "    summary_stats = {}\n",
    "    win_count = loss_count = draw_count = 0\n",
    "\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "\n",
    "    phase = \"Random\"\n",
    "\n",
    "    pbar = tqdm(total=episodes, desc=\"PPO Training\")\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Get phase settings\n",
    "        new_phase, lookahead_depth, memory_prune = get_phase_ppo(episode)\n",
    "        if new_phase != phase:\n",
    "            phase = new_phase\n",
    "\n",
    "        # Train one episode\n",
    "        final_result, final_board = train_one_episode(\n",
    "            env=env,\n",
    "            model=model,\n",
    "            memory=memory,\n",
    "            episode=episode,\n",
    "            lookahead_depth=lookahead_depth\n",
    "        )\n",
    "\n",
    "        # Track results\n",
    "        episode_rewards.append(final_result)\n",
    "        reward_history.append(final_result)\n",
    "\n",
    "        if final_result == 1:\n",
    "            win_count += 1\n",
    "            win_history.append(1)\n",
    "        elif final_result == -1:\n",
    "            loss_count += 1\n",
    "            win_history.append(0)\n",
    "        elif final_result == 0.5:\n",
    "            draw_count += 1\n",
    "            win_history.append(0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid final_result\")\n",
    "\n",
    "        # PPO update\n",
    "        if (episode + 1) % batch_size == 0:\n",
    "            returns, advantages = compute_returns_and_advantages(\n",
    "                memory.rewards, memory.dones, memory.values\n",
    "            )\n",
    "            total_loss, policy_loss, value_loss, entropy = ppo_update(\n",
    "                model, optimizer,\n",
    "                memory.states, memory.actions, memory.log_probs,\n",
    "                returns, advantages\n",
    "            )\n",
    "            policy_losses.append(policy_loss)\n",
    "            value_losses.append(value_loss)\n",
    "            entropies.append(entropy)\n",
    "            memory.clear()\n",
    "\n",
    "        # Summary\n",
    "        if (episode + 1) % 25 == 0:\n",
    "            recent_rewards = reward_history[-25:]\n",
    "            recent_win_rate = np.mean(win_history[-25:])\n",
    "            summary_stats[episode + 1] = {\n",
    "                \"phase\": phase,\n",
    "                \"lookahead\": lookahead_depth,\n",
    "                \"wins\": win_count,\n",
    "                \"losses\": loss_count,\n",
    "                \"draws\": draw_count,\n",
    "                \"avg_reward_25\": round(np.mean(recent_rewards), 2),\n",
    "                \"win_rate_25\": round(recent_win_rate, 2)\n",
    "            }\n",
    "\n",
    "        # Live plotting\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            Connect4_BoardDisplayer.display_board(final_board)\n",
    "\n",
    "            fig, ax = plt.subplots(5, 1, figsize=(12, 16), sharex=True)\n",
    "\n",
    "            # Reward\n",
    "            ax[0].plot(reward_history, label='Reward')\n",
    "            if len(reward_history) >= 25:\n",
    "                avg = np.convolve(reward_history, np.ones(25)/25, mode='valid')\n",
    "                ax[0].plot(range(24, len(reward_history)), avg, label='25-ep Moving Avg', linewidth=2)\n",
    "            ax[0].set_ylabel(\"Reward\")\n",
    "            ax[0].legend()\n",
    "            ax[0].grid(True)\n",
    "\n",
    "            # Win rate\n",
    "            if len(win_history) >= 25:\n",
    "                win_avg = np.convolve(win_history, np.ones(25)/25, mode='valid')\n",
    "                ax[1].plot(range(24, len(win_history)), win_avg, label='Win Rate (25 ep)', color='green')\n",
    "            ax[1].set_ylabel(\"Win Rate\")\n",
    "            ax[1].legend()\n",
    "            ax[1].grid(True)\n",
    "\n",
    "            # x values at PPO updates\n",
    "            update_steps = list(range(batch_size - 1, episode + 1, batch_size))\n",
    "\n",
    "            # Policy Loss\n",
    "            ax[2].plot(update_steps, policy_losses, label='Policy Loss', color='orange')\n",
    "            if len(policy_losses) >= 5:\n",
    "                policy_losses_avg = np.convolve(policy_losses, np.ones(5)/5, mode='valid')\n",
    "                ax[2].plot(update_steps[4:], policy_losses_avg, label='Policy loss MA (5)', color='orange', linestyle='--')\n",
    "            ax[2].set_ylabel(\"Policy Loss\")\n",
    "            ax[2].legend()\n",
    "            ax[2].grid(True)\n",
    "\n",
    "            # Value Loss\n",
    "            ax[3].plot(update_steps, value_losses, label='Value Loss', color='blue')\n",
    "            ax[3].set_ylabel(\"Value Loss\")\n",
    "            ax[3].legend()\n",
    "            ax[3].grid(True)\n",
    "\n",
    "            # Entropy\n",
    "            ax[4].plot(update_steps, entropies, label='Entropy', color='purple')\n",
    "            if len(entropies) >= 5:\n",
    "                entropy_avg = np.convolve(entropies, np.ones(5)/5, mode='valid')\n",
    "                ax[4].plot(update_steps[4:], entropy_avg, label='Entropy MA (5)', color='magenta', linestyle='--')\n",
    "            ax[4].set_ylabel(\"Entropy\")\n",
    "            ax[4].set_xlabel(\"Episode\")\n",
    "            ax[4].legend()\n",
    "            ax[4].grid(True)\n",
    "\n",
    "            # Phase vertical lines\n",
    "            for name, meta in phase_config.items():\n",
    "                ep_limit = meta[\"length\"]\n",
    "                if ep_limit is not None and ep_limit <= episode:\n",
    "                    for axis in ax:\n",
    "                        axis.axvline(ep_limit, color='black', linestyle='dotted', linewidth=1)\n",
    "                        axis.text(ep_limit + 2, axis.get_ylim()[1]*0.95, name,\n",
    "                                  rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "            fig.suptitle(f\"Episode {episode+1} â€” Phase: {phase} | Wins: {win_count}, Losses: {loss_count}, Draws: {draw_count}\")\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            display(fig)\n",
    "            plt.close()\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return episode_rewards, summary_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519b0de-c99d-4179-aa16-ac44ce5478e1",
   "metadata": {},
   "source": [
    "# PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba501813-293e-4fa0-8cb4-e3a753ffcade",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Environment and Model Setup ---\n",
    "env = Connect4Env()\n",
    "model = ActorCritic().to(DEVICE)\n",
    "#Learning_rate = 3e-4 \n",
    "Learning_rate = 5e-4\n",
    "#2.5e-4\n",
    "# or 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Train PPO Agent with Curriculum Phases ---\n",
    "episode_rewards, summary_stats = train_ppo(\n",
    "    env=env,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    phase_config=TRAINING_PHASES,\n",
    "    episodes=N_EPISODES,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes ({elapsed / N_EPISODES:.2f} s/episode)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a4fde6-0762-4442-b3c6-748ca216e762",
   "metadata": {},
   "source": [
    "### Final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322049d-9c8c-438f-8fcf-96594a9ce335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot Training Progress with Phase Transitions ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards, label='Episode Reward')\n",
    "\n",
    "# Add moving average\n",
    "if len(episode_rewards) >= 25:\n",
    "    avg = np.convolve(episode_rewards, np.ones(25)/25, mode='valid')\n",
    "    plt.plot(range(24, len(episode_rewards)), avg, label='25-ep Moving Avg', color='orange', linewidth=2)\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Final Reward')\n",
    "plt.title('PPO Training Reward with Phase Transitions')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Draw vertical phase transitions\n",
    "for name, cfg in TRAINING_PHASES.items():\n",
    "    ep = cfg[\"length\"]\n",
    "    if ep is not None and ep < len(episode_rewards):\n",
    "        plt.axvline(ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        plt.text(ep + 2, plt.ylim()[1]*0.95, name, rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "reward_plot_path = f\"{PLOTS}PPQ-{TRAINING_SESSION}_reward_plot.png\"\n",
    "plt.savefig(reward_plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“ˆ Reward plot saved to: {reward_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da054a-1116-484a-a31c-0a30a8b448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_phase_summary(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fd5d9-dafc-4b91-a6aa-22e11c441361",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200\n",
    "smoothed = [np.mean(episode_rewards[max(0, i-window):i+1]) for i in range(len(episode_rewards))]\n",
    "\n",
    "final_reward_fig, final_reward_ax = plt.subplots(figsize=(10, 5))\n",
    "final_reward_ax.plot(smoothed, label=f\"Smoothed Reward (window={window})\", color='blue')\n",
    "\n",
    "# --- Add phase transitions ---\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    ep = meta[\"length\"]\n",
    "    if ep is not None and ep <= len(episode_rewards):\n",
    "        final_reward_ax.axvline(x=ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_reward_ax.text(ep + 5, max(smoothed) * 0.95, name,\n",
    "                             rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "final_reward_ax.set_title(\"Smoothed Reward Over Episodes\")\n",
    "final_reward_ax.set_xlabel(\"Episode\")\n",
    "final_reward_ax.set_ylabel(\"Smoothed Reward\")\n",
    "final_reward_ax.legend()\n",
    "final_reward_ax.grid(True)\n",
    "final_reward_fig.tight_layout()\n",
    "\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d94ac-c689-4a9e-8f1e-e1d73e19ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame.from_dict(summary_stats, orient=\"index\")\n",
    "df_summary.to_excel(f\"{LOG_DIR}PPO-{TRAINING_SESSION}_summary.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5e697-5315-45ed-9af2-8595d438cab3",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b1a48-39dc-426c-a021-c792eca76ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 ppo_model_{timestamp} episodes-{N_EPISODES} lookahead-{lookahead_depth}.pt\"\n",
    "default_model_path = \"Connect4 PPQ model.pt\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "torch.save(model.state_dict(), default_model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70c723-1d5a-4b77-bcb1-6020f55ad23a",
   "metadata": {},
   "source": [
    "## Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cf5b4-2ba5-43a0-9e51-0f84ee92faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = ActorCritic().to(DEVICE)\n",
    "load_path = default_model_path\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    loaded_model.load_state_dict(torch.load(load_path, map_location=DEVICE))\n",
    "\n",
    "loaded_model.eval()\n",
    "print(f\"Model successfully loaded from {load_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1adc007-acdc-401e-86f5-6734e1263488",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9db9f-7844-43c7-b86b-4885d1f7062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION CONFIGURATION ===\n",
    "evaluation_opponents = {\n",
    "    \"Random\": 100,\n",
    "    \"Lookahead-1\": 100,\n",
    "    \"Lookahead-2\": 100,\n",
    "    \"Lookahead-3\": 25,\n",
    "    # \"Lookahead-5\": 10, # too soon to test\n",
    "    # \"Lookahead-7\": 5  # too soon to test\n",
    "}\n",
    "\n",
    "evaluation_results = {}\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for label, num_games in evaluation_opponents.items():\n",
    "    wins = losses = draws = 0\n",
    "    depth = int(label.split(\"-\")[1]) if label.startswith(\"Lookahead\") else None\n",
    "\n",
    "    with tqdm(total=num_games, desc=f\"Opponent: {label}\") as pbar:\n",
    "        for _ in range(num_games):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            agent_first = random.choice([True, False])\n",
    "\n",
    "            while not done:\n",
    "                if (env.current_player == 1 and agent_first) or (env.current_player == -1 and not agent_first):\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "                    action, _, _ = loaded_model.act(state_tensor)\n",
    "                else:\n",
    "                    if label == \"Random\":\n",
    "                        action = random.choice(env.available_actions())\n",
    "                    else:\n",
    "                        action = Lookahead.n_step_lookahead(state, env.current_player, depth=depth)\n",
    "\n",
    "                state, reward, done = env.step(action)\n",
    "\n",
    "            winner = -env.current_player\n",
    "            if winner == 1:\n",
    "                if agent_first:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "            elif winner == -1:\n",
    "                if not agent_first:\n",
    "                    wins += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    evaluation_results[label] = {\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"draws\": draws,\n",
    "        \"win_rate\": round(wins / num_games, 2),\n",
    "        \"loss_rate\": round(losses / num_games, 2),\n",
    "        \"draw_rate\": round(draws / num_games, 2)\n",
    "    }\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Evaluation completed in {elapsed/60:.1f} minutes\")\n",
    "\n",
    "print(\"\\nðŸ“Š Evaluation Summary:\")\n",
    "for label, stats in evaluation_results.items():\n",
    "    print(f\"{label}: {stats['wins']}W / {stats['losses']}L / {stats['draws']}D â†’ \"\n",
    "          f\"Win Rate: {stats['win_rate']*100:.1f}%, Loss: {stats['loss_rate']*100:.1f}%, Draws: {stats['draw_rate']*100:.1f}%\")\n",
    "\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates = [evaluation_results[k]['win_rate'] * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %', color='green')\n",
    "plt.bar([i + bar_width for i in x], loss_rates, width=bar_width, label='Loss %', color='red')\n",
    "plt.bar([i + 2 * bar_width for i in x], draw_rates, width=bar_width, label='Draw %', color='gray')\n",
    "\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('PPO Agent Performance vs Various Opponents')\n",
    "plt.xticks([i + bar_width for i in x], labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save evaluation results\n",
    "df_eval = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "df_eval.index.name = \"Opponent\"\n",
    "df_eval.to_excel(f\"{LOG_DIR}{TRAINING_SESSION}_PPO_evaluation_results.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40fe8e-3c07-41d3-84b1-da60664d1ced",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e609264-b56c-4d75-b4b3-f6b9e8f526ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = (total_end_time - begin_start_time) / 3600\n",
    "print(f\"Evaluation completed in {total_elapsed:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e5a875-d772-46e2-8e59-400af71c1972",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822f2b3-4c5e-4179-be49-8d58fe7c2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING_SESSION\n",
    "\n",
    "training_log_file = \"PPO Training_Sessions.xlsx\"\n",
    "log_row = {\"TRAINING_SESSION\": TRAINING_SESSION, \"TIME [h]\": total_elapsed, \"EPISODES\": N_EPISODES}\n",
    "\n",
    "for label, stats in evaluation_results.items():\n",
    "    log_row[label] = stats[\"win_rate\"]\n",
    "\n",
    "# === Load or Create Excel File ===\n",
    "if os.path.exists(training_log_file):\n",
    "    df_log = pd.read_excel(training_log_file)\n",
    "else:\n",
    "    df_log = pd.DataFrame()\n",
    "\n",
    "# === Append and Save ===\n",
    "df_log = pd.concat([df_log, pd.DataFrame([log_row])], ignore_index=True)\n",
    "df_log.to_excel(training_log_file, index=False)\n",
    "\n",
    "print(f\"\\nðŸ“ Training session logged to: {training_log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c0957-66a5-47bc-84d3-74a02a7b19ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
