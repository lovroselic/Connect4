{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1db620-d548-4056-b511-e9ac63c7dc6c",
   "metadata": {},
   "source": [
    "# Connect4 DQN model\n",
    "By LaughingSkull \n",
    "as new RL agent for my game COnnect-4: https://www.laughingskull.org/Games/Connect4/Connect4.php\n",
    "\n",
    "\n",
    "## DQN (Deep Q-Network)\n",
    "\n",
    "* A value-based reinforcement learning method.\n",
    "* Uses a neural network to approximate the Q-function: estimates the expected future rewards for taking actions in given states.\n",
    "* Learns via Q-learning: updates the Q-values using the Bellman equation.\n",
    "* Typically uses techniques like experience replay and target networks to stabilize training.\n",
    "\n",
    "### version log    \n",
    "\n",
    "* 0.8.0 - start with new convolutional part\n",
    "    * keeping L2 regularization\n",
    "* 0.9.0 - extension to shallow training\n",
    "* 0.10.0 - extension to L3 training\n",
    "    * NO PRUNE:\n",
    "        * worse \n",
    "    * SHARPer PRUNE\n",
    "    * extend phases\n",
    "        * MixedR12: no improvement; &cross;\n",
    "        * Shallow\n",
    "        * Fixed2\n",
    "        * Variable23\n",
    "        * Variable3\n",
    "* 0.11.0 - recover fixed 2\n",
    "    * corrected reward leaking bug\n",
    "    * corrected draw bug\n",
    "* 0.12.0 refactoring and restarting\n",
    "* 0.13 restarting again\n",
    "    * strategy weights djustment\n",
    "* 0.14 new restart from scratch\n",
    "    * intermediate phases\n",
    "* 0.15 shaped rewards\n",
    "* 0.16 testing target_update_interval\n",
    "* 0.17 changing CNN\n",
    "* 0.18 8-part weights approach, rebalancing and restarting (again)\n",
    "    * frozen model for self play\n",
    "    * adding mirrored rewards\n",
    "    * adding player POV then getting rid of it\n",
    "        * splitting agent  and oppo players channel\n",
    "* 0.19 restart with tweaking weights (again)\n",
    "    * new ENV step weights\n",
    "    * added forking and fork blocking\n",
    "    * included forks in lookahead\n",
    "* 0.20 adding PER - prioritized experience replay\n",
    "    * retuning phases and weights (yet again)\n",
    "    * adding immediate win penalty in env\n",
    "* 0.21 3-step returns\n",
    "* 0.22 tactical guard\n",
    "    * random 10%\n",
    "    * changed evaluation\n",
    "    * restart\n",
    "    * rewards less defensive as before, but more as attack\n",
    "    * corrected pov in n-step\n",
    "* 0.23 tune to stabilize TD error, TU tuning\n",
    "* 0.24 training phase tuning\n",
    "    * removed history based memory prune\n",
    "    * priority based memory pruning\n",
    "* 0.25 optimistic weight initialization\n",
    "    * small penalty per step - train faster bastard!\n",
    "* 0.30 seeded start using game states from DATA generator\n",
    "* 0.31 per partes training\n",
    "* 0.32 another model change\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56d151-4c55-4871-b5c6-5a91c75cab0a",
   "metadata": {},
   "source": [
    "## Links, learning from\n",
    "\n",
    "[https://github.com/t-brewer/connect4_CNN](https://github.com/t-brewer/connect4_CNN)\n",
    "<br>\n",
    "\n",
    "[https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "<br>\n",
    "[https://pettingzoo.farama.org/tutorials/agilerl/DQN/](https://pettingzoo.farama.org/tutorials/agilerl/DQN/)\n",
    "<br>\n",
    "[https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/](https://www.geeksforgeeks.org/machine-learning/understanding-prioritized-experience-replay/)\n",
    "<br>\n",
    "### Other helpful links\n",
    "\n",
    "<br>[https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967](https://medium.com/@vishwapatel214/building-a-connect-4-game-bot-with-deep-learning-models-dbcd019d8967)\n",
    "<br>[https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.](https://codebox.net/pages/connect4#:~:text=This%20requires%20a%20lot%20of%20work%20up%2Dfront,possible%20action%20at%20each%20step%20is%20impractical.)\n",
    "<br>[https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041](https://medium.com/advanced-machine-learning/deep-learning-meets-board-games-creating-a-connect-4-ai-using-cnns-and-vits-89c8cdab0041)\n",
    "<br>[https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.](https://medium.com/@piyushkashyap045/understanding-dropout-in-deep-learning-a-guide-to-reducing-overfitting-26cbb68d5575#:~:text=Choosing%20Dropout%20Rate:%20Common%20dropout,is%20better%20for%20simpler%20models.)\n",
    "<br>\n",
    "[https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68](https://medium.com/oracledevs/lessons-from-alphazero-connect-four-e4a0ae82af68)\n",
    "<br>\n",
    "[https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html](https://docs.agilerl.com/en/latest/tutorials/pettingzoo/dqn.html)\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5dd4e-2529-4d7c-9e38-7bef1135eff8",
   "metadata": {},
   "source": [
    "## Import dependecies and recheck installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b768e305-7069-4d06-9530-a2d5617560eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies imported successfully.\n",
      "torch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "GPU name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pprint;\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "print(\"All dependencies imported successfully.\")\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced052f-8ccc-4e0f-a8cd-dcf6c27a7446",
   "metadata": {},
   "source": [
    "### Fixed Random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b36059b-73c8-4356-ba7a-037473dca494",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4653a09a-6263-4cdf-ad4d-ca960a72645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR =\"Logs/DQN/\"\n",
    "MODEL_DIR =\"Models/DQN/\"\n",
    "PLOTS = \"Plots/DQN/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f9655-bbba-4950-9260-548b64261ac9",
   "metadata": {},
   "source": [
    "## Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b037ed-2332-4851-ae94-945d511deeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from C4.connect4_env import Connect4Env\n",
    "from C4.connect4_lookahead import Connect4Lookahead\n",
    "from DQN.training_phases_config import TRAINING_PHASES\n",
    "from DQN.training_phases_config import set_training_phases_length\n",
    "from DQN.opponent_action import get_opponent_action\n",
    "from DQN.DQN_replay_memory_per import PrioritizedReplayMemory\n",
    "from DQN.dqn_model import DQN\n",
    "from DQN.dqn_agent import DQNAgent\n",
    "from DQN.dqn_utilities import *\n",
    "from C4.connect4_board_display import Connect4_BoardDisplayer\n",
    "from C4.plot_phase_summary import plot_phase_summary\n",
    "from DQN.nstep_buffer import NStepBuffer\n",
    "from DQN.eval_utilities import build_input, evaluate_with_leaks\n",
    "from DQN.TD_error import *\n",
    "from DQN.c4_seed_util import seed_from_dataframe\n",
    "\n",
    "Lookahead = Connect4Lookahead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024c11d-10d5-4f35-bb88-5b822e3551b0",
   "metadata": {},
   "source": [
    "# Training phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5597e001-442c-49ce-934d-7577b778e007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>duration</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>epsilon_min</th>\n",
       "      <th>memory_prune_low</th>\n",
       "      <th>sumWeights</th>\n",
       "      <th>sumOppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SelfPlay_L1_init</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2_Attack</th>\n",
       "      <td>1250</td>\n",
       "      <td>750</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SelfPlay_L2</th>\n",
       "      <td>1750</td>\n",
       "      <td>500</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final</th>\n",
       "      <td>1751</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_training_phases_length(TRAINING_PHASES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0589098-7517-4db7-bfb2-04a24f2d8e77",
   "metadata": {},
   "source": [
    "# Training session name and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84eb8661-d4cf-4cce-a79f-9100c87e6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training session SelfPlay_L2_Attack1-500-TU-500-BS-128\n"
     ]
    }
   ],
   "source": [
    "lookahead_depth = 7  # prophet = 7\n",
    "num_episodes = 500\n",
    "batch_size = 128\n",
    "target_update_interval = 500 # this has changed from episodes to env steps\n",
    "#target_update_interval = 500\n",
    "plot_interval = 10\n",
    "log_every_x_episode = 100\n",
    "tag = \"SelfPlay_L2_Attack1\"\n",
    "\n",
    "TRAINING_SESSION = f\"{tag}-{num_episodes}-TU-{target_update_interval}-BS-{batch_size}\"\n",
    "begin_start_time = time.time()\n",
    "\n",
    "print(\"Started training session\", TRAINING_SESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75673c-d12f-4749-947c-bbf08cf5a2cb",
   "metadata": {},
   "source": [
    "### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125b1923-c3e5-49b5-9d35-d31d37263051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config written: {'excel': 'Logs/DQN/DQN-SelfPlay_L2_Attack1-500-TU-500-BS-128_training_config.xlsx'}\n"
     ]
    }
   ],
   "source": [
    "# --- Save training configuration to Excel ---\n",
    "from C4.training_config_logger import export_training_config\n",
    "\n",
    "paths = export_training_config(\n",
    "    training_phases=TRAINING_PHASES,\n",
    "    lookahead_depth=lookahead_depth,\n",
    "    num_episodes=num_episodes,\n",
    "    batch_size=batch_size,\n",
    "    target_update_interval=target_update_interval,\n",
    "    log_dir=LOG_DIR,\n",
    "    session_name=TRAINING_SESSION,\n",
    "    write_excel=True,    # set False if you only want CSV/JSON\n",
    "    write_json=False,     # handy for exact reproduction later\n",
    ")\n",
    "\n",
    "print(\"config written:\", paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59762c43-70fe-4a7b-9e8e-d5da65d7e628",
   "metadata": {},
   "source": [
    "### Model overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b42c8fc-51bd-4cb5-af36-0d21b93a6a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DQN                                      [1, 7]                    --\n",
       "â”œâ”€Sequential: 1-1                        [1, 64, 2, 2]             --\n",
       "â”‚    â””â”€Conv2d: 2-1                       [1, 32, 5, 6]             544\n",
       "â”‚    â””â”€ReLU: 2-2                         [1, 32, 5, 6]             --\n",
       "â”‚    â””â”€Conv2d: 2-3                       [1, 64, 5, 6]             18,496\n",
       "â”‚    â””â”€ReLU: 2-4                         [1, 64, 5, 6]             --\n",
       "â”‚    â””â”€Conv2d: 2-5                       [1, 64, 4, 5]             65,600\n",
       "â”‚    â””â”€ReLU: 2-6                         [1, 64, 4, 5]             --\n",
       "â”‚    â””â”€MaxPool2d: 2-7                    [1, 64, 2, 2]             --\n",
       "â”œâ”€AdaptiveAvgPool2d: 1-2                 [1, 64, 1, 1]             --\n",
       "â”œâ”€Sequential: 1-3                        [1, 1]                    --\n",
       "â”‚    â””â”€Linear: 2-8                       [1, 128]                  8,320\n",
       "â”‚    â””â”€ReLU: 2-9                         [1, 128]                  --\n",
       "â”‚    â””â”€Linear: 2-10                      [1, 1]                    129\n",
       "â”œâ”€Sequential: 1-4                        [1, 7]                    --\n",
       "â”‚    â””â”€Linear: 2-11                      [1, 128]                  8,320\n",
       "â”‚    â””â”€ReLU: 2-12                        [1, 128]                  --\n",
       "â”‚    â””â”€Linear: 2-13                      [1, 7]                    903\n",
       "==========================================================================================\n",
       "Total params: 102,312\n",
       "Trainable params: 102,312\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.90\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.41\n",
       "Estimated Total Size (MB): 0.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model = DQN()\n",
    "summary(_model, input_size=(1, 4, 6, 7))  # batch=1, channels=4, height=6, width=7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07130d-5ebe-491c-992b-80af316c1879",
   "metadata": {},
   "source": [
    "# Loading Lookahead play data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceeb476c-e3bc-4e28-a4fe-91adeba41a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>winner</th>\n",
       "      <th>0-0</th>\n",
       "      <th>0-1</th>\n",
       "      <th>0-2</th>\n",
       "      <th>0-3</th>\n",
       "      <th>0-4</th>\n",
       "      <th>0-5</th>\n",
       "      <th>0-6</th>\n",
       "      <th>1-0</th>\n",
       "      <th>...</th>\n",
       "      <th>4-4</th>\n",
       "      <th>4-5</th>\n",
       "      <th>4-6</th>\n",
       "      <th>5-0</th>\n",
       "      <th>5-1</th>\n",
       "      <th>5-2</th>\n",
       "      <th>5-3</th>\n",
       "      <th>5-4</th>\n",
       "      <th>5-5</th>\n",
       "      <th>5-6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>L7L6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>L6L7</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>L6L7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>L7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>L7</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  winner  0-0  0-1  0-2  0-3  0-4  0-5  0-6  1-0  ...  4-4  4-5  4-6  \\\n",
       "65  L7L6       0    1   -1   -1    1   -1    1   -1   -1  ...    1   -1    1   \n",
       "66  L6L7      -1   -1    0   -1    1    1    1   -1    1  ...   -1   -1   -1   \n",
       "67  L6L7       1    0    0    0   -1    0    0    0    0  ...    0    0   -1   \n",
       "68    L7      -1    0    1   -1   -1   -1    0   -1    0  ...    1   -1   -1   \n",
       "69    L7      -1    1    0    1   -1    1    1   -1   -1  ...    1   -1   -1   \n",
       "\n",
       "    5-0  5-1  5-2  5-3  5-4  5-5  5-6  \n",
       "65   -1    1    1    1   -1    1   -1  \n",
       "66    1    0   -1    1    1   -1    1  \n",
       "67   -1    1   -1    1    0    0   -1  \n",
       "68    0    1   -1    1   -1    1    1  \n",
       "69   -1    1   -1    1   -1    1    1  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILE = \"C4.data.xlsx\"\n",
    "DATA = pd.read_excel(DATA_FILE)\n",
    "BOARD_COLS = [f\"{r}-{c}\" for r in range(6) for c in range(7)]\n",
    "DATA[\"winner\"] = DATA[\"winner\"].astype(\"int8\")\n",
    "DATA[BOARD_COLS] = DATA[BOARD_COLS].astype(\"int8\")\n",
    "DATA.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087f0085-9180-4849-ab83-01b0c61fcb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 70\n",
      "Winners distribution:\n",
      " winner\n",
      "-1    29\n",
      " 0     8\n",
      " 1    33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {len(DATA)}\")\n",
    "print(\"Winners distribution:\\n\", DATA[\"winner\"].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d81c26-194c-4b97-8578-4333d99f85bc",
   "metadata": {},
   "source": [
    "## Training loop - DQN against lookahead opponent (Prophet-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa44b92-350d-4733-b718-5f2bdec9f0c9",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85828dad-898a-4e57-b167-1a7755a4e855",
   "metadata": {},
   "source": [
    "#### Loading L1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f3c5f2-7346-4d62-b666-0eb10c679738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "341ed919-650d-42ed-9544-7382481397bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(device=device)  # Fresh agent instance\n",
    "#state_dict = torch.load(\"DQN_L1.pt\", map_location=device, weights_only=True)\n",
    "#agent.model.load_state_dict(state_dict)\n",
    "#agent.update_target_model()\n",
    "#agent.epsilon = 0.0  \n",
    "#print(\"âœ… L1 model loaded and ready for further training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63557d48-e56a-4619-a050-f20eca02b47b",
   "metadata": {},
   "source": [
    "#### Seeding from lookahed games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cecba7-c880-4948-83b4-18e74b59c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seeding:   6%|â–Œ         | 4/70 [00:00<00:13,  4.90it/s]"
     ]
    }
   ],
   "source": [
    "env = Connect4Env()\n",
    "\n",
    "N_STEP = 3\n",
    "seed_start_time = time.time()\n",
    "agent.memory.begin_seeding()\n",
    "nstep_buf = seed_from_dataframe(DATA, agent, n_step=N_STEP, gamma=agent.gamma, verbose=True)\n",
    "agent.memory.end_seeding()\n",
    "seed_end_time = time.time()\n",
    "seed_elapsed = seed_end_time - seed_start_time\n",
    "print(f\"Seeding completed in {seed_elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d5faa-e492-4d29-8a63-814d3baa76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = {}  \n",
    "reward_history = []\n",
    "win_history = []\n",
    "epsilon_history = []\n",
    "epsilon_min_history = []\n",
    "memory_prune_low_history = []\n",
    "\n",
    "win_count = loss_count = draw_count = 0\n",
    "phase = None\n",
    "frozen_opp = None\n",
    "strategy_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334f9db-3bdb-432c-96c4-b1db012333eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training loop  ===\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with tqdm(total=num_episodes, desc=\"Training Episodes\") as pbar:\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        nstep_buf.reset()  # important: clear rolling window per episode\n",
    "\n",
    "        total_reward = 0      # terminal reward only\n",
    "        done = False\n",
    "        final_result = None   # 1 win, -1 loss, 0.5 draw\n",
    "        env_steps = 0\n",
    "\n",
    "        # --- handle phase & hyperparams ---\n",
    "        new_phase, strategy_weights, epsilon, memory_prune_low, epsilon_min = get_phase(episode)\n",
    "        phase, frozen_opp = handle_phase_change(\n",
    "            agent, new_phase, phase, epsilon,\n",
    "            memory_prune_low, epsilon_min, frozen_opp\n",
    "        )\n",
    "\n",
    "        # --------- main training loop ---------\n",
    "        # alternating start\n",
    "        if episode % 2 == 1:\n",
    "            env_steps +=1\n",
    "            s0 = state\n",
    "            opp_action = get_opponent_action(env, agent, episode, s0, player=-1,\n",
    "                                             depth=lookahead_depth, frozen_opp=frozen_opp, phase=phase)\n",
    "            s1, r_opp, done = env.step(opp_action)\n",
    "            # append the opponent step\n",
    "            nstep_buf.append(s0, opp_action, r_opp, s1, done, player=-1)\n",
    "            state = s1\n",
    "\n",
    "        # Turn-by-turn until terminal\n",
    "        while not done:\n",
    "            # --- Agent (+1) acts ---\n",
    "            env_steps +=1\n",
    "            valid_actions = env.available_actions()\n",
    "\n",
    "            \n",
    "            #DEBUG\n",
    "            if not valid_actions:\n",
    "                print(f\"\\nâŒ No valid actions detected! Likely a full board or invalid game state.\")\n",
    "                print(f\"ðŸ“¦ Episode: {episode}, Step: {env_steps}\")\n",
    "                print(f\"ðŸŽ¯ Current player: {env.current_player}\")\n",
    "                print(f\"ðŸ“Š Top row: {env.board[0]}\")\n",
    "                print(f\"ðŸ§  Decoded board from state:\")\n",
    "                decoded_board = agent.decode_board_from_state(state, player=+1)\n",
    "                print(decoded_board)\n",
    "                print(f\"ðŸ§© Full environment board:\")\n",
    "                print(env.board)\n",
    "                print(f\"ðŸªª env.done = {env.done}, env.winner = {env.winner}\")\n",
    "    \n",
    "                raise ValueError(f\"[main loop] No valid actions for agent â€” this should be terminal. Check step() logic or training loop.\")\n",
    "\n",
    "\n",
    "            action = agent.act(state, valid_actions, player=+1, depth=lookahead_depth, strategy_weights=strategy_weights)\n",
    "            next_state, r_agent, done = env.step(action)\n",
    "\n",
    "            # append agent step\n",
    "            nstep_buf.append(state, action, r_agent, next_state, done, player=+1)\n",
    "\n",
    "            if done:\n",
    "                # terminal on agent move\n",
    "                final_result = evaluate_final_result(env, agent_player=+1)\n",
    "                total_reward = map_final_result_to_reward(final_result)\n",
    "                state = next_state\n",
    "                break\n",
    "\n",
    "            # --- Opponent (âˆ’1) responds ---\n",
    "            env_steps +=1\n",
    "            opp_action = get_opponent_action(env, agent, episode, next_state, player=-1,\n",
    "                                             depth=lookahead_depth, frozen_opp=frozen_opp, phase=phase)\n",
    "            next_state2, r_opp, done = env.step(opp_action)\n",
    "\n",
    "            # append opponent step\n",
    "            nstep_buf.append(next_state, opp_action, r_opp, next_state2, done, player=-1)\n",
    "\n",
    "            if done:\n",
    "                final_result = evaluate_final_result(env, agent_player=+1)\n",
    "                total_reward = map_final_result_to_reward(final_result)\n",
    "                state = next_state2\n",
    "                break\n",
    "\n",
    "            # continue\n",
    "            state = next_state2\n",
    "\n",
    "        # Episode ended â€” flush remaining short tails so they also yield n-step targets\n",
    "        nstep_buf.flush()\n",
    "\n",
    "        # One (or a few) optimization steps\n",
    "        WARMUP = max(batch_size * 5, 2000)\n",
    "        UPDATES_PER_EP = 4\n",
    "        if len(agent.memory) >= WARMUP:\n",
    "            for _ in range(UPDATES_PER_EP):\n",
    "                agent.replay(batch_size, mix_1step=0.6)  # try 0.5â€“0.7\n",
    "        elif len(agent.memory) >= WARMUP // 2:\n",
    "            agent.replay(max(batch_size // 2, 32), mix_1step=0.8)\n",
    "\n",
    "\n",
    "        # --------- bookkeeping ---------\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        epsilon_min_history.append(agent.epsilon_min)\n",
    "        reward_history.append(total_reward)\n",
    "        memory_prune_low_history.append(memory_prune_low)\n",
    "\n",
    "        # Epsilon decay (per episode)\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "            agent.epsilon = max(agent.epsilon, agent.epsilon_min)\n",
    "\n",
    "        # Win/loss/draw counters\n",
    "        wins, losses, draws = track_result(final_result, win_history)\n",
    "        win_count += wins\n",
    "        loss_count += losses\n",
    "        draw_count += draws\n",
    "\n",
    "        # Target network sync, with envs steps (not episodes)\n",
    "        if env_steps % target_update_interval == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        # Live plot\n",
    "        if episode % plot_interval == 0:\n",
    "            avg_reward = np.mean(reward_history[-25:]) if len(reward_history) >= 1 else 0.0\n",
    "            pbar.set_postfix(avg_reward=f\"{avg_reward:.2f}\", epsilon=f\"{agent.epsilon:.3f}\",\n",
    "                             wins=win_count, losses=loss_count, draws=draw_count, phase=phase)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            # optional board show:\n",
    "            # Connect4_BoardDisplayer.display_board(state)\n",
    "            plot_live_training(\n",
    "                episode, reward_history, win_history, epsilon_history,\n",
    "                phase, win_count, loss_count, draw_count, TRAINING_SESSION,\n",
    "                 epsilon_min_history, memory_prune_low_history,\n",
    "                agent=agent\n",
    "            )\n",
    "\n",
    "        # Periodic logging\n",
    "        if episode % log_every_x_episode == 0:\n",
    "            log_summary_stats(\n",
    "                episode=episode, reward_history=reward_history, win_history=win_history, phase=phase,\n",
    "                strategy_weights=strategy_weights, agent=agent, win_count=win_count, loss_count=loss_count,\n",
    "                draw_count=draw_count, summary_stats_dict=summary_stats\n",
    "            )\n",
    "            \n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"\\nTraining completed in {elapsed/60:.1f} minutes ({elapsed / num_episodes:.2f} s/episode)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd75ba-a7ac-4694-9550-a280aeeb672d",
   "metadata": {},
   "source": [
    "## Final training plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e429a-62e8-4d22-b439-2b7ecb829bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_live_training(episode, reward_history, win_history, epsilon_history, \n",
    "                               phase, win_count, loss_count, draw_count, TRAINING_SESSION,\n",
    "                              epsilon_min_history, memory_prune_low_history,agent=agent,\n",
    "                   save = True, path= PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e6de7-05c1-47e5-addf-b786a2ded901",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_td_error_hist(agent)\n",
    "plot_td_running(agent, window=1000)\n",
    "plot_is_weights(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc5d38-d108-40a3-804a-2c679f54bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save final Win Rate plot ---\n",
    "save_final_winrate_plot(win_history=win_history, training_phases=TRAINING_PHASES, save_path=PLOTS, session_name=TRAINING_SESSION)\n",
    "print(f\"Win rate plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_winrate.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216ccf7-ac10-48ba-8cf3-c01b1586e6e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nSummary stats (every {log_every_x_episode} episodes):\")\n",
    "pd.DataFrame.from_dict(summary_stats, orient='index').to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-training_summary.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2badbf-dabe-4008-8c45-391a67652a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_phase_summary(summary_stats, TRAINING_PHASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ebe25-6a98-4fa3-83ed-706bfa90e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26e8ff-1e6d-4f9c-ba89-048f152a10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dda0c-1079-4204-9881-757af54a747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1000\n",
    "smoothed = [np.mean(reward_history[max(0, i-window):i+1]) for i in range(len(reward_history))]\n",
    "plt.plot(smoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232098e-bae9-425b-b74b-4b1e5ce71a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 250\n",
    "smoothed = [np.mean(reward_history[max(0, i - window):i + 1]) for i in range(len(reward_history))]\n",
    "\n",
    "final_reward_fig, final_reward_ax = plt.subplots(figsize=(10, 5))\n",
    "final_reward_ax.plot(smoothed, label=f\"Smoothed Reward (window={window})\", color='blue')\n",
    "\n",
    "# --- Add phase transitions ---\n",
    "for name, meta in TRAINING_PHASES.items():\n",
    "    ep = meta[\"length\"]\n",
    "    if ep is not None and ep <= len(reward_history):\n",
    "        final_reward_ax.axvline(x=ep, color='black', linestyle='dotted', linewidth=1)\n",
    "        final_reward_ax.text(ep + 5, max(smoothed) * 0.95, name,\n",
    "                             rotation=90, va='top', ha='left', fontsize=8)\n",
    "\n",
    "final_reward_ax.set_title(\"Smoothed Reward Over Episodes\")\n",
    "final_reward_ax.set_xlabel(\"Episode\")\n",
    "final_reward_ax.set_ylabel(\"Smoothed Reward\")\n",
    "final_reward_ax.legend()\n",
    "final_reward_ax.grid(True)\n",
    "final_reward_fig.tight_layout()\n",
    "\n",
    "# --- Show plot ---\n",
    "plt.show()\n",
    "\n",
    "# --- Save to file ---\n",
    "final_reward_fig.savefig(f\"{PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n",
    "plt.close(final_reward_fig)\n",
    "print(f\"Smoothed reward plot saved to {PLOTS}DQN-{TRAINING_SESSION}_final_reward_smoothed.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f636ff5-a35d-4aa3-bbfd-389616cdc3b9",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bdd1a-62a5-4fc6-b191-d8927b1da9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = f\"{MODEL_DIR}{TRAINING_SESSION}_Connect4 dqn_model_{timestamp} episodes-{num_episodes} lookahead-{lookahead_depth}.pt\"\n",
    "default_model_path = \"Connect4 DQN model.pt\"\n",
    "\n",
    "torch.save(agent.model.state_dict(), model_path)\n",
    "torch.save(agent.model.state_dict(), default_model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c93c4b-80f3-4e23-9e2d-1f01f9112d8d",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87360d37-f8d9-48e2-935a-660863a3da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(device=device)  # Fresh agent instance\n",
    "state_dict = torch.load(default_model_path, map_location=device, weights_only=True)\n",
    "agent.model.load_state_dict(state_dict)\n",
    "agent.update_target_model()\n",
    "agent.epsilon = 0.0  # Fully greedy â€” no exploration\n",
    "print(\"âœ… Trained model loaded and ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31153e6e-79a6-4106-af92-b6cac6b6f834",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb37c78-44b6-47d8-8fc3-75b12780f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pure-model evaluation ( ===\n",
    "# Opponents and games\n",
    "evaluation_opponents = {\n",
    "    \"Random\": 203,\n",
    "    \"Lookahead-1\": 101,\n",
    "    \"Lookahead-2\": 53,\n",
    "    \"Lookahead-3\": 11,\n",
    "}\n",
    "evaluation_results = {}\n",
    "\n",
    "# Make model deterministic & greedy\n",
    "agent_model_mode  = agent.model.training\n",
    "agent_target_mode = agent.target_model.training\n",
    "_eps_backup, _epsmin_backup = agent.epsilon, agent.epsilon_min\n",
    "\n",
    "agent.model.eval()\n",
    "agent.target_model.eval()\n",
    "agent.epsilon = 0.0\n",
    "agent.epsilon_min = 0.0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for label, num_games in evaluation_opponents.items():\n",
    "    wins = losses = draws = 0\n",
    "    depth = int(label.split(\"-\")[1]) if label.startswith(\"Lookahead\") else None\n",
    "\n",
    "    with tqdm(total=num_games, desc=f\"Opponent: {label}\") as pbar:\n",
    "        for game_index in range(num_games):\n",
    "            state = env.reset()\n",
    "            # Alternate starters; agent is +1 in this env\n",
    "            env.current_player = 1 if (game_index % 2 == 0) else -1\n",
    "            #env.current_player = 1 \n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                valid_actions = env.available_actions()\n",
    "\n",
    "                if env.current_player == 1:\n",
    "                    # --- Pure model forward, mask illegal, argmax ---\n",
    "                    with torch.no_grad():\n",
    "                        x = build_input(agent, state, device)                 # (1, C, 6, 7)\n",
    "                        q = agent.model(x).squeeze(0).detach().cpu().numpy()  # (7,)\n",
    "                    mask = np.full_like(q, -1e9, dtype=np.float32)\n",
    "                    for a in valid_actions:\n",
    "                        mask[a] = 0.0\n",
    "                    action = int(np.argmax(q + mask))\n",
    "                    if action not in valid_actions:\n",
    "                        action = random.choice(valid_actions)\n",
    "                else:\n",
    "                    # Opponent move\n",
    "                    if label == \"Random\":\n",
    "                        action = random.choice(valid_actions)\n",
    "                    else:\n",
    "                        action = Lookahead.n_step_lookahead(np.array(state), -1, depth=depth)\n",
    "                        if action not in valid_actions:\n",
    "                            action = random.choice(valid_actions)\n",
    "\n",
    "                state, _, done = env.step(action)\n",
    "\n",
    "            # Tally from agentâ€™s perspective (+1 is agent)\n",
    "            if env.winner == 1:\n",
    "                wins += 1\n",
    "            elif env.winner == -1:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    evaluation_results[label] = {\n",
    "        \"wins\": wins,\n",
    "        \"losses\": losses,\n",
    "        \"draws\": draws,\n",
    "        \"win_rate\": round(wins / num_games, 3),\n",
    "        \"loss_rate\": round(losses / num_games, 3),\n",
    "        \"draw_rate\": round(draws / num_games, 3),\n",
    "    }\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Evaluation completed in {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016fd424-c11f-48d9-b7a7-68f8285e556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Print Summary ===\n",
    "print(\"\\nðŸ“Š Evaluation Summary:\")\n",
    "for label, stats in evaluation_results.items():\n",
    "    print(f\"{label}: {stats['wins']}W / {stats['losses']}L / {stats['draws']}D â†’ \"\n",
    "          f\"Win: {stats['win_rate']*100:.1f}%, Loss: {stats['loss_rate']*100:.1f}%, Draw: {stats['draw_rate']*100:.1f}%\")\n",
    "\n",
    "# === Bar Plot Summary ===\n",
    "labels = list(evaluation_results.keys())\n",
    "win_rates  = [evaluation_results[k]['win_rate']  * 100 for k in labels]\n",
    "loss_rates = [evaluation_results[k]['loss_rate'] * 100 for k in labels]\n",
    "draw_rates = [evaluation_results[k]['draw_rate'] * 100 for k in labels]\n",
    "\n",
    "x = range(len(labels))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, win_rates, width=bar_width, label='Win %')\n",
    "plt.bar([i + bar_width for i in x], loss_rates, width=bar_width, label='Loss %')\n",
    "plt.bar([i + 2 * bar_width for i in x], draw_rates, width=bar_width, label='Draw %')\n",
    "plt.xlabel('Opponent Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('DQN Agent Performance vs Various Opponents')\n",
    "plt.xticks([i + bar_width for i in x], labels, rotation=15)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = f\"{PLOTS}DQN-{TRAINING_SESSION}-evaluation_plot.png\"\n",
    "plt.savefig(plot_path, dpi=150)\n",
    "print(f\"ðŸ“Š Plot saved to {plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "df_eval = pd.DataFrame.from_dict(evaluation_results, orient='index')\n",
    "df_eval.index.name = \"Opponent\"\n",
    "# Use Excel if available; otherwise fall back to CSV\n",
    "try:\n",
    "    df_eval.to_excel(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-evaluation_results.xlsx\", index=True)\n",
    "except Exception as e:\n",
    "    print(\"Excel export failed, saving CSV instead:\", e)\n",
    "    df_eval.to_csv(f\"{LOG_DIR}DQN-{TRAINING_SESSION}-evaluation_results.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b01784-2970-4c87-a91b-ca40a635ab0d",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cabf946-a47a-427e-8aa9-ccc9c844a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_end_time = time.time()\n",
    "total_elapsed = (total_end_time - begin_start_time) / 3600\n",
    "print(f\"Evaluation completed in {total_elapsed:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e8d05-4a25-47b9-afb9-aea190a1f90e",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601c437-841c-459f-93d4-029cdfa9dc43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING_SESSION\n",
    "\n",
    "training_log_file = \"DQN training_sessions.xlsx\"\n",
    "log_row = {\"TRAINING_SESSION\": TRAINING_SESSION, \"TIME [h]\": total_elapsed, \"EPISODES\": num_episodes}\n",
    "\n",
    "for label, stats in evaluation_results.items():\n",
    "    log_row[label] = stats[\"win_rate\"]\n",
    "\n",
    "# === Load or Create Excel File ===\n",
    "if os.path.exists(training_log_file):\n",
    "    df_log = pd.read_excel(training_log_file)\n",
    "else:\n",
    "    df_log = pd.DataFrame()\n",
    "\n",
    "# === Append and Save ===\n",
    "df_log = pd.concat([df_log, pd.DataFrame([log_row])], ignore_index=True)\n",
    "df_log.to_excel(training_log_file, index=False)\n",
    "\n",
    "print(f\"\\nðŸ“ Training session logged to: {training_log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
